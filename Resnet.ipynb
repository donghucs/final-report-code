{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcXVyWTBsHgc/ZmV/iG4O+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ebfb584bbcc42a6bbebe2195b3b30c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8d313832f1c64f1191e061446a77b0c0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8eebee156b174a19be980b6cd7a56e9a",
              "IPY_MODEL_cf245147fc624aa5b2084f7fe8ff3468",
              "IPY_MODEL_0aa99b490aad4495953adeeae3004330"
            ]
          }
        },
        "8d313832f1c64f1191e061446a77b0c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8eebee156b174a19be980b6cd7a56e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d32f669079504652bed5bdb6ec49337d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_607a2326712b44ee8012593f7dbac0e9"
          }
        },
        "cf245147fc624aa5b2084f7fe8ff3468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_061c2e76eab14d909785f7140f8e7aee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_209ba50f64814f4b907e217d6288a7ef"
          }
        },
        "0aa99b490aad4495953adeeae3004330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8096b598b6284d1eb23ed98269ff47b2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:05&lt;00:00, 31760572.00it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_134bb9acb2504175811bc535f4bfe7cc"
          }
        },
        "d32f669079504652bed5bdb6ec49337d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "607a2326712b44ee8012593f7dbac0e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "061c2e76eab14d909785f7140f8e7aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "209ba50f64814f4b907e217d6288a7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8096b598b6284d1eb23ed98269ff47b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "134bb9acb2504175811bc535f4bfe7cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donghucs/final-report-code/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import os\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
        "start_time = time.time()\n",
        "batch_size = 128\n",
        "learning_rate = 0.01\n",
        "root_dir = 'drive/app/cifar10/'\n",
        "default_directory = 'drive/app/torch/save_models'\n",
        "\n",
        "# Data Augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n",
        "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
        "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
        "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
        "    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n",
        "                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n",
        "])\n",
        "\n",
        "# automatically download\n",
        "train_dataset = datasets.CIFAR10(root=root_dir,\n",
        "                                 train=True,\n",
        "                                 transform=transform_train,\n",
        "                                 download=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root=root_dir,\n",
        "                                train=False,\n",
        "                                transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
        "                                           num_workers=4)           # CPU loader number\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
        "                                          num_workers=4)            # CPU loader number\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = out + self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "model = ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "optimizer = optim.SGD(model.parameters(), learning_rate,\n",
        "                                momentum=0.9,\n",
        "                                weight_decay=5e-4,\n",
        "                                nesterov=True)\n",
        "'''\n",
        "optimizer = optim.Adam(model.parameters(), learning_rate,\n",
        "                                eps=1e-08,\n",
        "                                weight_decay=1e-4\n",
        "                                )\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if torch.cuda.device_count() > 0:\n",
        "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model).cuda()\n",
        "    cudnn.benchmark = True\n",
        "else:\n",
        "    print(\"USE ONLY CPU!\")\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0 \n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
        "        else:\n",
        "            data, target = Variable(data), Variable(target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target.data).cpu().sum()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
        "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
        "        else:\n",
        "            data, target = Variable(data), Variable(target)\n",
        "\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target.data).cpu().sum()\n",
        "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
        "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
        "\n",
        "\n",
        "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    model_filename = os.path.join(directory, filename)\n",
        "    torch.save(state, model_filename)\n",
        "    print(\"=> saving checkpoint\")\n",
        "\n",
        "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
        "\n",
        "    model_filename = os.path.join(directory, filename)\n",
        "    if os.path.exists(model_filename):\n",
        "        print(\"=> loading checkpoint\")\n",
        "        state = torch.load(model_filename)\n",
        "        return state\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "checkpoint = load_checkpoint(default_directory)\n",
        "if not checkpoint:\n",
        "    pass\n",
        "else:\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "for epoch in range(start_epoch, 165):\n",
        "\n",
        "    if epoch < 80:\n",
        "        lr = learning_rate\n",
        "    elif epoch < 120:\n",
        "        lr = learning_rate * 0.1\n",
        "    else:\n",
        "        lr = learning_rate * 0.01\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    train(epoch)\n",
        "    save_checkpoint(default_directory, {\n",
        "        'epoch': epoch,\n",
        "        'model': model,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    })\n",
        "    test()  \n",
        "\n",
        "now = time.gmtime(time.time() - start_time)\n",
        "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6ebfb584bbcc42a6bbebe2195b3b30c8",
            "8d313832f1c64f1191e061446a77b0c0",
            "8eebee156b174a19be980b6cd7a56e9a",
            "cf245147fc624aa5b2084f7fe8ff3468",
            "0aa99b490aad4495953adeeae3004330",
            "d32f669079504652bed5bdb6ec49337d",
            "607a2326712b44ee8012593f7dbac0e9",
            "061c2e76eab14d909785f7140f8e7aee",
            "209ba50f64814f4b907e217d6288a7ef",
            "8096b598b6284d1eb23ed98269ff47b2",
            "134bb9acb2504175811bc535f4bfe7cc"
          ]
        },
        "id": "xsaR60QEVAVI",
        "outputId": "300a9dd2-263e-4270-8149-0f93bfa11267"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to drive/app/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ebfb584bbcc42a6bbebe2195b3b30c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting drive/app/cifar10/cifar-10-python.tar.gz to drive/app/cifar10/\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "# TEST : Loss: (0.4375) | Acc: (87.29%) (8729/10000)\n",
            "Epoch: 46 | Batch_idx: 0 |  Loss: (0.0988) | Acc: (94.53%) (121/128)\n",
            "Epoch: 46 | Batch_idx: 10 |  Loss: (0.1362) | Acc: (94.96%) (1337/1408)\n",
            "Epoch: 46 | Batch_idx: 20 |  Loss: (0.1265) | Acc: (95.68%) (2572/2688)\n",
            "Epoch: 46 | Batch_idx: 30 |  Loss: (0.1179) | Acc: (96.02%) (3810/3968)\n",
            "Epoch: 46 | Batch_idx: 40 |  Loss: (0.1148) | Acc: (96.21%) (5049/5248)\n",
            "Epoch: 46 | Batch_idx: 50 |  Loss: (0.1113) | Acc: (96.38%) (6292/6528)\n",
            "Epoch: 46 | Batch_idx: 60 |  Loss: (0.1104) | Acc: (96.35%) (7523/7808)\n",
            "Epoch: 46 | Batch_idx: 70 |  Loss: (0.1094) | Acc: (96.40%) (8761/9088)\n",
            "Epoch: 46 | Batch_idx: 80 |  Loss: (0.1102) | Acc: (96.29%) (9983/10368)\n",
            "Epoch: 46 | Batch_idx: 90 |  Loss: (0.1091) | Acc: (96.30%) (11217/11648)\n",
            "Epoch: 46 | Batch_idx: 100 |  Loss: (0.1083) | Acc: (96.32%) (12452/12928)\n",
            "Epoch: 46 | Batch_idx: 110 |  Loss: (0.1083) | Acc: (96.35%) (13690/14208)\n",
            "Epoch: 46 | Batch_idx: 120 |  Loss: (0.1085) | Acc: (96.32%) (14918/15488)\n",
            "Epoch: 46 | Batch_idx: 130 |  Loss: (0.1086) | Acc: (96.32%) (16151/16768)\n",
            "Epoch: 46 | Batch_idx: 140 |  Loss: (0.1096) | Acc: (96.27%) (17375/18048)\n",
            "Epoch: 46 | Batch_idx: 150 |  Loss: (0.1105) | Acc: (96.23%) (18599/19328)\n",
            "Epoch: 46 | Batch_idx: 160 |  Loss: (0.1108) | Acc: (96.20%) (19825/20608)\n",
            "Epoch: 46 | Batch_idx: 170 |  Loss: (0.1132) | Acc: (96.08%) (21030/21888)\n",
            "Epoch: 46 | Batch_idx: 180 |  Loss: (0.1129) | Acc: (96.06%) (22256/23168)\n",
            "Epoch: 46 | Batch_idx: 190 |  Loss: (0.1139) | Acc: (96.04%) (23479/24448)\n",
            "Epoch: 46 | Batch_idx: 200 |  Loss: (0.1141) | Acc: (96.03%) (24706/25728)\n",
            "Epoch: 46 | Batch_idx: 210 |  Loss: (0.1141) | Acc: (96.03%) (25936/27008)\n",
            "Epoch: 46 | Batch_idx: 220 |  Loss: (0.1150) | Acc: (95.99%) (27155/28288)\n",
            "Epoch: 46 | Batch_idx: 230 |  Loss: (0.1151) | Acc: (95.99%) (28382/29568)\n",
            "Epoch: 46 | Batch_idx: 240 |  Loss: (0.1155) | Acc: (95.97%) (29605/30848)\n",
            "Epoch: 46 | Batch_idx: 250 |  Loss: (0.1153) | Acc: (96.01%) (30847/32128)\n",
            "Epoch: 46 | Batch_idx: 260 |  Loss: (0.1152) | Acc: (96.01%) (32075/33408)\n",
            "Epoch: 46 | Batch_idx: 270 |  Loss: (0.1144) | Acc: (96.04%) (33314/34688)\n",
            "Epoch: 46 | Batch_idx: 280 |  Loss: (0.1145) | Acc: (96.03%) (34540/35968)\n",
            "Epoch: 46 | Batch_idx: 290 |  Loss: (0.1151) | Acc: (96.01%) (35763/37248)\n",
            "Epoch: 46 | Batch_idx: 300 |  Loss: (0.1154) | Acc: (96.01%) (36992/38528)\n",
            "Epoch: 46 | Batch_idx: 310 |  Loss: (0.1156) | Acc: (96.00%) (38215/39808)\n",
            "Epoch: 46 | Batch_idx: 320 |  Loss: (0.1160) | Acc: (95.99%) (39441/41088)\n",
            "Epoch: 46 | Batch_idx: 330 |  Loss: (0.1160) | Acc: (96.00%) (40674/42368)\n",
            "Epoch: 46 | Batch_idx: 340 |  Loss: (0.1160) | Acc: (96.00%) (41903/43648)\n",
            "Epoch: 46 | Batch_idx: 350 |  Loss: (0.1158) | Acc: (96.02%) (43139/44928)\n",
            "Epoch: 46 | Batch_idx: 360 |  Loss: (0.1161) | Acc: (96.01%) (44364/46208)\n",
            "Epoch: 46 | Batch_idx: 370 |  Loss: (0.1160) | Acc: (96.01%) (45595/47488)\n",
            "Epoch: 46 | Batch_idx: 380 |  Loss: (0.1162) | Acc: (96.01%) (46822/48768)\n",
            "Epoch: 46 | Batch_idx: 390 |  Loss: (0.1165) | Acc: (96.00%) (48002/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3409) | Acc: (89.93%) (8993/10000)\n",
            "Epoch: 47 | Batch_idx: 0 |  Loss: (0.0661) | Acc: (98.44%) (126/128)\n",
            "Epoch: 47 | Batch_idx: 10 |  Loss: (0.0888) | Acc: (97.02%) (1366/1408)\n",
            "Epoch: 47 | Batch_idx: 20 |  Loss: (0.0931) | Acc: (96.80%) (2602/2688)\n",
            "Epoch: 47 | Batch_idx: 30 |  Loss: (0.0991) | Acc: (96.62%) (3834/3968)\n",
            "Epoch: 47 | Batch_idx: 40 |  Loss: (0.1012) | Acc: (96.49%) (5064/5248)\n",
            "Epoch: 47 | Batch_idx: 50 |  Loss: (0.1054) | Acc: (96.48%) (6298/6528)\n",
            "Epoch: 47 | Batch_idx: 60 |  Loss: (0.1057) | Acc: (96.49%) (7534/7808)\n",
            "Epoch: 47 | Batch_idx: 70 |  Loss: (0.1032) | Acc: (96.57%) (8776/9088)\n",
            "Epoch: 47 | Batch_idx: 80 |  Loss: (0.1051) | Acc: (96.56%) (10011/10368)\n",
            "Epoch: 47 | Batch_idx: 90 |  Loss: (0.1028) | Acc: (96.66%) (11259/11648)\n",
            "Epoch: 47 | Batch_idx: 100 |  Loss: (0.1020) | Acc: (96.65%) (12495/12928)\n",
            "Epoch: 47 | Batch_idx: 110 |  Loss: (0.1042) | Acc: (96.52%) (13714/14208)\n",
            "Epoch: 47 | Batch_idx: 120 |  Loss: (0.1048) | Acc: (96.46%) (14940/15488)\n",
            "Epoch: 47 | Batch_idx: 130 |  Loss: (0.1049) | Acc: (96.47%) (16176/16768)\n",
            "Epoch: 47 | Batch_idx: 140 |  Loss: (0.1055) | Acc: (96.43%) (17404/18048)\n",
            "Epoch: 47 | Batch_idx: 150 |  Loss: (0.1064) | Acc: (96.38%) (18629/19328)\n",
            "Epoch: 47 | Batch_idx: 160 |  Loss: (0.1062) | Acc: (96.38%) (19862/20608)\n",
            "Epoch: 47 | Batch_idx: 170 |  Loss: (0.1049) | Acc: (96.45%) (21110/21888)\n",
            "Epoch: 47 | Batch_idx: 180 |  Loss: (0.1053) | Acc: (96.46%) (22347/23168)\n",
            "Epoch: 47 | Batch_idx: 190 |  Loss: (0.1047) | Acc: (96.48%) (23588/24448)\n",
            "Epoch: 47 | Batch_idx: 200 |  Loss: (0.1044) | Acc: (96.52%) (24832/25728)\n",
            "Epoch: 47 | Batch_idx: 210 |  Loss: (0.1053) | Acc: (96.48%) (26057/27008)\n",
            "Epoch: 47 | Batch_idx: 220 |  Loss: (0.1053) | Acc: (96.48%) (27292/28288)\n",
            "Epoch: 47 | Batch_idx: 230 |  Loss: (0.1064) | Acc: (96.46%) (28522/29568)\n",
            "Epoch: 47 | Batch_idx: 240 |  Loss: (0.1074) | Acc: (96.42%) (29745/30848)\n",
            "Epoch: 47 | Batch_idx: 250 |  Loss: (0.1073) | Acc: (96.42%) (30978/32128)\n",
            "Epoch: 47 | Batch_idx: 260 |  Loss: (0.1080) | Acc: (96.38%) (32199/33408)\n",
            "Epoch: 47 | Batch_idx: 270 |  Loss: (0.1082) | Acc: (96.35%) (33423/34688)\n",
            "Epoch: 47 | Batch_idx: 280 |  Loss: (0.1087) | Acc: (96.33%) (34647/35968)\n",
            "Epoch: 47 | Batch_idx: 290 |  Loss: (0.1083) | Acc: (96.33%) (35881/37248)\n",
            "Epoch: 47 | Batch_idx: 300 |  Loss: (0.1087) | Acc: (96.31%) (37108/38528)\n",
            "Epoch: 47 | Batch_idx: 310 |  Loss: (0.1091) | Acc: (96.30%) (38335/39808)\n",
            "Epoch: 47 | Batch_idx: 320 |  Loss: (0.1089) | Acc: (96.31%) (39570/41088)\n",
            "Epoch: 47 | Batch_idx: 330 |  Loss: (0.1094) | Acc: (96.28%) (40792/42368)\n",
            "Epoch: 47 | Batch_idx: 340 |  Loss: (0.1095) | Acc: (96.28%) (42023/43648)\n",
            "Epoch: 47 | Batch_idx: 350 |  Loss: (0.1094) | Acc: (96.29%) (43260/44928)\n",
            "Epoch: 47 | Batch_idx: 360 |  Loss: (0.1091) | Acc: (96.28%) (44491/46208)\n",
            "Epoch: 47 | Batch_idx: 370 |  Loss: (0.1094) | Acc: (96.27%) (45717/47488)\n",
            "Epoch: 47 | Batch_idx: 380 |  Loss: (0.1100) | Acc: (96.26%) (46943/48768)\n",
            "Epoch: 47 | Batch_idx: 390 |  Loss: (0.1100) | Acc: (96.26%) (48128/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3997) | Acc: (88.23%) (8823/10000)\n",
            "Epoch: 48 | Batch_idx: 0 |  Loss: (0.0865) | Acc: (98.44%) (126/128)\n",
            "Epoch: 48 | Batch_idx: 10 |  Loss: (0.0956) | Acc: (96.88%) (1364/1408)\n",
            "Epoch: 48 | Batch_idx: 20 |  Loss: (0.0915) | Acc: (96.95%) (2606/2688)\n",
            "Epoch: 48 | Batch_idx: 30 |  Loss: (0.0925) | Acc: (96.95%) (3847/3968)\n",
            "Epoch: 48 | Batch_idx: 40 |  Loss: (0.0917) | Acc: (96.95%) (5088/5248)\n",
            "Epoch: 48 | Batch_idx: 50 |  Loss: (0.0915) | Acc: (96.95%) (6329/6528)\n",
            "Epoch: 48 | Batch_idx: 60 |  Loss: (0.0926) | Acc: (96.90%) (7566/7808)\n",
            "Epoch: 48 | Batch_idx: 70 |  Loss: (0.0913) | Acc: (96.91%) (8807/9088)\n",
            "Epoch: 48 | Batch_idx: 80 |  Loss: (0.0940) | Acc: (96.80%) (10036/10368)\n",
            "Epoch: 48 | Batch_idx: 90 |  Loss: (0.0946) | Acc: (96.72%) (11266/11648)\n",
            "Epoch: 48 | Batch_idx: 100 |  Loss: (0.0950) | Acc: (96.73%) (12505/12928)\n",
            "Epoch: 48 | Batch_idx: 110 |  Loss: (0.0967) | Acc: (96.68%) (13736/14208)\n",
            "Epoch: 48 | Batch_idx: 120 |  Loss: (0.0971) | Acc: (96.63%) (14966/15488)\n",
            "Epoch: 48 | Batch_idx: 130 |  Loss: (0.0971) | Acc: (96.59%) (16197/16768)\n",
            "Epoch: 48 | Batch_idx: 140 |  Loss: (0.0957) | Acc: (96.67%) (17447/18048)\n",
            "Epoch: 48 | Batch_idx: 150 |  Loss: (0.0956) | Acc: (96.68%) (18687/19328)\n",
            "Epoch: 48 | Batch_idx: 160 |  Loss: (0.0967) | Acc: (96.63%) (19914/20608)\n",
            "Epoch: 48 | Batch_idx: 170 |  Loss: (0.0997) | Acc: (96.54%) (21130/21888)\n",
            "Epoch: 48 | Batch_idx: 180 |  Loss: (0.0989) | Acc: (96.59%) (22378/23168)\n",
            "Epoch: 48 | Batch_idx: 190 |  Loss: (0.0984) | Acc: (96.58%) (23613/24448)\n",
            "Epoch: 48 | Batch_idx: 200 |  Loss: (0.0983) | Acc: (96.58%) (24849/25728)\n",
            "Epoch: 48 | Batch_idx: 210 |  Loss: (0.0981) | Acc: (96.59%) (26086/27008)\n",
            "Epoch: 48 | Batch_idx: 220 |  Loss: (0.0989) | Acc: (96.59%) (27324/28288)\n",
            "Epoch: 48 | Batch_idx: 230 |  Loss: (0.0989) | Acc: (96.60%) (28563/29568)\n",
            "Epoch: 48 | Batch_idx: 240 |  Loss: (0.0993) | Acc: (96.59%) (29796/30848)\n",
            "Epoch: 48 | Batch_idx: 250 |  Loss: (0.1001) | Acc: (96.57%) (31027/32128)\n",
            "Epoch: 48 | Batch_idx: 260 |  Loss: (0.1002) | Acc: (96.59%) (32270/33408)\n",
            "Epoch: 48 | Batch_idx: 270 |  Loss: (0.1001) | Acc: (96.60%) (33510/34688)\n",
            "Epoch: 48 | Batch_idx: 280 |  Loss: (0.1000) | Acc: (96.60%) (34745/35968)\n",
            "Epoch: 48 | Batch_idx: 290 |  Loss: (0.1006) | Acc: (96.57%) (35972/37248)\n",
            "Epoch: 48 | Batch_idx: 300 |  Loss: (0.1012) | Acc: (96.55%) (37200/38528)\n",
            "Epoch: 48 | Batch_idx: 310 |  Loss: (0.1019) | Acc: (96.52%) (38421/39808)\n",
            "Epoch: 48 | Batch_idx: 320 |  Loss: (0.1021) | Acc: (96.53%) (39661/41088)\n",
            "Epoch: 48 | Batch_idx: 330 |  Loss: (0.1027) | Acc: (96.51%) (40888/42368)\n",
            "Epoch: 48 | Batch_idx: 340 |  Loss: (0.1033) | Acc: (96.49%) (42116/43648)\n",
            "Epoch: 48 | Batch_idx: 350 |  Loss: (0.1036) | Acc: (96.47%) (43344/44928)\n",
            "Epoch: 48 | Batch_idx: 360 |  Loss: (0.1037) | Acc: (96.47%) (44576/46208)\n",
            "Epoch: 48 | Batch_idx: 370 |  Loss: (0.1041) | Acc: (96.46%) (45808/47488)\n",
            "Epoch: 48 | Batch_idx: 380 |  Loss: (0.1047) | Acc: (96.44%) (47030/48768)\n",
            "Epoch: 48 | Batch_idx: 390 |  Loss: (0.1050) | Acc: (96.43%) (48214/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3947) | Acc: (88.94%) (8894/10000)\n",
            "Epoch: 49 | Batch_idx: 0 |  Loss: (0.0510) | Acc: (98.44%) (126/128)\n",
            "Epoch: 49 | Batch_idx: 10 |  Loss: (0.0876) | Acc: (97.09%) (1367/1408)\n",
            "Epoch: 49 | Batch_idx: 20 |  Loss: (0.0847) | Acc: (97.40%) (2618/2688)\n",
            "Epoch: 49 | Batch_idx: 30 |  Loss: (0.0859) | Acc: (97.30%) (3861/3968)\n",
            "Epoch: 49 | Batch_idx: 40 |  Loss: (0.0860) | Acc: (97.20%) (5101/5248)\n",
            "Epoch: 49 | Batch_idx: 50 |  Loss: (0.0918) | Acc: (96.97%) (6330/6528)\n",
            "Epoch: 49 | Batch_idx: 60 |  Loss: (0.0920) | Acc: (96.95%) (7570/7808)\n",
            "Epoch: 49 | Batch_idx: 70 |  Loss: (0.0949) | Acc: (96.78%) (8795/9088)\n",
            "Epoch: 49 | Batch_idx: 80 |  Loss: (0.0949) | Acc: (96.83%) (10039/10368)\n",
            "Epoch: 49 | Batch_idx: 90 |  Loss: (0.0945) | Acc: (96.81%) (11276/11648)\n",
            "Epoch: 49 | Batch_idx: 100 |  Loss: (0.0956) | Acc: (96.77%) (12511/12928)\n",
            "Epoch: 49 | Batch_idx: 110 |  Loss: (0.0961) | Acc: (96.74%) (13745/14208)\n",
            "Epoch: 49 | Batch_idx: 120 |  Loss: (0.0959) | Acc: (96.73%) (14982/15488)\n",
            "Epoch: 49 | Batch_idx: 130 |  Loss: (0.0972) | Acc: (96.67%) (16210/16768)\n",
            "Epoch: 49 | Batch_idx: 140 |  Loss: (0.0970) | Acc: (96.69%) (17451/18048)\n",
            "Epoch: 49 | Batch_idx: 150 |  Loss: (0.0964) | Acc: (96.71%) (18693/19328)\n",
            "Epoch: 49 | Batch_idx: 160 |  Loss: (0.0972) | Acc: (96.70%) (19927/20608)\n",
            "Epoch: 49 | Batch_idx: 170 |  Loss: (0.0971) | Acc: (96.71%) (21167/21888)\n",
            "Epoch: 49 | Batch_idx: 180 |  Loss: (0.0984) | Acc: (96.68%) (22399/23168)\n",
            "Epoch: 49 | Batch_idx: 190 |  Loss: (0.0988) | Acc: (96.67%) (23635/24448)\n",
            "Epoch: 49 | Batch_idx: 200 |  Loss: (0.0988) | Acc: (96.69%) (24876/25728)\n",
            "Epoch: 49 | Batch_idx: 210 |  Loss: (0.0994) | Acc: (96.66%) (26105/27008)\n",
            "Epoch: 49 | Batch_idx: 220 |  Loss: (0.0998) | Acc: (96.65%) (27339/28288)\n",
            "Epoch: 49 | Batch_idx: 230 |  Loss: (0.1003) | Acc: (96.60%) (28563/29568)\n",
            "Epoch: 49 | Batch_idx: 240 |  Loss: (0.1002) | Acc: (96.60%) (29800/30848)\n",
            "Epoch: 49 | Batch_idx: 250 |  Loss: (0.1012) | Acc: (96.57%) (31026/32128)\n",
            "Epoch: 49 | Batch_idx: 260 |  Loss: (0.1018) | Acc: (96.55%) (32255/33408)\n",
            "Epoch: 49 | Batch_idx: 270 |  Loss: (0.1023) | Acc: (96.51%) (33476/34688)\n",
            "Epoch: 49 | Batch_idx: 280 |  Loss: (0.1023) | Acc: (96.51%) (34713/35968)\n",
            "Epoch: 49 | Batch_idx: 290 |  Loss: (0.1027) | Acc: (96.51%) (35949/37248)\n",
            "Epoch: 49 | Batch_idx: 300 |  Loss: (0.1034) | Acc: (96.49%) (37175/38528)\n",
            "Epoch: 49 | Batch_idx: 310 |  Loss: (0.1037) | Acc: (96.47%) (38404/39808)\n",
            "Epoch: 49 | Batch_idx: 320 |  Loss: (0.1039) | Acc: (96.47%) (39638/41088)\n",
            "Epoch: 49 | Batch_idx: 330 |  Loss: (0.1038) | Acc: (96.49%) (40879/42368)\n",
            "Epoch: 49 | Batch_idx: 340 |  Loss: (0.1048) | Acc: (96.44%) (42094/43648)\n",
            "Epoch: 49 | Batch_idx: 350 |  Loss: (0.1047) | Acc: (96.45%) (43333/44928)\n",
            "Epoch: 49 | Batch_idx: 360 |  Loss: (0.1052) | Acc: (96.45%) (44566/46208)\n",
            "Epoch: 49 | Batch_idx: 370 |  Loss: (0.1055) | Acc: (96.45%) (45800/47488)\n",
            "Epoch: 49 | Batch_idx: 380 |  Loss: (0.1062) | Acc: (96.43%) (47026/48768)\n",
            "Epoch: 49 | Batch_idx: 390 |  Loss: (0.1062) | Acc: (96.42%) (48210/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4206) | Acc: (88.55%) (8855/10000)\n",
            "Epoch: 50 | Batch_idx: 0 |  Loss: (0.1211) | Acc: (96.09%) (123/128)\n",
            "Epoch: 50 | Batch_idx: 10 |  Loss: (0.1022) | Acc: (96.16%) (1354/1408)\n",
            "Epoch: 50 | Batch_idx: 20 |  Loss: (0.1015) | Acc: (96.09%) (2583/2688)\n",
            "Epoch: 50 | Batch_idx: 30 |  Loss: (0.0941) | Acc: (96.50%) (3829/3968)\n",
            "Epoch: 50 | Batch_idx: 40 |  Loss: (0.0966) | Acc: (96.51%) (5065/5248)\n",
            "Epoch: 50 | Batch_idx: 50 |  Loss: (0.0961) | Acc: (96.52%) (6301/6528)\n",
            "Epoch: 50 | Batch_idx: 60 |  Loss: (0.0956) | Acc: (96.58%) (7541/7808)\n",
            "Epoch: 50 | Batch_idx: 70 |  Loss: (0.0963) | Acc: (96.56%) (8775/9088)\n",
            "Epoch: 50 | Batch_idx: 80 |  Loss: (0.0962) | Acc: (96.59%) (10014/10368)\n",
            "Epoch: 50 | Batch_idx: 90 |  Loss: (0.0980) | Acc: (96.59%) (11251/11648)\n",
            "Epoch: 50 | Batch_idx: 100 |  Loss: (0.0976) | Acc: (96.59%) (12487/12928)\n",
            "Epoch: 50 | Batch_idx: 110 |  Loss: (0.0978) | Acc: (96.57%) (13720/14208)\n",
            "Epoch: 50 | Batch_idx: 120 |  Loss: (0.0987) | Acc: (96.56%) (14955/15488)\n",
            "Epoch: 50 | Batch_idx: 130 |  Loss: (0.1003) | Acc: (96.54%) (16188/16768)\n",
            "Epoch: 50 | Batch_idx: 140 |  Loss: (0.1013) | Acc: (96.50%) (17417/18048)\n",
            "Epoch: 50 | Batch_idx: 150 |  Loss: (0.1034) | Acc: (96.45%) (18642/19328)\n",
            "Epoch: 50 | Batch_idx: 160 |  Loss: (0.1029) | Acc: (96.44%) (19875/20608)\n",
            "Epoch: 50 | Batch_idx: 170 |  Loss: (0.1032) | Acc: (96.44%) (21109/21888)\n",
            "Epoch: 50 | Batch_idx: 180 |  Loss: (0.1034) | Acc: (96.47%) (22350/23168)\n",
            "Epoch: 50 | Batch_idx: 190 |  Loss: (0.1036) | Acc: (96.46%) (23582/24448)\n",
            "Epoch: 50 | Batch_idx: 200 |  Loss: (0.1043) | Acc: (96.43%) (24810/25728)\n",
            "Epoch: 50 | Batch_idx: 210 |  Loss: (0.1041) | Acc: (96.45%) (26050/27008)\n",
            "Epoch: 50 | Batch_idx: 220 |  Loss: (0.1043) | Acc: (96.46%) (27287/28288)\n",
            "Epoch: 50 | Batch_idx: 230 |  Loss: (0.1048) | Acc: (96.44%) (28514/29568)\n",
            "Epoch: 50 | Batch_idx: 240 |  Loss: (0.1053) | Acc: (96.41%) (29742/30848)\n",
            "Epoch: 50 | Batch_idx: 250 |  Loss: (0.1046) | Acc: (96.44%) (30985/32128)\n",
            "Epoch: 50 | Batch_idx: 260 |  Loss: (0.1046) | Acc: (96.43%) (32216/33408)\n",
            "Epoch: 50 | Batch_idx: 270 |  Loss: (0.1044) | Acc: (96.45%) (33457/34688)\n",
            "Epoch: 50 | Batch_idx: 280 |  Loss: (0.1042) | Acc: (96.45%) (34692/35968)\n",
            "Epoch: 50 | Batch_idx: 290 |  Loss: (0.1043) | Acc: (96.45%) (35925/37248)\n",
            "Epoch: 50 | Batch_idx: 300 |  Loss: (0.1041) | Acc: (96.46%) (37163/38528)\n",
            "Epoch: 50 | Batch_idx: 310 |  Loss: (0.1045) | Acc: (96.45%) (38394/39808)\n",
            "Epoch: 50 | Batch_idx: 320 |  Loss: (0.1045) | Acc: (96.45%) (39630/41088)\n",
            "Epoch: 50 | Batch_idx: 330 |  Loss: (0.1045) | Acc: (96.46%) (40870/42368)\n",
            "Epoch: 50 | Batch_idx: 340 |  Loss: (0.1046) | Acc: (96.45%) (42100/43648)\n",
            "Epoch: 50 | Batch_idx: 350 |  Loss: (0.1043) | Acc: (96.47%) (43341/44928)\n",
            "Epoch: 50 | Batch_idx: 360 |  Loss: (0.1041) | Acc: (96.48%) (44580/46208)\n",
            "Epoch: 50 | Batch_idx: 370 |  Loss: (0.1043) | Acc: (96.47%) (45813/47488)\n",
            "Epoch: 50 | Batch_idx: 380 |  Loss: (0.1051) | Acc: (96.44%) (47034/48768)\n",
            "Epoch: 50 | Batch_idx: 390 |  Loss: (0.1062) | Acc: (96.42%) (48211/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3450) | Acc: (89.87%) (8987/10000)\n",
            "Epoch: 51 | Batch_idx: 0 |  Loss: (0.1954) | Acc: (94.53%) (121/128)\n",
            "Epoch: 51 | Batch_idx: 10 |  Loss: (0.0941) | Acc: (97.30%) (1370/1408)\n",
            "Epoch: 51 | Batch_idx: 20 |  Loss: (0.0962) | Acc: (97.06%) (2609/2688)\n",
            "Epoch: 51 | Batch_idx: 30 |  Loss: (0.0886) | Acc: (97.18%) (3856/3968)\n",
            "Epoch: 51 | Batch_idx: 40 |  Loss: (0.0936) | Acc: (96.97%) (5089/5248)\n",
            "Epoch: 51 | Batch_idx: 50 |  Loss: (0.0941) | Acc: (96.98%) (6331/6528)\n",
            "Epoch: 51 | Batch_idx: 60 |  Loss: (0.0944) | Acc: (97.02%) (7575/7808)\n",
            "Epoch: 51 | Batch_idx: 70 |  Loss: (0.0931) | Acc: (97.06%) (8821/9088)\n",
            "Epoch: 51 | Batch_idx: 80 |  Loss: (0.0935) | Acc: (97.06%) (10063/10368)\n",
            "Epoch: 51 | Batch_idx: 90 |  Loss: (0.0948) | Acc: (96.98%) (11296/11648)\n",
            "Epoch: 51 | Batch_idx: 100 |  Loss: (0.0939) | Acc: (96.96%) (12535/12928)\n",
            "Epoch: 51 | Batch_idx: 110 |  Loss: (0.0931) | Acc: (96.99%) (13781/14208)\n",
            "Epoch: 51 | Batch_idx: 120 |  Loss: (0.0932) | Acc: (96.97%) (15018/15488)\n",
            "Epoch: 51 | Batch_idx: 130 |  Loss: (0.0924) | Acc: (96.99%) (16263/16768)\n",
            "Epoch: 51 | Batch_idx: 140 |  Loss: (0.0923) | Acc: (96.96%) (17500/18048)\n",
            "Epoch: 51 | Batch_idx: 150 |  Loss: (0.0927) | Acc: (96.92%) (18733/19328)\n",
            "Epoch: 51 | Batch_idx: 160 |  Loss: (0.0921) | Acc: (96.94%) (19978/20608)\n",
            "Epoch: 51 | Batch_idx: 170 |  Loss: (0.0932) | Acc: (96.87%) (21203/21888)\n",
            "Epoch: 51 | Batch_idx: 180 |  Loss: (0.0930) | Acc: (96.88%) (22446/23168)\n",
            "Epoch: 51 | Batch_idx: 190 |  Loss: (0.0934) | Acc: (96.86%) (23681/24448)\n",
            "Epoch: 51 | Batch_idx: 200 |  Loss: (0.0940) | Acc: (96.85%) (24917/25728)\n",
            "Epoch: 51 | Batch_idx: 210 |  Loss: (0.0941) | Acc: (96.84%) (26155/27008)\n",
            "Epoch: 51 | Batch_idx: 220 |  Loss: (0.0940) | Acc: (96.86%) (27399/28288)\n",
            "Epoch: 51 | Batch_idx: 230 |  Loss: (0.0952) | Acc: (96.82%) (28629/29568)\n",
            "Epoch: 51 | Batch_idx: 240 |  Loss: (0.0966) | Acc: (96.77%) (29853/30848)\n",
            "Epoch: 51 | Batch_idx: 250 |  Loss: (0.0975) | Acc: (96.73%) (31077/32128)\n",
            "Epoch: 51 | Batch_idx: 260 |  Loss: (0.0981) | Acc: (96.74%) (32318/33408)\n",
            "Epoch: 51 | Batch_idx: 270 |  Loss: (0.0987) | Acc: (96.71%) (33548/34688)\n",
            "Epoch: 51 | Batch_idx: 280 |  Loss: (0.0992) | Acc: (96.69%) (34779/35968)\n",
            "Epoch: 51 | Batch_idx: 290 |  Loss: (0.0995) | Acc: (96.70%) (36017/37248)\n",
            "Epoch: 51 | Batch_idx: 300 |  Loss: (0.0991) | Acc: (96.71%) (37259/38528)\n",
            "Epoch: 51 | Batch_idx: 310 |  Loss: (0.0991) | Acc: (96.72%) (38502/39808)\n",
            "Epoch: 51 | Batch_idx: 320 |  Loss: (0.0990) | Acc: (96.71%) (39737/41088)\n",
            "Epoch: 51 | Batch_idx: 330 |  Loss: (0.0983) | Acc: (96.74%) (40987/42368)\n",
            "Epoch: 51 | Batch_idx: 340 |  Loss: (0.0984) | Acc: (96.75%) (42228/43648)\n",
            "Epoch: 51 | Batch_idx: 350 |  Loss: (0.0990) | Acc: (96.71%) (43452/44928)\n",
            "Epoch: 51 | Batch_idx: 360 |  Loss: (0.0992) | Acc: (96.70%) (44683/46208)\n",
            "Epoch: 51 | Batch_idx: 370 |  Loss: (0.0994) | Acc: (96.69%) (45916/47488)\n",
            "Epoch: 51 | Batch_idx: 380 |  Loss: (0.1001) | Acc: (96.66%) (47139/48768)\n",
            "Epoch: 51 | Batch_idx: 390 |  Loss: (0.1003) | Acc: (96.66%) (48330/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3942) | Acc: (88.79%) (8879/10000)\n",
            "Epoch: 52 | Batch_idx: 0 |  Loss: (0.1022) | Acc: (96.09%) (123/128)\n",
            "Epoch: 52 | Batch_idx: 10 |  Loss: (0.0979) | Acc: (96.59%) (1360/1408)\n",
            "Epoch: 52 | Batch_idx: 20 |  Loss: (0.0974) | Acc: (96.61%) (2597/2688)\n",
            "Epoch: 52 | Batch_idx: 30 |  Loss: (0.0944) | Acc: (96.70%) (3837/3968)\n",
            "Epoch: 52 | Batch_idx: 40 |  Loss: (0.0911) | Acc: (96.82%) (5081/5248)\n",
            "Epoch: 52 | Batch_idx: 50 |  Loss: (0.0949) | Acc: (96.63%) (6308/6528)\n",
            "Epoch: 52 | Batch_idx: 60 |  Loss: (0.0939) | Acc: (96.72%) (7552/7808)\n",
            "Epoch: 52 | Batch_idx: 70 |  Loss: (0.0928) | Acc: (96.79%) (8796/9088)\n",
            "Epoch: 52 | Batch_idx: 80 |  Loss: (0.0949) | Acc: (96.74%) (10030/10368)\n",
            "Epoch: 52 | Batch_idx: 90 |  Loss: (0.0938) | Acc: (96.81%) (11276/11648)\n",
            "Epoch: 52 | Batch_idx: 100 |  Loss: (0.0940) | Acc: (96.79%) (12513/12928)\n",
            "Epoch: 52 | Batch_idx: 110 |  Loss: (0.0933) | Acc: (96.83%) (13757/14208)\n",
            "Epoch: 52 | Batch_idx: 120 |  Loss: (0.0951) | Acc: (96.70%) (14977/15488)\n",
            "Epoch: 52 | Batch_idx: 130 |  Loss: (0.0957) | Acc: (96.65%) (16207/16768)\n",
            "Epoch: 52 | Batch_idx: 140 |  Loss: (0.0955) | Acc: (96.64%) (17441/18048)\n",
            "Epoch: 52 | Batch_idx: 150 |  Loss: (0.0954) | Acc: (96.65%) (18680/19328)\n",
            "Epoch: 52 | Batch_idx: 160 |  Loss: (0.0952) | Acc: (96.64%) (19916/20608)\n",
            "Epoch: 52 | Batch_idx: 170 |  Loss: (0.0950) | Acc: (96.66%) (21156/21888)\n",
            "Epoch: 52 | Batch_idx: 180 |  Loss: (0.0950) | Acc: (96.70%) (22404/23168)\n",
            "Epoch: 52 | Batch_idx: 190 |  Loss: (0.0955) | Acc: (96.69%) (23638/24448)\n",
            "Epoch: 52 | Batch_idx: 200 |  Loss: (0.0969) | Acc: (96.66%) (24868/25728)\n",
            "Epoch: 52 | Batch_idx: 210 |  Loss: (0.0975) | Acc: (96.62%) (26096/27008)\n",
            "Epoch: 52 | Batch_idx: 220 |  Loss: (0.0976) | Acc: (96.61%) (27330/28288)\n",
            "Epoch: 52 | Batch_idx: 230 |  Loss: (0.0978) | Acc: (96.61%) (28565/29568)\n",
            "Epoch: 52 | Batch_idx: 240 |  Loss: (0.0977) | Acc: (96.64%) (29811/30848)\n",
            "Epoch: 52 | Batch_idx: 250 |  Loss: (0.0977) | Acc: (96.63%) (31046/32128)\n",
            "Epoch: 52 | Batch_idx: 260 |  Loss: (0.0979) | Acc: (96.62%) (32279/33408)\n",
            "Epoch: 52 | Batch_idx: 270 |  Loss: (0.0984) | Acc: (96.61%) (33512/34688)\n",
            "Epoch: 52 | Batch_idx: 280 |  Loss: (0.0992) | Acc: (96.59%) (34740/35968)\n",
            "Epoch: 52 | Batch_idx: 290 |  Loss: (0.0995) | Acc: (96.56%) (35966/37248)\n",
            "Epoch: 52 | Batch_idx: 300 |  Loss: (0.1002) | Acc: (96.53%) (37190/38528)\n",
            "Epoch: 52 | Batch_idx: 310 |  Loss: (0.1006) | Acc: (96.52%) (38421/39808)\n",
            "Epoch: 52 | Batch_idx: 320 |  Loss: (0.1005) | Acc: (96.53%) (39662/41088)\n",
            "Epoch: 52 | Batch_idx: 330 |  Loss: (0.1009) | Acc: (96.52%) (40894/42368)\n",
            "Epoch: 52 | Batch_idx: 340 |  Loss: (0.1016) | Acc: (96.49%) (42118/43648)\n",
            "Epoch: 52 | Batch_idx: 350 |  Loss: (0.1021) | Acc: (96.48%) (43345/44928)\n",
            "Epoch: 52 | Batch_idx: 360 |  Loss: (0.1022) | Acc: (96.46%) (44570/46208)\n",
            "Epoch: 52 | Batch_idx: 370 |  Loss: (0.1025) | Acc: (96.45%) (45801/47488)\n",
            "Epoch: 52 | Batch_idx: 380 |  Loss: (0.1026) | Acc: (96.44%) (47034/48768)\n",
            "Epoch: 52 | Batch_idx: 390 |  Loss: (0.1029) | Acc: (96.42%) (48211/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3685) | Acc: (89.26%) (8926/10000)\n",
            "Epoch: 53 | Batch_idx: 0 |  Loss: (0.0741) | Acc: (96.88%) (124/128)\n",
            "Epoch: 53 | Batch_idx: 10 |  Loss: (0.1008) | Acc: (96.66%) (1361/1408)\n",
            "Epoch: 53 | Batch_idx: 20 |  Loss: (0.0933) | Acc: (96.91%) (2605/2688)\n",
            "Epoch: 53 | Batch_idx: 30 |  Loss: (0.0895) | Acc: (96.98%) (3848/3968)\n",
            "Epoch: 53 | Batch_idx: 40 |  Loss: (0.0936) | Acc: (96.80%) (5080/5248)\n",
            "Epoch: 53 | Batch_idx: 50 |  Loss: (0.0927) | Acc: (96.95%) (6329/6528)\n",
            "Epoch: 53 | Batch_idx: 60 |  Loss: (0.0940) | Acc: (96.90%) (7566/7808)\n",
            "Epoch: 53 | Batch_idx: 70 |  Loss: (0.0936) | Acc: (96.90%) (8806/9088)\n",
            "Epoch: 53 | Batch_idx: 80 |  Loss: (0.0930) | Acc: (96.91%) (10048/10368)\n",
            "Epoch: 53 | Batch_idx: 90 |  Loss: (0.0937) | Acc: (96.88%) (11285/11648)\n",
            "Epoch: 53 | Batch_idx: 100 |  Loss: (0.0921) | Acc: (96.94%) (12532/12928)\n",
            "Epoch: 53 | Batch_idx: 110 |  Loss: (0.0928) | Acc: (96.92%) (13771/14208)\n",
            "Epoch: 53 | Batch_idx: 120 |  Loss: (0.0919) | Acc: (96.95%) (15015/15488)\n",
            "Epoch: 53 | Batch_idx: 130 |  Loss: (0.0917) | Acc: (96.95%) (16257/16768)\n",
            "Epoch: 53 | Batch_idx: 140 |  Loss: (0.0920) | Acc: (96.93%) (17494/18048)\n",
            "Epoch: 53 | Batch_idx: 150 |  Loss: (0.0935) | Acc: (96.88%) (18724/19328)\n",
            "Epoch: 53 | Batch_idx: 160 |  Loss: (0.0939) | Acc: (96.85%) (19959/20608)\n",
            "Epoch: 53 | Batch_idx: 170 |  Loss: (0.0949) | Acc: (96.80%) (21188/21888)\n",
            "Epoch: 53 | Batch_idx: 180 |  Loss: (0.0951) | Acc: (96.79%) (22424/23168)\n",
            "Epoch: 53 | Batch_idx: 190 |  Loss: (0.0959) | Acc: (96.77%) (23659/24448)\n",
            "Epoch: 53 | Batch_idx: 200 |  Loss: (0.0962) | Acc: (96.78%) (24899/25728)\n",
            "Epoch: 53 | Batch_idx: 210 |  Loss: (0.0960) | Acc: (96.81%) (26147/27008)\n",
            "Epoch: 53 | Batch_idx: 220 |  Loss: (0.0961) | Acc: (96.81%) (27386/28288)\n",
            "Epoch: 53 | Batch_idx: 230 |  Loss: (0.0958) | Acc: (96.83%) (28630/29568)\n",
            "Epoch: 53 | Batch_idx: 240 |  Loss: (0.0963) | Acc: (96.81%) (29863/30848)\n",
            "Epoch: 53 | Batch_idx: 250 |  Loss: (0.0956) | Acc: (96.83%) (31111/32128)\n",
            "Epoch: 53 | Batch_idx: 260 |  Loss: (0.0961) | Acc: (96.80%) (32339/33408)\n",
            "Epoch: 53 | Batch_idx: 270 |  Loss: (0.0966) | Acc: (96.77%) (33566/34688)\n",
            "Epoch: 53 | Batch_idx: 280 |  Loss: (0.0975) | Acc: (96.75%) (34798/35968)\n",
            "Epoch: 53 | Batch_idx: 290 |  Loss: (0.0979) | Acc: (96.76%) (36042/37248)\n",
            "Epoch: 53 | Batch_idx: 300 |  Loss: (0.0978) | Acc: (96.76%) (37278/38528)\n",
            "Epoch: 53 | Batch_idx: 310 |  Loss: (0.0982) | Acc: (96.75%) (38514/39808)\n",
            "Epoch: 53 | Batch_idx: 320 |  Loss: (0.0986) | Acc: (96.75%) (39752/41088)\n",
            "Epoch: 53 | Batch_idx: 330 |  Loss: (0.0989) | Acc: (96.73%) (40982/42368)\n",
            "Epoch: 53 | Batch_idx: 340 |  Loss: (0.0991) | Acc: (96.71%) (42212/43648)\n",
            "Epoch: 53 | Batch_idx: 350 |  Loss: (0.0992) | Acc: (96.71%) (43450/44928)\n",
            "Epoch: 53 | Batch_idx: 360 |  Loss: (0.1002) | Acc: (96.67%) (44669/46208)\n",
            "Epoch: 53 | Batch_idx: 370 |  Loss: (0.1000) | Acc: (96.67%) (45909/47488)\n",
            "Epoch: 53 | Batch_idx: 380 |  Loss: (0.1006) | Acc: (96.66%) (47137/48768)\n",
            "Epoch: 53 | Batch_idx: 390 |  Loss: (0.1008) | Acc: (96.64%) (48318/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3557) | Acc: (89.82%) (8982/10000)\n",
            "Epoch: 54 | Batch_idx: 0 |  Loss: (0.0505) | Acc: (99.22%) (127/128)\n",
            "Epoch: 54 | Batch_idx: 10 |  Loss: (0.0874) | Acc: (96.95%) (1365/1408)\n",
            "Epoch: 54 | Batch_idx: 20 |  Loss: (0.0913) | Acc: (96.61%) (2597/2688)\n",
            "Epoch: 54 | Batch_idx: 30 |  Loss: (0.0906) | Acc: (96.80%) (3841/3968)\n",
            "Epoch: 54 | Batch_idx: 40 |  Loss: (0.0894) | Acc: (96.91%) (5086/5248)\n",
            "Epoch: 54 | Batch_idx: 50 |  Loss: (0.0917) | Acc: (96.78%) (6318/6528)\n",
            "Epoch: 54 | Batch_idx: 60 |  Loss: (0.0897) | Acc: (96.79%) (7557/7808)\n",
            "Epoch: 54 | Batch_idx: 70 |  Loss: (0.0883) | Acc: (96.93%) (8809/9088)\n",
            "Epoch: 54 | Batch_idx: 80 |  Loss: (0.0897) | Acc: (96.91%) (10048/10368)\n",
            "Epoch: 54 | Batch_idx: 90 |  Loss: (0.0930) | Acc: (96.88%) (11284/11648)\n",
            "Epoch: 54 | Batch_idx: 100 |  Loss: (0.0930) | Acc: (96.89%) (12526/12928)\n",
            "Epoch: 54 | Batch_idx: 110 |  Loss: (0.0922) | Acc: (96.96%) (13776/14208)\n",
            "Epoch: 54 | Batch_idx: 120 |  Loss: (0.0924) | Acc: (96.91%) (15009/15488)\n",
            "Epoch: 54 | Batch_idx: 130 |  Loss: (0.0927) | Acc: (96.90%) (16248/16768)\n",
            "Epoch: 54 | Batch_idx: 140 |  Loss: (0.0942) | Acc: (96.83%) (17475/18048)\n",
            "Epoch: 54 | Batch_idx: 150 |  Loss: (0.0951) | Acc: (96.77%) (18703/19328)\n",
            "Epoch: 54 | Batch_idx: 160 |  Loss: (0.0944) | Acc: (96.76%) (19941/20608)\n",
            "Epoch: 54 | Batch_idx: 170 |  Loss: (0.0946) | Acc: (96.76%) (21179/21888)\n",
            "Epoch: 54 | Batch_idx: 180 |  Loss: (0.0939) | Acc: (96.79%) (22424/23168)\n",
            "Epoch: 54 | Batch_idx: 190 |  Loss: (0.0936) | Acc: (96.81%) (23669/24448)\n",
            "Epoch: 54 | Batch_idx: 200 |  Loss: (0.0937) | Acc: (96.80%) (24905/25728)\n",
            "Epoch: 54 | Batch_idx: 210 |  Loss: (0.0933) | Acc: (96.79%) (26142/27008)\n",
            "Epoch: 54 | Batch_idx: 220 |  Loss: (0.0932) | Acc: (96.77%) (27375/28288)\n",
            "Epoch: 54 | Batch_idx: 230 |  Loss: (0.0937) | Acc: (96.76%) (28609/29568)\n",
            "Epoch: 54 | Batch_idx: 240 |  Loss: (0.0948) | Acc: (96.74%) (29841/30848)\n",
            "Epoch: 54 | Batch_idx: 250 |  Loss: (0.0956) | Acc: (96.69%) (31063/32128)\n",
            "Epoch: 54 | Batch_idx: 260 |  Loss: (0.0952) | Acc: (96.70%) (32305/33408)\n",
            "Epoch: 54 | Batch_idx: 270 |  Loss: (0.0954) | Acc: (96.69%) (33541/34688)\n",
            "Epoch: 54 | Batch_idx: 280 |  Loss: (0.0956) | Acc: (96.69%) (34779/35968)\n",
            "Epoch: 54 | Batch_idx: 290 |  Loss: (0.0956) | Acc: (96.72%) (36025/37248)\n",
            "Epoch: 54 | Batch_idx: 300 |  Loss: (0.0960) | Acc: (96.71%) (37260/38528)\n",
            "Epoch: 54 | Batch_idx: 310 |  Loss: (0.0964) | Acc: (96.69%) (38489/39808)\n",
            "Epoch: 54 | Batch_idx: 320 |  Loss: (0.0972) | Acc: (96.65%) (39713/41088)\n",
            "Epoch: 54 | Batch_idx: 330 |  Loss: (0.0975) | Acc: (96.64%) (40943/42368)\n",
            "Epoch: 54 | Batch_idx: 340 |  Loss: (0.0978) | Acc: (96.62%) (42171/43648)\n",
            "Epoch: 54 | Batch_idx: 350 |  Loss: (0.0978) | Acc: (96.61%) (43403/44928)\n",
            "Epoch: 54 | Batch_idx: 360 |  Loss: (0.0983) | Acc: (96.58%) (44626/46208)\n",
            "Epoch: 54 | Batch_idx: 370 |  Loss: (0.0986) | Acc: (96.58%) (45863/47488)\n",
            "Epoch: 54 | Batch_idx: 380 |  Loss: (0.0988) | Acc: (96.57%) (47096/48768)\n",
            "Epoch: 54 | Batch_idx: 390 |  Loss: (0.0989) | Acc: (96.57%) (48284/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3531) | Acc: (89.17%) (8917/10000)\n",
            "Epoch: 55 | Batch_idx: 0 |  Loss: (0.1479) | Acc: (94.53%) (121/128)\n",
            "Epoch: 55 | Batch_idx: 10 |  Loss: (0.0988) | Acc: (96.52%) (1359/1408)\n",
            "Epoch: 55 | Batch_idx: 20 |  Loss: (0.0987) | Acc: (96.54%) (2595/2688)\n",
            "Epoch: 55 | Batch_idx: 30 |  Loss: (0.0958) | Acc: (96.47%) (3828/3968)\n",
            "Epoch: 55 | Batch_idx: 40 |  Loss: (0.0935) | Acc: (96.63%) (5071/5248)\n",
            "Epoch: 55 | Batch_idx: 50 |  Loss: (0.0959) | Acc: (96.51%) (6300/6528)\n",
            "Epoch: 55 | Batch_idx: 60 |  Loss: (0.0934) | Acc: (96.68%) (7549/7808)\n",
            "Epoch: 55 | Batch_idx: 70 |  Loss: (0.0935) | Acc: (96.75%) (8793/9088)\n",
            "Epoch: 55 | Batch_idx: 80 |  Loss: (0.0942) | Acc: (96.75%) (10031/10368)\n",
            "Epoch: 55 | Batch_idx: 90 |  Loss: (0.0949) | Acc: (96.76%) (11271/11648)\n",
            "Epoch: 55 | Batch_idx: 100 |  Loss: (0.0948) | Acc: (96.81%) (12515/12928)\n",
            "Epoch: 55 | Batch_idx: 110 |  Loss: (0.0935) | Acc: (96.86%) (13762/14208)\n",
            "Epoch: 55 | Batch_idx: 120 |  Loss: (0.0916) | Acc: (96.92%) (15011/15488)\n",
            "Epoch: 55 | Batch_idx: 130 |  Loss: (0.0918) | Acc: (96.90%) (16249/16768)\n",
            "Epoch: 55 | Batch_idx: 140 |  Loss: (0.0913) | Acc: (96.91%) (17491/18048)\n",
            "Epoch: 55 | Batch_idx: 150 |  Loss: (0.0911) | Acc: (96.95%) (18738/19328)\n",
            "Epoch: 55 | Batch_idx: 160 |  Loss: (0.0905) | Acc: (96.96%) (19982/20608)\n",
            "Epoch: 55 | Batch_idx: 170 |  Loss: (0.0909) | Acc: (96.96%) (21223/21888)\n",
            "Epoch: 55 | Batch_idx: 180 |  Loss: (0.0899) | Acc: (97.00%) (22474/23168)\n",
            "Epoch: 55 | Batch_idx: 190 |  Loss: (0.0904) | Acc: (97.00%) (23714/24448)\n",
            "Epoch: 55 | Batch_idx: 200 |  Loss: (0.0905) | Acc: (96.98%) (24951/25728)\n",
            "Epoch: 55 | Batch_idx: 210 |  Loss: (0.0910) | Acc: (96.95%) (26185/27008)\n",
            "Epoch: 55 | Batch_idx: 220 |  Loss: (0.0908) | Acc: (96.97%) (27432/28288)\n",
            "Epoch: 55 | Batch_idx: 230 |  Loss: (0.0915) | Acc: (96.95%) (28666/29568)\n",
            "Epoch: 55 | Batch_idx: 240 |  Loss: (0.0911) | Acc: (96.99%) (29919/30848)\n",
            "Epoch: 55 | Batch_idx: 250 |  Loss: (0.0912) | Acc: (97.00%) (31165/32128)\n",
            "Epoch: 55 | Batch_idx: 260 |  Loss: (0.0915) | Acc: (96.96%) (32394/33408)\n",
            "Epoch: 55 | Batch_idx: 270 |  Loss: (0.0913) | Acc: (96.98%) (33640/34688)\n",
            "Epoch: 55 | Batch_idx: 280 |  Loss: (0.0917) | Acc: (96.99%) (34884/35968)\n",
            "Epoch: 55 | Batch_idx: 290 |  Loss: (0.0921) | Acc: (96.98%) (36122/37248)\n",
            "Epoch: 55 | Batch_idx: 300 |  Loss: (0.0924) | Acc: (96.98%) (37363/38528)\n",
            "Epoch: 55 | Batch_idx: 310 |  Loss: (0.0923) | Acc: (96.98%) (38605/39808)\n",
            "Epoch: 55 | Batch_idx: 320 |  Loss: (0.0920) | Acc: (96.99%) (39853/41088)\n",
            "Epoch: 55 | Batch_idx: 330 |  Loss: (0.0929) | Acc: (96.96%) (41078/42368)\n",
            "Epoch: 55 | Batch_idx: 340 |  Loss: (0.0929) | Acc: (96.95%) (42316/43648)\n",
            "Epoch: 55 | Batch_idx: 350 |  Loss: (0.0940) | Acc: (96.90%) (43536/44928)\n",
            "Epoch: 55 | Batch_idx: 360 |  Loss: (0.0952) | Acc: (96.86%) (44759/46208)\n",
            "Epoch: 55 | Batch_idx: 370 |  Loss: (0.0958) | Acc: (96.82%) (45980/47488)\n",
            "Epoch: 55 | Batch_idx: 380 |  Loss: (0.0963) | Acc: (96.82%) (47215/48768)\n",
            "Epoch: 55 | Batch_idx: 390 |  Loss: (0.0965) | Acc: (96.80%) (48401/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3653) | Acc: (89.29%) (8929/10000)\n",
            "Epoch: 56 | Batch_idx: 0 |  Loss: (0.1053) | Acc: (95.31%) (122/128)\n",
            "Epoch: 56 | Batch_idx: 10 |  Loss: (0.0868) | Acc: (97.16%) (1368/1408)\n",
            "Epoch: 56 | Batch_idx: 20 |  Loss: (0.0866) | Acc: (97.25%) (2614/2688)\n",
            "Epoch: 56 | Batch_idx: 30 |  Loss: (0.0874) | Acc: (97.25%) (3859/3968)\n",
            "Epoch: 56 | Batch_idx: 40 |  Loss: (0.0846) | Acc: (97.37%) (5110/5248)\n",
            "Epoch: 56 | Batch_idx: 50 |  Loss: (0.0829) | Acc: (97.47%) (6363/6528)\n",
            "Epoch: 56 | Batch_idx: 60 |  Loss: (0.0802) | Acc: (97.55%) (7617/7808)\n",
            "Epoch: 56 | Batch_idx: 70 |  Loss: (0.0812) | Acc: (97.51%) (8862/9088)\n",
            "Epoch: 56 | Batch_idx: 80 |  Loss: (0.0832) | Acc: (97.39%) (10097/10368)\n",
            "Epoch: 56 | Batch_idx: 90 |  Loss: (0.0834) | Acc: (97.39%) (11344/11648)\n",
            "Epoch: 56 | Batch_idx: 100 |  Loss: (0.0850) | Acc: (97.28%) (12576/12928)\n",
            "Epoch: 56 | Batch_idx: 110 |  Loss: (0.0857) | Acc: (97.26%) (13818/14208)\n",
            "Epoch: 56 | Batch_idx: 120 |  Loss: (0.0857) | Acc: (97.27%) (15065/15488)\n",
            "Epoch: 56 | Batch_idx: 130 |  Loss: (0.0851) | Acc: (97.30%) (16316/16768)\n",
            "Epoch: 56 | Batch_idx: 140 |  Loss: (0.0857) | Acc: (97.27%) (17556/18048)\n",
            "Epoch: 56 | Batch_idx: 150 |  Loss: (0.0842) | Acc: (97.33%) (18812/19328)\n",
            "Epoch: 56 | Batch_idx: 160 |  Loss: (0.0849) | Acc: (97.29%) (20049/20608)\n",
            "Epoch: 56 | Batch_idx: 170 |  Loss: (0.0850) | Acc: (97.26%) (21289/21888)\n",
            "Epoch: 56 | Batch_idx: 180 |  Loss: (0.0850) | Acc: (97.25%) (22532/23168)\n",
            "Epoch: 56 | Batch_idx: 190 |  Loss: (0.0850) | Acc: (97.25%) (23775/24448)\n",
            "Epoch: 56 | Batch_idx: 200 |  Loss: (0.0851) | Acc: (97.26%) (25022/25728)\n",
            "Epoch: 56 | Batch_idx: 210 |  Loss: (0.0852) | Acc: (97.25%) (26265/27008)\n",
            "Epoch: 56 | Batch_idx: 220 |  Loss: (0.0851) | Acc: (97.24%) (27506/28288)\n",
            "Epoch: 56 | Batch_idx: 230 |  Loss: (0.0854) | Acc: (97.21%) (28742/29568)\n",
            "Epoch: 56 | Batch_idx: 240 |  Loss: (0.0860) | Acc: (97.18%) (29978/30848)\n",
            "Epoch: 56 | Batch_idx: 250 |  Loss: (0.0859) | Acc: (97.18%) (31221/32128)\n",
            "Epoch: 56 | Batch_idx: 260 |  Loss: (0.0858) | Acc: (97.18%) (32467/33408)\n",
            "Epoch: 56 | Batch_idx: 270 |  Loss: (0.0860) | Acc: (97.17%) (33708/34688)\n",
            "Epoch: 56 | Batch_idx: 280 |  Loss: (0.0861) | Acc: (97.15%) (34944/35968)\n",
            "Epoch: 56 | Batch_idx: 290 |  Loss: (0.0863) | Acc: (97.14%) (36182/37248)\n",
            "Epoch: 56 | Batch_idx: 300 |  Loss: (0.0866) | Acc: (97.14%) (37428/38528)\n",
            "Epoch: 56 | Batch_idx: 310 |  Loss: (0.0866) | Acc: (97.14%) (38671/39808)\n",
            "Epoch: 56 | Batch_idx: 320 |  Loss: (0.0873) | Acc: (97.12%) (39903/41088)\n",
            "Epoch: 56 | Batch_idx: 330 |  Loss: (0.0879) | Acc: (97.09%) (41135/42368)\n",
            "Epoch: 56 | Batch_idx: 340 |  Loss: (0.0883) | Acc: (97.06%) (42365/43648)\n",
            "Epoch: 56 | Batch_idx: 350 |  Loss: (0.0889) | Acc: (97.04%) (43596/44928)\n",
            "Epoch: 56 | Batch_idx: 360 |  Loss: (0.0891) | Acc: (97.03%) (44837/46208)\n",
            "Epoch: 56 | Batch_idx: 370 |  Loss: (0.0905) | Acc: (97.00%) (46062/47488)\n",
            "Epoch: 56 | Batch_idx: 380 |  Loss: (0.0905) | Acc: (96.99%) (47302/48768)\n",
            "Epoch: 56 | Batch_idx: 390 |  Loss: (0.0910) | Acc: (96.98%) (48490/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3589) | Acc: (89.93%) (8993/10000)\n",
            "Epoch: 57 | Batch_idx: 0 |  Loss: (0.0700) | Acc: (96.88%) (124/128)\n",
            "Epoch: 57 | Batch_idx: 10 |  Loss: (0.0679) | Acc: (97.59%) (1374/1408)\n",
            "Epoch: 57 | Batch_idx: 20 |  Loss: (0.0758) | Acc: (97.36%) (2617/2688)\n",
            "Epoch: 57 | Batch_idx: 30 |  Loss: (0.0759) | Acc: (97.45%) (3867/3968)\n",
            "Epoch: 57 | Batch_idx: 40 |  Loss: (0.0791) | Acc: (97.35%) (5109/5248)\n",
            "Epoch: 57 | Batch_idx: 50 |  Loss: (0.0774) | Acc: (97.35%) (6355/6528)\n",
            "Epoch: 57 | Batch_idx: 60 |  Loss: (0.0800) | Acc: (97.30%) (7597/7808)\n",
            "Epoch: 57 | Batch_idx: 70 |  Loss: (0.0814) | Acc: (97.27%) (8840/9088)\n",
            "Epoch: 57 | Batch_idx: 80 |  Loss: (0.0796) | Acc: (97.37%) (10095/10368)\n",
            "Epoch: 57 | Batch_idx: 90 |  Loss: (0.0799) | Acc: (97.34%) (11338/11648)\n",
            "Epoch: 57 | Batch_idx: 100 |  Loss: (0.0792) | Acc: (97.35%) (12586/12928)\n",
            "Epoch: 57 | Batch_idx: 110 |  Loss: (0.0787) | Acc: (97.37%) (13835/14208)\n",
            "Epoch: 57 | Batch_idx: 120 |  Loss: (0.0800) | Acc: (97.33%) (15075/15488)\n",
            "Epoch: 57 | Batch_idx: 130 |  Loss: (0.0810) | Acc: (97.30%) (16315/16768)\n",
            "Epoch: 57 | Batch_idx: 140 |  Loss: (0.0813) | Acc: (97.30%) (17561/18048)\n",
            "Epoch: 57 | Batch_idx: 150 |  Loss: (0.0825) | Acc: (97.25%) (18797/19328)\n",
            "Epoch: 57 | Batch_idx: 160 |  Loss: (0.0835) | Acc: (97.20%) (20032/20608)\n",
            "Epoch: 57 | Batch_idx: 170 |  Loss: (0.0835) | Acc: (97.20%) (21276/21888)\n",
            "Epoch: 57 | Batch_idx: 180 |  Loss: (0.0849) | Acc: (97.16%) (22509/23168)\n",
            "Epoch: 57 | Batch_idx: 190 |  Loss: (0.0855) | Acc: (97.14%) (23750/24448)\n",
            "Epoch: 57 | Batch_idx: 200 |  Loss: (0.0865) | Acc: (97.09%) (24980/25728)\n",
            "Epoch: 57 | Batch_idx: 210 |  Loss: (0.0874) | Acc: (97.04%) (26208/27008)\n",
            "Epoch: 57 | Batch_idx: 220 |  Loss: (0.0870) | Acc: (97.06%) (27455/28288)\n",
            "Epoch: 57 | Batch_idx: 230 |  Loss: (0.0881) | Acc: (96.99%) (28678/29568)\n",
            "Epoch: 57 | Batch_idx: 240 |  Loss: (0.0885) | Acc: (96.98%) (29915/30848)\n",
            "Epoch: 57 | Batch_idx: 250 |  Loss: (0.0895) | Acc: (96.95%) (31149/32128)\n",
            "Epoch: 57 | Batch_idx: 260 |  Loss: (0.0895) | Acc: (96.94%) (32387/33408)\n",
            "Epoch: 57 | Batch_idx: 270 |  Loss: (0.0901) | Acc: (96.93%) (33622/34688)\n",
            "Epoch: 57 | Batch_idx: 280 |  Loss: (0.0908) | Acc: (96.93%) (34864/35968)\n",
            "Epoch: 57 | Batch_idx: 290 |  Loss: (0.0909) | Acc: (96.93%) (36103/37248)\n",
            "Epoch: 57 | Batch_idx: 300 |  Loss: (0.0911) | Acc: (96.92%) (37341/38528)\n",
            "Epoch: 57 | Batch_idx: 310 |  Loss: (0.0914) | Acc: (96.92%) (38580/39808)\n",
            "Epoch: 57 | Batch_idx: 320 |  Loss: (0.0911) | Acc: (96.92%) (39823/41088)\n",
            "Epoch: 57 | Batch_idx: 330 |  Loss: (0.0920) | Acc: (96.89%) (41052/42368)\n",
            "Epoch: 57 | Batch_idx: 340 |  Loss: (0.0920) | Acc: (96.90%) (42294/43648)\n",
            "Epoch: 57 | Batch_idx: 350 |  Loss: (0.0924) | Acc: (96.88%) (43524/44928)\n",
            "Epoch: 57 | Batch_idx: 360 |  Loss: (0.0923) | Acc: (96.89%) (44770/46208)\n",
            "Epoch: 57 | Batch_idx: 370 |  Loss: (0.0921) | Acc: (96.90%) (46015/47488)\n",
            "Epoch: 57 | Batch_idx: 380 |  Loss: (0.0924) | Acc: (96.88%) (47248/48768)\n",
            "Epoch: 57 | Batch_idx: 390 |  Loss: (0.0924) | Acc: (96.89%) (48447/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3999) | Acc: (88.83%) (8883/10000)\n",
            "Epoch: 58 | Batch_idx: 0 |  Loss: (0.0782) | Acc: (96.88%) (124/128)\n",
            "Epoch: 58 | Batch_idx: 10 |  Loss: (0.1007) | Acc: (96.52%) (1359/1408)\n",
            "Epoch: 58 | Batch_idx: 20 |  Loss: (0.0905) | Acc: (96.91%) (2605/2688)\n",
            "Epoch: 58 | Batch_idx: 30 |  Loss: (0.0897) | Acc: (96.90%) (3845/3968)\n",
            "Epoch: 58 | Batch_idx: 40 |  Loss: (0.0846) | Acc: (97.12%) (5097/5248)\n",
            "Epoch: 58 | Batch_idx: 50 |  Loss: (0.0840) | Acc: (97.18%) (6344/6528)\n",
            "Epoch: 58 | Batch_idx: 60 |  Loss: (0.0828) | Acc: (97.28%) (7596/7808)\n",
            "Epoch: 58 | Batch_idx: 70 |  Loss: (0.0823) | Acc: (97.28%) (8841/9088)\n",
            "Epoch: 58 | Batch_idx: 80 |  Loss: (0.0806) | Acc: (97.32%) (10090/10368)\n",
            "Epoch: 58 | Batch_idx: 90 |  Loss: (0.0799) | Acc: (97.36%) (11340/11648)\n",
            "Epoch: 58 | Batch_idx: 100 |  Loss: (0.0792) | Acc: (97.41%) (12593/12928)\n",
            "Epoch: 58 | Batch_idx: 110 |  Loss: (0.0797) | Acc: (97.42%) (13841/14208)\n",
            "Epoch: 58 | Batch_idx: 120 |  Loss: (0.0808) | Acc: (97.36%) (15079/15488)\n",
            "Epoch: 58 | Batch_idx: 130 |  Loss: (0.0805) | Acc: (97.32%) (16319/16768)\n",
            "Epoch: 58 | Batch_idx: 140 |  Loss: (0.0815) | Acc: (97.30%) (17561/18048)\n",
            "Epoch: 58 | Batch_idx: 150 |  Loss: (0.0814) | Acc: (97.28%) (18803/19328)\n",
            "Epoch: 58 | Batch_idx: 160 |  Loss: (0.0826) | Acc: (97.24%) (20039/20608)\n",
            "Epoch: 58 | Batch_idx: 170 |  Loss: (0.0826) | Acc: (97.22%) (21279/21888)\n",
            "Epoch: 58 | Batch_idx: 180 |  Loss: (0.0830) | Acc: (97.18%) (22514/23168)\n",
            "Epoch: 58 | Batch_idx: 190 |  Loss: (0.0842) | Acc: (97.16%) (23754/24448)\n",
            "Epoch: 58 | Batch_idx: 200 |  Loss: (0.0840) | Acc: (97.17%) (24999/25728)\n",
            "Epoch: 58 | Batch_idx: 210 |  Loss: (0.0840) | Acc: (97.16%) (26242/27008)\n",
            "Epoch: 58 | Batch_idx: 220 |  Loss: (0.0846) | Acc: (97.15%) (27482/28288)\n",
            "Epoch: 58 | Batch_idx: 230 |  Loss: (0.0855) | Acc: (97.12%) (28715/29568)\n",
            "Epoch: 58 | Batch_idx: 240 |  Loss: (0.0853) | Acc: (97.13%) (29963/30848)\n",
            "Epoch: 58 | Batch_idx: 250 |  Loss: (0.0864) | Acc: (97.08%) (31190/32128)\n",
            "Epoch: 58 | Batch_idx: 260 |  Loss: (0.0860) | Acc: (97.10%) (32440/33408)\n",
            "Epoch: 58 | Batch_idx: 270 |  Loss: (0.0861) | Acc: (97.11%) (33684/34688)\n",
            "Epoch: 58 | Batch_idx: 280 |  Loss: (0.0867) | Acc: (97.08%) (34917/35968)\n",
            "Epoch: 58 | Batch_idx: 290 |  Loss: (0.0866) | Acc: (97.08%) (36162/37248)\n",
            "Epoch: 58 | Batch_idx: 300 |  Loss: (0.0875) | Acc: (97.07%) (37399/38528)\n",
            "Epoch: 58 | Batch_idx: 310 |  Loss: (0.0878) | Acc: (97.06%) (38638/39808)\n",
            "Epoch: 58 | Batch_idx: 320 |  Loss: (0.0877) | Acc: (97.07%) (39883/41088)\n",
            "Epoch: 58 | Batch_idx: 330 |  Loss: (0.0878) | Acc: (97.06%) (41124/42368)\n",
            "Epoch: 58 | Batch_idx: 340 |  Loss: (0.0879) | Acc: (97.07%) (42371/43648)\n",
            "Epoch: 58 | Batch_idx: 350 |  Loss: (0.0882) | Acc: (97.06%) (43609/44928)\n",
            "Epoch: 58 | Batch_idx: 360 |  Loss: (0.0886) | Acc: (97.05%) (44845/46208)\n",
            "Epoch: 58 | Batch_idx: 370 |  Loss: (0.0890) | Acc: (97.04%) (46080/47488)\n",
            "Epoch: 58 | Batch_idx: 380 |  Loss: (0.0893) | Acc: (97.01%) (47312/48768)\n",
            "Epoch: 58 | Batch_idx: 390 |  Loss: (0.0897) | Acc: (96.99%) (48495/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4054) | Acc: (88.57%) (8857/10000)\n",
            "Epoch: 59 | Batch_idx: 0 |  Loss: (0.1153) | Acc: (93.75%) (120/128)\n",
            "Epoch: 59 | Batch_idx: 10 |  Loss: (0.0904) | Acc: (96.59%) (1360/1408)\n",
            "Epoch: 59 | Batch_idx: 20 |  Loss: (0.0992) | Acc: (96.73%) (2600/2688)\n",
            "Epoch: 59 | Batch_idx: 30 |  Loss: (0.0988) | Acc: (96.82%) (3842/3968)\n",
            "Epoch: 59 | Batch_idx: 40 |  Loss: (0.1013) | Acc: (96.57%) (5068/5248)\n",
            "Epoch: 59 | Batch_idx: 50 |  Loss: (0.0981) | Acc: (96.72%) (6314/6528)\n",
            "Epoch: 59 | Batch_idx: 60 |  Loss: (0.0960) | Acc: (96.88%) (7564/7808)\n",
            "Epoch: 59 | Batch_idx: 70 |  Loss: (0.0974) | Acc: (96.82%) (8799/9088)\n",
            "Epoch: 59 | Batch_idx: 80 |  Loss: (0.0981) | Acc: (96.80%) (10036/10368)\n",
            "Epoch: 59 | Batch_idx: 90 |  Loss: (0.0956) | Acc: (96.91%) (11288/11648)\n",
            "Epoch: 59 | Batch_idx: 100 |  Loss: (0.0954) | Acc: (96.90%) (12527/12928)\n",
            "Epoch: 59 | Batch_idx: 110 |  Loss: (0.0963) | Acc: (96.89%) (13766/14208)\n",
            "Epoch: 59 | Batch_idx: 120 |  Loss: (0.0963) | Acc: (96.89%) (15007/15488)\n",
            "Epoch: 59 | Batch_idx: 130 |  Loss: (0.0957) | Acc: (96.95%) (16257/16768)\n",
            "Epoch: 59 | Batch_idx: 140 |  Loss: (0.0956) | Acc: (96.95%) (17497/18048)\n",
            "Epoch: 59 | Batch_idx: 150 |  Loss: (0.0965) | Acc: (96.91%) (18730/19328)\n",
            "Epoch: 59 | Batch_idx: 160 |  Loss: (0.0953) | Acc: (96.96%) (19981/20608)\n",
            "Epoch: 59 | Batch_idx: 170 |  Loss: (0.0955) | Acc: (96.95%) (21220/21888)\n",
            "Epoch: 59 | Batch_idx: 180 |  Loss: (0.0951) | Acc: (96.94%) (22458/23168)\n",
            "Epoch: 59 | Batch_idx: 190 |  Loss: (0.0945) | Acc: (96.96%) (23704/24448)\n",
            "Epoch: 59 | Batch_idx: 200 |  Loss: (0.0941) | Acc: (96.97%) (24948/25728)\n",
            "Epoch: 59 | Batch_idx: 210 |  Loss: (0.0938) | Acc: (96.98%) (26192/27008)\n",
            "Epoch: 59 | Batch_idx: 220 |  Loss: (0.0946) | Acc: (96.92%) (27418/28288)\n",
            "Epoch: 59 | Batch_idx: 230 |  Loss: (0.0942) | Acc: (96.93%) (28659/29568)\n",
            "Epoch: 59 | Batch_idx: 240 |  Loss: (0.0952) | Acc: (96.87%) (29883/30848)\n",
            "Epoch: 59 | Batch_idx: 250 |  Loss: (0.0951) | Acc: (96.86%) (31120/32128)\n",
            "Epoch: 59 | Batch_idx: 260 |  Loss: (0.0959) | Acc: (96.83%) (32350/33408)\n",
            "Epoch: 59 | Batch_idx: 270 |  Loss: (0.0962) | Acc: (96.81%) (33582/34688)\n",
            "Epoch: 59 | Batch_idx: 280 |  Loss: (0.0966) | Acc: (96.79%) (34812/35968)\n",
            "Epoch: 59 | Batch_idx: 290 |  Loss: (0.0968) | Acc: (96.77%) (36045/37248)\n",
            "Epoch: 59 | Batch_idx: 300 |  Loss: (0.0971) | Acc: (96.76%) (37278/38528)\n",
            "Epoch: 59 | Batch_idx: 310 |  Loss: (0.0971) | Acc: (96.75%) (38514/39808)\n",
            "Epoch: 59 | Batch_idx: 320 |  Loss: (0.0972) | Acc: (96.74%) (39748/41088)\n",
            "Epoch: 59 | Batch_idx: 330 |  Loss: (0.0969) | Acc: (96.75%) (40991/42368)\n",
            "Epoch: 59 | Batch_idx: 340 |  Loss: (0.0966) | Acc: (96.77%) (42236/43648)\n",
            "Epoch: 59 | Batch_idx: 350 |  Loss: (0.0968) | Acc: (96.77%) (43478/44928)\n",
            "Epoch: 59 | Batch_idx: 360 |  Loss: (0.0964) | Acc: (96.79%) (44726/46208)\n",
            "Epoch: 59 | Batch_idx: 370 |  Loss: (0.0964) | Acc: (96.79%) (45965/47488)\n",
            "Epoch: 59 | Batch_idx: 380 |  Loss: (0.0965) | Acc: (96.80%) (47205/48768)\n",
            "Epoch: 59 | Batch_idx: 390 |  Loss: (0.0969) | Acc: (96.79%) (48393/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3847) | Acc: (88.88%) (8888/10000)\n",
            "Epoch: 60 | Batch_idx: 0 |  Loss: (0.0407) | Acc: (100.00%) (128/128)\n",
            "Epoch: 60 | Batch_idx: 10 |  Loss: (0.0904) | Acc: (97.09%) (1367/1408)\n",
            "Epoch: 60 | Batch_idx: 20 |  Loss: (0.0828) | Acc: (97.28%) (2615/2688)\n",
            "Epoch: 60 | Batch_idx: 30 |  Loss: (0.0822) | Acc: (97.38%) (3864/3968)\n",
            "Epoch: 60 | Batch_idx: 40 |  Loss: (0.0806) | Acc: (97.37%) (5110/5248)\n",
            "Epoch: 60 | Batch_idx: 50 |  Loss: (0.0825) | Acc: (97.21%) (6346/6528)\n",
            "Epoch: 60 | Batch_idx: 60 |  Loss: (0.0842) | Acc: (97.20%) (7589/7808)\n",
            "Epoch: 60 | Batch_idx: 70 |  Loss: (0.0843) | Acc: (97.17%) (8831/9088)\n",
            "Epoch: 60 | Batch_idx: 80 |  Loss: (0.0838) | Acc: (97.17%) (10075/10368)\n",
            "Epoch: 60 | Batch_idx: 90 |  Loss: (0.0836) | Acc: (97.17%) (11318/11648)\n",
            "Epoch: 60 | Batch_idx: 100 |  Loss: (0.0839) | Acc: (97.12%) (12556/12928)\n",
            "Epoch: 60 | Batch_idx: 110 |  Loss: (0.0853) | Acc: (97.08%) (13793/14208)\n",
            "Epoch: 60 | Batch_idx: 120 |  Loss: (0.0867) | Acc: (97.01%) (15025/15488)\n",
            "Epoch: 60 | Batch_idx: 130 |  Loss: (0.0878) | Acc: (96.99%) (16263/16768)\n",
            "Epoch: 60 | Batch_idx: 140 |  Loss: (0.0873) | Acc: (97.00%) (17507/18048)\n",
            "Epoch: 60 | Batch_idx: 150 |  Loss: (0.0866) | Acc: (97.06%) (18760/19328)\n",
            "Epoch: 60 | Batch_idx: 160 |  Loss: (0.0878) | Acc: (97.04%) (19997/20608)\n",
            "Epoch: 60 | Batch_idx: 170 |  Loss: (0.0882) | Acc: (96.99%) (21230/21888)\n",
            "Epoch: 60 | Batch_idx: 180 |  Loss: (0.0890) | Acc: (96.98%) (22469/23168)\n",
            "Epoch: 60 | Batch_idx: 190 |  Loss: (0.0889) | Acc: (96.98%) (23710/24448)\n",
            "Epoch: 60 | Batch_idx: 200 |  Loss: (0.0892) | Acc: (96.96%) (24946/25728)\n",
            "Epoch: 60 | Batch_idx: 210 |  Loss: (0.0893) | Acc: (96.96%) (26186/27008)\n",
            "Epoch: 60 | Batch_idx: 220 |  Loss: (0.0896) | Acc: (96.96%) (27429/28288)\n",
            "Epoch: 60 | Batch_idx: 230 |  Loss: (0.0895) | Acc: (96.98%) (28674/29568)\n",
            "Epoch: 60 | Batch_idx: 240 |  Loss: (0.0896) | Acc: (96.97%) (29912/30848)\n",
            "Epoch: 60 | Batch_idx: 250 |  Loss: (0.0899) | Acc: (96.95%) (31148/32128)\n",
            "Epoch: 60 | Batch_idx: 260 |  Loss: (0.0899) | Acc: (96.96%) (32393/33408)\n",
            "Epoch: 60 | Batch_idx: 270 |  Loss: (0.0904) | Acc: (96.94%) (33628/34688)\n",
            "Epoch: 60 | Batch_idx: 280 |  Loss: (0.0907) | Acc: (96.93%) (34862/35968)\n",
            "Epoch: 60 | Batch_idx: 290 |  Loss: (0.0903) | Acc: (96.93%) (36103/37248)\n",
            "Epoch: 60 | Batch_idx: 300 |  Loss: (0.0900) | Acc: (96.94%) (37350/38528)\n",
            "Epoch: 60 | Batch_idx: 310 |  Loss: (0.0903) | Acc: (96.93%) (38587/39808)\n",
            "Epoch: 60 | Batch_idx: 320 |  Loss: (0.0902) | Acc: (96.93%) (39825/41088)\n",
            "Epoch: 60 | Batch_idx: 330 |  Loss: (0.0901) | Acc: (96.92%) (41062/42368)\n",
            "Epoch: 60 | Batch_idx: 340 |  Loss: (0.0907) | Acc: (96.90%) (42293/43648)\n",
            "Epoch: 60 | Batch_idx: 350 |  Loss: (0.0911) | Acc: (96.88%) (43526/44928)\n",
            "Epoch: 60 | Batch_idx: 360 |  Loss: (0.0909) | Acc: (96.89%) (44772/46208)\n",
            "Epoch: 60 | Batch_idx: 370 |  Loss: (0.0909) | Acc: (96.89%) (46012/47488)\n",
            "Epoch: 60 | Batch_idx: 380 |  Loss: (0.0909) | Acc: (96.89%) (47253/48768)\n",
            "Epoch: 60 | Batch_idx: 390 |  Loss: (0.0912) | Acc: (96.88%) (48440/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3599) | Acc: (89.46%) (8946/10000)\n",
            "Epoch: 61 | Batch_idx: 0 |  Loss: (0.0635) | Acc: (98.44%) (126/128)\n",
            "Epoch: 61 | Batch_idx: 10 |  Loss: (0.0782) | Acc: (97.66%) (1375/1408)\n",
            "Epoch: 61 | Batch_idx: 20 |  Loss: (0.0814) | Acc: (97.51%) (2621/2688)\n",
            "Epoch: 61 | Batch_idx: 30 |  Loss: (0.0790) | Acc: (97.48%) (3868/3968)\n",
            "Epoch: 61 | Batch_idx: 40 |  Loss: (0.0833) | Acc: (97.39%) (5111/5248)\n",
            "Epoch: 61 | Batch_idx: 50 |  Loss: (0.0814) | Acc: (97.41%) (6359/6528)\n",
            "Epoch: 61 | Batch_idx: 60 |  Loss: (0.0815) | Acc: (97.45%) (7609/7808)\n",
            "Epoch: 61 | Batch_idx: 70 |  Loss: (0.0847) | Acc: (97.33%) (8845/9088)\n",
            "Epoch: 61 | Batch_idx: 80 |  Loss: (0.0837) | Acc: (97.34%) (10092/10368)\n",
            "Epoch: 61 | Batch_idx: 90 |  Loss: (0.0838) | Acc: (97.35%) (11339/11648)\n",
            "Epoch: 61 | Batch_idx: 100 |  Loss: (0.0832) | Acc: (97.35%) (12585/12928)\n",
            "Epoch: 61 | Batch_idx: 110 |  Loss: (0.0821) | Acc: (97.39%) (13837/14208)\n",
            "Epoch: 61 | Batch_idx: 120 |  Loss: (0.0825) | Acc: (97.37%) (15080/15488)\n",
            "Epoch: 61 | Batch_idx: 130 |  Loss: (0.0803) | Acc: (97.47%) (16343/16768)\n",
            "Epoch: 61 | Batch_idx: 140 |  Loss: (0.0788) | Acc: (97.52%) (17600/18048)\n",
            "Epoch: 61 | Batch_idx: 150 |  Loss: (0.0784) | Acc: (97.50%) (18844/19328)\n",
            "Epoch: 61 | Batch_idx: 160 |  Loss: (0.0779) | Acc: (97.51%) (20095/20608)\n",
            "Epoch: 61 | Batch_idx: 170 |  Loss: (0.0777) | Acc: (97.53%) (21348/21888)\n",
            "Epoch: 61 | Batch_idx: 180 |  Loss: (0.0776) | Acc: (97.51%) (22592/23168)\n",
            "Epoch: 61 | Batch_idx: 190 |  Loss: (0.0777) | Acc: (97.51%) (23839/24448)\n",
            "Epoch: 61 | Batch_idx: 200 |  Loss: (0.0781) | Acc: (97.50%) (25086/25728)\n",
            "Epoch: 61 | Batch_idx: 210 |  Loss: (0.0793) | Acc: (97.46%) (26323/27008)\n",
            "Epoch: 61 | Batch_idx: 220 |  Loss: (0.0797) | Acc: (97.45%) (27566/28288)\n",
            "Epoch: 61 | Batch_idx: 230 |  Loss: (0.0801) | Acc: (97.42%) (28806/29568)\n",
            "Epoch: 61 | Batch_idx: 240 |  Loss: (0.0812) | Acc: (97.38%) (30041/30848)\n",
            "Epoch: 61 | Batch_idx: 250 |  Loss: (0.0818) | Acc: (97.35%) (31278/32128)\n",
            "Epoch: 61 | Batch_idx: 260 |  Loss: (0.0830) | Acc: (97.32%) (32511/33408)\n",
            "Epoch: 61 | Batch_idx: 270 |  Loss: (0.0834) | Acc: (97.29%) (33747/34688)\n",
            "Epoch: 61 | Batch_idx: 280 |  Loss: (0.0838) | Acc: (97.28%) (34988/35968)\n",
            "Epoch: 61 | Batch_idx: 290 |  Loss: (0.0842) | Acc: (97.26%) (36227/37248)\n",
            "Epoch: 61 | Batch_idx: 300 |  Loss: (0.0847) | Acc: (97.24%) (37466/38528)\n",
            "Epoch: 61 | Batch_idx: 310 |  Loss: (0.0850) | Acc: (97.23%) (38705/39808)\n",
            "Epoch: 61 | Batch_idx: 320 |  Loss: (0.0854) | Acc: (97.19%) (39935/41088)\n",
            "Epoch: 61 | Batch_idx: 330 |  Loss: (0.0855) | Acc: (97.18%) (41173/42368)\n",
            "Epoch: 61 | Batch_idx: 340 |  Loss: (0.0866) | Acc: (97.15%) (42402/43648)\n",
            "Epoch: 61 | Batch_idx: 350 |  Loss: (0.0866) | Acc: (97.14%) (43645/44928)\n",
            "Epoch: 61 | Batch_idx: 360 |  Loss: (0.0865) | Acc: (97.15%) (44891/46208)\n",
            "Epoch: 61 | Batch_idx: 370 |  Loss: (0.0868) | Acc: (97.14%) (46128/47488)\n",
            "Epoch: 61 | Batch_idx: 380 |  Loss: (0.0875) | Acc: (97.11%) (47359/48768)\n",
            "Epoch: 61 | Batch_idx: 390 |  Loss: (0.0882) | Acc: (97.08%) (48539/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4839) | Acc: (86.79%) (8679/10000)\n",
            "Epoch: 62 | Batch_idx: 0 |  Loss: (0.0821) | Acc: (96.88%) (124/128)\n",
            "Epoch: 62 | Batch_idx: 10 |  Loss: (0.0737) | Acc: (97.44%) (1372/1408)\n",
            "Epoch: 62 | Batch_idx: 20 |  Loss: (0.0834) | Acc: (97.51%) (2621/2688)\n",
            "Epoch: 62 | Batch_idx: 30 |  Loss: (0.0786) | Acc: (97.61%) (3873/3968)\n",
            "Epoch: 62 | Batch_idx: 40 |  Loss: (0.0773) | Acc: (97.60%) (5122/5248)\n",
            "Epoch: 62 | Batch_idx: 50 |  Loss: (0.0763) | Acc: (97.66%) (6375/6528)\n",
            "Epoch: 62 | Batch_idx: 60 |  Loss: (0.0760) | Acc: (97.59%) (7620/7808)\n",
            "Epoch: 62 | Batch_idx: 70 |  Loss: (0.0756) | Acc: (97.58%) (8868/9088)\n",
            "Epoch: 62 | Batch_idx: 80 |  Loss: (0.0762) | Acc: (97.54%) (10113/10368)\n",
            "Epoch: 62 | Batch_idx: 90 |  Loss: (0.0761) | Acc: (97.52%) (11359/11648)\n",
            "Epoch: 62 | Batch_idx: 100 |  Loss: (0.0768) | Acc: (97.50%) (12605/12928)\n",
            "Epoch: 62 | Batch_idx: 110 |  Loss: (0.0765) | Acc: (97.52%) (13856/14208)\n",
            "Epoch: 62 | Batch_idx: 120 |  Loss: (0.0767) | Acc: (97.48%) (15097/15488)\n",
            "Epoch: 62 | Batch_idx: 130 |  Loss: (0.0757) | Acc: (97.55%) (16357/16768)\n",
            "Epoch: 62 | Batch_idx: 140 |  Loss: (0.0763) | Acc: (97.50%) (17597/18048)\n",
            "Epoch: 62 | Batch_idx: 150 |  Loss: (0.0771) | Acc: (97.48%) (18841/19328)\n",
            "Epoch: 62 | Batch_idx: 160 |  Loss: (0.0772) | Acc: (97.48%) (20088/20608)\n",
            "Epoch: 62 | Batch_idx: 170 |  Loss: (0.0783) | Acc: (97.46%) (21331/21888)\n",
            "Epoch: 62 | Batch_idx: 180 |  Loss: (0.0795) | Acc: (97.38%) (22562/23168)\n",
            "Epoch: 62 | Batch_idx: 190 |  Loss: (0.0799) | Acc: (97.35%) (23800/24448)\n",
            "Epoch: 62 | Batch_idx: 200 |  Loss: (0.0804) | Acc: (97.35%) (25045/25728)\n",
            "Epoch: 62 | Batch_idx: 210 |  Loss: (0.0803) | Acc: (97.35%) (26293/27008)\n",
            "Epoch: 62 | Batch_idx: 220 |  Loss: (0.0803) | Acc: (97.35%) (27539/28288)\n",
            "Epoch: 62 | Batch_idx: 230 |  Loss: (0.0805) | Acc: (97.35%) (28783/29568)\n",
            "Epoch: 62 | Batch_idx: 240 |  Loss: (0.0805) | Acc: (97.32%) (30022/30848)\n",
            "Epoch: 62 | Batch_idx: 250 |  Loss: (0.0810) | Acc: (97.29%) (31257/32128)\n",
            "Epoch: 62 | Batch_idx: 260 |  Loss: (0.0810) | Acc: (97.29%) (32503/33408)\n",
            "Epoch: 62 | Batch_idx: 270 |  Loss: (0.0816) | Acc: (97.27%) (33742/34688)\n",
            "Epoch: 62 | Batch_idx: 280 |  Loss: (0.0818) | Acc: (97.28%) (34988/35968)\n",
            "Epoch: 62 | Batch_idx: 290 |  Loss: (0.0819) | Acc: (97.27%) (36232/37248)\n",
            "Epoch: 62 | Batch_idx: 300 |  Loss: (0.0823) | Acc: (97.26%) (37472/38528)\n",
            "Epoch: 62 | Batch_idx: 310 |  Loss: (0.0825) | Acc: (97.25%) (38713/39808)\n",
            "Epoch: 62 | Batch_idx: 320 |  Loss: (0.0826) | Acc: (97.24%) (39953/41088)\n",
            "Epoch: 62 | Batch_idx: 330 |  Loss: (0.0829) | Acc: (97.22%) (41191/42368)\n",
            "Epoch: 62 | Batch_idx: 340 |  Loss: (0.0837) | Acc: (97.19%) (42420/43648)\n",
            "Epoch: 62 | Batch_idx: 350 |  Loss: (0.0844) | Acc: (97.14%) (43645/44928)\n",
            "Epoch: 62 | Batch_idx: 360 |  Loss: (0.0849) | Acc: (97.13%) (44881/46208)\n",
            "Epoch: 62 | Batch_idx: 370 |  Loss: (0.0849) | Acc: (97.14%) (46128/47488)\n",
            "Epoch: 62 | Batch_idx: 380 |  Loss: (0.0851) | Acc: (97.12%) (47362/48768)\n",
            "Epoch: 62 | Batch_idx: 390 |  Loss: (0.0852) | Acc: (97.13%) (48564/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3245) | Acc: (90.63%) (9063/10000)\n",
            "Epoch: 63 | Batch_idx: 0 |  Loss: (0.1028) | Acc: (98.44%) (126/128)\n",
            "Epoch: 63 | Batch_idx: 10 |  Loss: (0.0767) | Acc: (97.66%) (1375/1408)\n",
            "Epoch: 63 | Batch_idx: 20 |  Loss: (0.0845) | Acc: (97.17%) (2612/2688)\n",
            "Epoch: 63 | Batch_idx: 30 |  Loss: (0.0802) | Acc: (97.43%) (3866/3968)\n",
            "Epoch: 63 | Batch_idx: 40 |  Loss: (0.0769) | Acc: (97.54%) (5119/5248)\n",
            "Epoch: 63 | Batch_idx: 50 |  Loss: (0.0756) | Acc: (97.56%) (6369/6528)\n",
            "Epoch: 63 | Batch_idx: 60 |  Loss: (0.0761) | Acc: (97.50%) (7613/7808)\n",
            "Epoch: 63 | Batch_idx: 70 |  Loss: (0.0749) | Acc: (97.54%) (8864/9088)\n",
            "Epoch: 63 | Batch_idx: 80 |  Loss: (0.0762) | Acc: (97.51%) (10110/10368)\n",
            "Epoch: 63 | Batch_idx: 90 |  Loss: (0.0766) | Acc: (97.50%) (11357/11648)\n",
            "Epoch: 63 | Batch_idx: 100 |  Loss: (0.0766) | Acc: (97.51%) (12606/12928)\n",
            "Epoch: 63 | Batch_idx: 110 |  Loss: (0.0779) | Acc: (97.42%) (13841/14208)\n",
            "Epoch: 63 | Batch_idx: 120 |  Loss: (0.0792) | Acc: (97.35%) (15077/15488)\n",
            "Epoch: 63 | Batch_idx: 130 |  Loss: (0.0797) | Acc: (97.32%) (16319/16768)\n",
            "Epoch: 63 | Batch_idx: 140 |  Loss: (0.0794) | Acc: (97.35%) (17570/18048)\n",
            "Epoch: 63 | Batch_idx: 150 |  Loss: (0.0796) | Acc: (97.34%) (18813/19328)\n",
            "Epoch: 63 | Batch_idx: 160 |  Loss: (0.0797) | Acc: (97.33%) (20058/20608)\n",
            "Epoch: 63 | Batch_idx: 170 |  Loss: (0.0803) | Acc: (97.32%) (21301/21888)\n",
            "Epoch: 63 | Batch_idx: 180 |  Loss: (0.0804) | Acc: (97.33%) (22550/23168)\n",
            "Epoch: 63 | Batch_idx: 190 |  Loss: (0.0798) | Acc: (97.36%) (23803/24448)\n",
            "Epoch: 63 | Batch_idx: 200 |  Loss: (0.0798) | Acc: (97.36%) (25048/25728)\n",
            "Epoch: 63 | Batch_idx: 210 |  Loss: (0.0790) | Acc: (97.40%) (26305/27008)\n",
            "Epoch: 63 | Batch_idx: 220 |  Loss: (0.0791) | Acc: (97.39%) (27550/28288)\n",
            "Epoch: 63 | Batch_idx: 230 |  Loss: (0.0793) | Acc: (97.38%) (28792/29568)\n",
            "Epoch: 63 | Batch_idx: 240 |  Loss: (0.0794) | Acc: (97.38%) (30039/30848)\n",
            "Epoch: 63 | Batch_idx: 250 |  Loss: (0.0792) | Acc: (97.39%) (31288/32128)\n",
            "Epoch: 63 | Batch_idx: 260 |  Loss: (0.0793) | Acc: (97.38%) (32532/33408)\n",
            "Epoch: 63 | Batch_idx: 270 |  Loss: (0.0797) | Acc: (97.37%) (33774/34688)\n",
            "Epoch: 63 | Batch_idx: 280 |  Loss: (0.0795) | Acc: (97.38%) (35024/35968)\n",
            "Epoch: 63 | Batch_idx: 290 |  Loss: (0.0790) | Acc: (97.40%) (36278/37248)\n",
            "Epoch: 63 | Batch_idx: 300 |  Loss: (0.0790) | Acc: (97.40%) (37526/38528)\n",
            "Epoch: 63 | Batch_idx: 310 |  Loss: (0.0795) | Acc: (97.38%) (38765/39808)\n",
            "Epoch: 63 | Batch_idx: 320 |  Loss: (0.0796) | Acc: (97.36%) (40002/41088)\n",
            "Epoch: 63 | Batch_idx: 330 |  Loss: (0.0805) | Acc: (97.32%) (41234/42368)\n",
            "Epoch: 63 | Batch_idx: 340 |  Loss: (0.0806) | Acc: (97.32%) (42480/43648)\n",
            "Epoch: 63 | Batch_idx: 350 |  Loss: (0.0808) | Acc: (97.32%) (43722/44928)\n",
            "Epoch: 63 | Batch_idx: 360 |  Loss: (0.0811) | Acc: (97.29%) (44958/46208)\n",
            "Epoch: 63 | Batch_idx: 370 |  Loss: (0.0822) | Acc: (97.25%) (46181/47488)\n",
            "Epoch: 63 | Batch_idx: 380 |  Loss: (0.0828) | Acc: (97.23%) (47415/48768)\n",
            "Epoch: 63 | Batch_idx: 390 |  Loss: (0.0829) | Acc: (97.23%) (48614/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3549) | Acc: (89.86%) (8986/10000)\n",
            "Epoch: 64 | Batch_idx: 0 |  Loss: (0.0785) | Acc: (96.88%) (124/128)\n",
            "Epoch: 64 | Batch_idx: 10 |  Loss: (0.0824) | Acc: (97.59%) (1374/1408)\n",
            "Epoch: 64 | Batch_idx: 20 |  Loss: (0.0900) | Acc: (97.06%) (2609/2688)\n",
            "Epoch: 64 | Batch_idx: 30 |  Loss: (0.0861) | Acc: (97.00%) (3849/3968)\n",
            "Epoch: 64 | Batch_idx: 40 |  Loss: (0.0835) | Acc: (97.08%) (5095/5248)\n",
            "Epoch: 64 | Batch_idx: 50 |  Loss: (0.0828) | Acc: (97.15%) (6342/6528)\n",
            "Epoch: 64 | Batch_idx: 60 |  Loss: (0.0820) | Acc: (97.23%) (7592/7808)\n",
            "Epoch: 64 | Batch_idx: 70 |  Loss: (0.0799) | Acc: (97.33%) (8845/9088)\n",
            "Epoch: 64 | Batch_idx: 80 |  Loss: (0.0801) | Acc: (97.34%) (10092/10368)\n",
            "Epoch: 64 | Batch_idx: 90 |  Loss: (0.0804) | Acc: (97.34%) (11338/11648)\n",
            "Epoch: 64 | Batch_idx: 100 |  Loss: (0.0816) | Acc: (97.33%) (12583/12928)\n",
            "Epoch: 64 | Batch_idx: 110 |  Loss: (0.0824) | Acc: (97.31%) (13826/14208)\n",
            "Epoch: 64 | Batch_idx: 120 |  Loss: (0.0833) | Acc: (97.26%) (15063/15488)\n",
            "Epoch: 64 | Batch_idx: 130 |  Loss: (0.0821) | Acc: (97.28%) (16312/16768)\n",
            "Epoch: 64 | Batch_idx: 140 |  Loss: (0.0822) | Acc: (97.31%) (17562/18048)\n",
            "Epoch: 64 | Batch_idx: 150 |  Loss: (0.0820) | Acc: (97.32%) (18810/19328)\n",
            "Epoch: 64 | Batch_idx: 160 |  Loss: (0.0821) | Acc: (97.32%) (20055/20608)\n",
            "Epoch: 64 | Batch_idx: 170 |  Loss: (0.0821) | Acc: (97.31%) (21300/21888)\n",
            "Epoch: 64 | Batch_idx: 180 |  Loss: (0.0820) | Acc: (97.31%) (22545/23168)\n",
            "Epoch: 64 | Batch_idx: 190 |  Loss: (0.0822) | Acc: (97.32%) (23792/24448)\n",
            "Epoch: 64 | Batch_idx: 200 |  Loss: (0.0828) | Acc: (97.30%) (25033/25728)\n",
            "Epoch: 64 | Batch_idx: 210 |  Loss: (0.0831) | Acc: (97.29%) (26276/27008)\n",
            "Epoch: 64 | Batch_idx: 220 |  Loss: (0.0830) | Acc: (97.28%) (27518/28288)\n",
            "Epoch: 64 | Batch_idx: 230 |  Loss: (0.0835) | Acc: (97.27%) (28762/29568)\n",
            "Epoch: 64 | Batch_idx: 240 |  Loss: (0.0833) | Acc: (97.28%) (30009/30848)\n",
            "Epoch: 64 | Batch_idx: 250 |  Loss: (0.0840) | Acc: (97.23%) (31237/32128)\n",
            "Epoch: 64 | Batch_idx: 260 |  Loss: (0.0845) | Acc: (97.21%) (32475/33408)\n",
            "Epoch: 64 | Batch_idx: 270 |  Loss: (0.0849) | Acc: (97.20%) (33718/34688)\n",
            "Epoch: 64 | Batch_idx: 280 |  Loss: (0.0852) | Acc: (97.19%) (34959/35968)\n",
            "Epoch: 64 | Batch_idx: 290 |  Loss: (0.0855) | Acc: (97.18%) (36198/37248)\n",
            "Epoch: 64 | Batch_idx: 300 |  Loss: (0.0853) | Acc: (97.19%) (37444/38528)\n",
            "Epoch: 64 | Batch_idx: 310 |  Loss: (0.0853) | Acc: (97.19%) (38691/39808)\n",
            "Epoch: 64 | Batch_idx: 320 |  Loss: (0.0855) | Acc: (97.18%) (39931/41088)\n",
            "Epoch: 64 | Batch_idx: 330 |  Loss: (0.0861) | Acc: (97.17%) (41168/42368)\n",
            "Epoch: 64 | Batch_idx: 340 |  Loss: (0.0866) | Acc: (97.15%) (42406/43648)\n",
            "Epoch: 64 | Batch_idx: 350 |  Loss: (0.0868) | Acc: (97.16%) (43651/44928)\n",
            "Epoch: 64 | Batch_idx: 360 |  Loss: (0.0869) | Acc: (97.15%) (44889/46208)\n",
            "Epoch: 64 | Batch_idx: 370 |  Loss: (0.0873) | Acc: (97.14%) (46129/47488)\n",
            "Epoch: 64 | Batch_idx: 380 |  Loss: (0.0874) | Acc: (97.12%) (47365/48768)\n",
            "Epoch: 64 | Batch_idx: 390 |  Loss: (0.0875) | Acc: (97.11%) (48557/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3442) | Acc: (90.18%) (9018/10000)\n",
            "Epoch: 65 | Batch_idx: 0 |  Loss: (0.1068) | Acc: (96.09%) (123/128)\n",
            "Epoch: 65 | Batch_idx: 10 |  Loss: (0.0857) | Acc: (96.95%) (1365/1408)\n",
            "Epoch: 65 | Batch_idx: 20 |  Loss: (0.0909) | Acc: (96.84%) (2603/2688)\n",
            "Epoch: 65 | Batch_idx: 30 |  Loss: (0.0892) | Acc: (97.08%) (3852/3968)\n",
            "Epoch: 65 | Batch_idx: 40 |  Loss: (0.0838) | Acc: (97.28%) (5105/5248)\n",
            "Epoch: 65 | Batch_idx: 50 |  Loss: (0.0830) | Acc: (97.32%) (6353/6528)\n",
            "Epoch: 65 | Batch_idx: 60 |  Loss: (0.0809) | Acc: (97.34%) (7600/7808)\n",
            "Epoch: 65 | Batch_idx: 70 |  Loss: (0.0808) | Acc: (97.40%) (8852/9088)\n",
            "Epoch: 65 | Batch_idx: 80 |  Loss: (0.0808) | Acc: (97.44%) (10103/10368)\n",
            "Epoch: 65 | Batch_idx: 90 |  Loss: (0.0810) | Acc: (97.44%) (11350/11648)\n",
            "Epoch: 65 | Batch_idx: 100 |  Loss: (0.0798) | Acc: (97.47%) (12601/12928)\n",
            "Epoch: 65 | Batch_idx: 110 |  Loss: (0.0791) | Acc: (97.47%) (13849/14208)\n",
            "Epoch: 65 | Batch_idx: 120 |  Loss: (0.0790) | Acc: (97.47%) (15096/15488)\n",
            "Epoch: 65 | Batch_idx: 130 |  Loss: (0.0795) | Acc: (97.47%) (16344/16768)\n",
            "Epoch: 65 | Batch_idx: 140 |  Loss: (0.0796) | Acc: (97.47%) (17592/18048)\n",
            "Epoch: 65 | Batch_idx: 150 |  Loss: (0.0803) | Acc: (97.43%) (18831/19328)\n",
            "Epoch: 65 | Batch_idx: 160 |  Loss: (0.0801) | Acc: (97.44%) (20081/20608)\n",
            "Epoch: 65 | Batch_idx: 170 |  Loss: (0.0812) | Acc: (97.38%) (21314/21888)\n",
            "Epoch: 65 | Batch_idx: 180 |  Loss: (0.0813) | Acc: (97.36%) (22556/23168)\n",
            "Epoch: 65 | Batch_idx: 190 |  Loss: (0.0815) | Acc: (97.35%) (23800/24448)\n",
            "Epoch: 65 | Batch_idx: 200 |  Loss: (0.0813) | Acc: (97.35%) (25046/25728)\n",
            "Epoch: 65 | Batch_idx: 210 |  Loss: (0.0809) | Acc: (97.37%) (26298/27008)\n",
            "Epoch: 65 | Batch_idx: 220 |  Loss: (0.0810) | Acc: (97.36%) (27541/28288)\n",
            "Epoch: 65 | Batch_idx: 230 |  Loss: (0.0813) | Acc: (97.35%) (28785/29568)\n",
            "Epoch: 65 | Batch_idx: 240 |  Loss: (0.0812) | Acc: (97.35%) (30030/30848)\n",
            "Epoch: 65 | Batch_idx: 250 |  Loss: (0.0810) | Acc: (97.36%) (31280/32128)\n",
            "Epoch: 65 | Batch_idx: 260 |  Loss: (0.0810) | Acc: (97.38%) (32532/33408)\n",
            "Epoch: 65 | Batch_idx: 270 |  Loss: (0.0812) | Acc: (97.36%) (33771/34688)\n",
            "Epoch: 65 | Batch_idx: 280 |  Loss: (0.0812) | Acc: (97.34%) (35012/35968)\n",
            "Epoch: 65 | Batch_idx: 290 |  Loss: (0.0814) | Acc: (97.33%) (36253/37248)\n",
            "Epoch: 65 | Batch_idx: 300 |  Loss: (0.0818) | Acc: (97.32%) (37495/38528)\n",
            "Epoch: 65 | Batch_idx: 310 |  Loss: (0.0818) | Acc: (97.31%) (38739/39808)\n",
            "Epoch: 65 | Batch_idx: 320 |  Loss: (0.0821) | Acc: (97.30%) (39979/41088)\n",
            "Epoch: 65 | Batch_idx: 330 |  Loss: (0.0823) | Acc: (97.30%) (41222/42368)\n",
            "Epoch: 65 | Batch_idx: 340 |  Loss: (0.0824) | Acc: (97.30%) (42469/43648)\n",
            "Epoch: 65 | Batch_idx: 350 |  Loss: (0.0828) | Acc: (97.29%) (43712/44928)\n",
            "Epoch: 65 | Batch_idx: 360 |  Loss: (0.0831) | Acc: (97.29%) (44956/46208)\n",
            "Epoch: 65 | Batch_idx: 370 |  Loss: (0.0834) | Acc: (97.28%) (46196/47488)\n",
            "Epoch: 65 | Batch_idx: 380 |  Loss: (0.0834) | Acc: (97.30%) (47449/48768)\n",
            "Epoch: 65 | Batch_idx: 390 |  Loss: (0.0837) | Acc: (97.27%) (48635/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4257) | Acc: (88.70%) (8870/10000)\n",
            "Epoch: 66 | Batch_idx: 0 |  Loss: (0.0705) | Acc: (99.22%) (127/128)\n",
            "Epoch: 66 | Batch_idx: 10 |  Loss: (0.0829) | Acc: (97.51%) (1373/1408)\n",
            "Epoch: 66 | Batch_idx: 20 |  Loss: (0.0728) | Acc: (97.77%) (2628/2688)\n",
            "Epoch: 66 | Batch_idx: 30 |  Loss: (0.0782) | Acc: (97.61%) (3873/3968)\n",
            "Epoch: 66 | Batch_idx: 40 |  Loss: (0.0724) | Acc: (97.85%) (5135/5248)\n",
            "Epoch: 66 | Batch_idx: 50 |  Loss: (0.0724) | Acc: (97.75%) (6381/6528)\n",
            "Epoch: 66 | Batch_idx: 60 |  Loss: (0.0739) | Acc: (97.66%) (7625/7808)\n",
            "Epoch: 66 | Batch_idx: 70 |  Loss: (0.0716) | Acc: (97.69%) (8878/9088)\n",
            "Epoch: 66 | Batch_idx: 80 |  Loss: (0.0722) | Acc: (97.65%) (10124/10368)\n",
            "Epoch: 66 | Batch_idx: 90 |  Loss: (0.0721) | Acc: (97.65%) (11374/11648)\n",
            "Epoch: 66 | Batch_idx: 100 |  Loss: (0.0734) | Acc: (97.61%) (12619/12928)\n",
            "Epoch: 66 | Batch_idx: 110 |  Loss: (0.0725) | Acc: (97.66%) (13876/14208)\n",
            "Epoch: 66 | Batch_idx: 120 |  Loss: (0.0724) | Acc: (97.64%) (15122/15488)\n",
            "Epoch: 66 | Batch_idx: 130 |  Loss: (0.0731) | Acc: (97.61%) (16367/16768)\n",
            "Epoch: 66 | Batch_idx: 140 |  Loss: (0.0732) | Acc: (97.60%) (17615/18048)\n",
            "Epoch: 66 | Batch_idx: 150 |  Loss: (0.0730) | Acc: (97.58%) (18860/19328)\n",
            "Epoch: 66 | Batch_idx: 160 |  Loss: (0.0732) | Acc: (97.58%) (20109/20608)\n",
            "Epoch: 66 | Batch_idx: 170 |  Loss: (0.0730) | Acc: (97.59%) (21360/21888)\n",
            "Epoch: 66 | Batch_idx: 180 |  Loss: (0.0734) | Acc: (97.60%) (22611/23168)\n",
            "Epoch: 66 | Batch_idx: 190 |  Loss: (0.0738) | Acc: (97.57%) (23855/24448)\n",
            "Epoch: 66 | Batch_idx: 200 |  Loss: (0.0747) | Acc: (97.53%) (25092/25728)\n",
            "Epoch: 66 | Batch_idx: 210 |  Loss: (0.0758) | Acc: (97.48%) (26327/27008)\n",
            "Epoch: 66 | Batch_idx: 220 |  Loss: (0.0759) | Acc: (97.48%) (27576/28288)\n",
            "Epoch: 66 | Batch_idx: 230 |  Loss: (0.0760) | Acc: (97.47%) (28821/29568)\n",
            "Epoch: 66 | Batch_idx: 240 |  Loss: (0.0770) | Acc: (97.44%) (30059/30848)\n",
            "Epoch: 66 | Batch_idx: 250 |  Loss: (0.0777) | Acc: (97.41%) (31297/32128)\n",
            "Epoch: 66 | Batch_idx: 260 |  Loss: (0.0779) | Acc: (97.39%) (32537/33408)\n",
            "Epoch: 66 | Batch_idx: 270 |  Loss: (0.0783) | Acc: (97.37%) (33774/34688)\n",
            "Epoch: 66 | Batch_idx: 280 |  Loss: (0.0791) | Acc: (97.35%) (35015/35968)\n",
            "Epoch: 66 | Batch_idx: 290 |  Loss: (0.0788) | Acc: (97.37%) (36270/37248)\n",
            "Epoch: 66 | Batch_idx: 300 |  Loss: (0.0795) | Acc: (97.33%) (37501/38528)\n",
            "Epoch: 66 | Batch_idx: 310 |  Loss: (0.0798) | Acc: (97.33%) (38745/39808)\n",
            "Epoch: 66 | Batch_idx: 320 |  Loss: (0.0799) | Acc: (97.32%) (39987/41088)\n",
            "Epoch: 66 | Batch_idx: 330 |  Loss: (0.0807) | Acc: (97.29%) (41220/42368)\n",
            "Epoch: 66 | Batch_idx: 340 |  Loss: (0.0807) | Acc: (97.29%) (42466/43648)\n",
            "Epoch: 66 | Batch_idx: 350 |  Loss: (0.0812) | Acc: (97.27%) (43702/44928)\n",
            "Epoch: 66 | Batch_idx: 360 |  Loss: (0.0821) | Acc: (97.23%) (44930/46208)\n",
            "Epoch: 66 | Batch_idx: 370 |  Loss: (0.0822) | Acc: (97.24%) (46175/47488)\n",
            "Epoch: 66 | Batch_idx: 380 |  Loss: (0.0823) | Acc: (97.22%) (47413/48768)\n",
            "Epoch: 66 | Batch_idx: 390 |  Loss: (0.0829) | Acc: (97.19%) (48595/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3514) | Acc: (89.59%) (8959/10000)\n",
            "Epoch: 67 | Batch_idx: 0 |  Loss: (0.0824) | Acc: (97.66%) (125/128)\n",
            "Epoch: 67 | Batch_idx: 10 |  Loss: (0.0927) | Acc: (96.73%) (1362/1408)\n",
            "Epoch: 67 | Batch_idx: 20 |  Loss: (0.0859) | Acc: (96.76%) (2601/2688)\n",
            "Epoch: 67 | Batch_idx: 30 |  Loss: (0.0833) | Acc: (97.13%) (3854/3968)\n",
            "Epoch: 67 | Batch_idx: 40 |  Loss: (0.0843) | Acc: (97.08%) (5095/5248)\n",
            "Epoch: 67 | Batch_idx: 50 |  Loss: (0.0863) | Acc: (97.12%) (6340/6528)\n",
            "Epoch: 67 | Batch_idx: 60 |  Loss: (0.0852) | Acc: (97.20%) (7589/7808)\n",
            "Epoch: 67 | Batch_idx: 70 |  Loss: (0.0861) | Acc: (97.10%) (8824/9088)\n",
            "Epoch: 67 | Batch_idx: 80 |  Loss: (0.0857) | Acc: (97.17%) (10075/10368)\n",
            "Epoch: 67 | Batch_idx: 90 |  Loss: (0.0845) | Acc: (97.22%) (11324/11648)\n",
            "Epoch: 67 | Batch_idx: 100 |  Loss: (0.0839) | Acc: (97.22%) (12568/12928)\n",
            "Epoch: 67 | Batch_idx: 110 |  Loss: (0.0842) | Acc: (97.19%) (13809/14208)\n",
            "Epoch: 67 | Batch_idx: 120 |  Loss: (0.0834) | Acc: (97.24%) (15061/15488)\n",
            "Epoch: 67 | Batch_idx: 130 |  Loss: (0.0823) | Acc: (97.28%) (16312/16768)\n",
            "Epoch: 67 | Batch_idx: 140 |  Loss: (0.0807) | Acc: (97.36%) (17572/18048)\n",
            "Epoch: 67 | Batch_idx: 150 |  Loss: (0.0806) | Acc: (97.36%) (18817/19328)\n",
            "Epoch: 67 | Batch_idx: 160 |  Loss: (0.0795) | Acc: (97.42%) (20076/20608)\n",
            "Epoch: 67 | Batch_idx: 170 |  Loss: (0.0796) | Acc: (97.42%) (21324/21888)\n",
            "Epoch: 67 | Batch_idx: 180 |  Loss: (0.0804) | Acc: (97.39%) (22564/23168)\n",
            "Epoch: 67 | Batch_idx: 190 |  Loss: (0.0813) | Acc: (97.39%) (23809/24448)\n",
            "Epoch: 67 | Batch_idx: 200 |  Loss: (0.0811) | Acc: (97.39%) (25056/25728)\n",
            "Epoch: 67 | Batch_idx: 210 |  Loss: (0.0809) | Acc: (97.40%) (26305/27008)\n",
            "Epoch: 67 | Batch_idx: 220 |  Loss: (0.0802) | Acc: (97.41%) (27555/28288)\n",
            "Epoch: 67 | Batch_idx: 230 |  Loss: (0.0814) | Acc: (97.36%) (28788/29568)\n",
            "Epoch: 67 | Batch_idx: 240 |  Loss: (0.0817) | Acc: (97.36%) (30034/30848)\n",
            "Epoch: 67 | Batch_idx: 250 |  Loss: (0.0828) | Acc: (97.31%) (31265/32128)\n",
            "Epoch: 67 | Batch_idx: 260 |  Loss: (0.0832) | Acc: (97.31%) (32509/33408)\n",
            "Epoch: 67 | Batch_idx: 270 |  Loss: (0.0840) | Acc: (97.26%) (33738/34688)\n",
            "Epoch: 67 | Batch_idx: 280 |  Loss: (0.0843) | Acc: (97.24%) (34976/35968)\n",
            "Epoch: 67 | Batch_idx: 290 |  Loss: (0.0841) | Acc: (97.24%) (36221/37248)\n",
            "Epoch: 67 | Batch_idx: 300 |  Loss: (0.0850) | Acc: (97.20%) (37450/38528)\n",
            "Epoch: 67 | Batch_idx: 310 |  Loss: (0.0853) | Acc: (97.20%) (38692/39808)\n",
            "Epoch: 67 | Batch_idx: 320 |  Loss: (0.0851) | Acc: (97.20%) (39937/41088)\n",
            "Epoch: 67 | Batch_idx: 330 |  Loss: (0.0854) | Acc: (97.19%) (41179/42368)\n",
            "Epoch: 67 | Batch_idx: 340 |  Loss: (0.0859) | Acc: (97.16%) (42410/43648)\n",
            "Epoch: 67 | Batch_idx: 350 |  Loss: (0.0862) | Acc: (97.16%) (43650/44928)\n",
            "Epoch: 67 | Batch_idx: 360 |  Loss: (0.0862) | Acc: (97.15%) (44889/46208)\n",
            "Epoch: 67 | Batch_idx: 370 |  Loss: (0.0866) | Acc: (97.12%) (46122/47488)\n",
            "Epoch: 67 | Batch_idx: 380 |  Loss: (0.0869) | Acc: (97.11%) (47357/48768)\n",
            "Epoch: 67 | Batch_idx: 390 |  Loss: (0.0875) | Acc: (97.08%) (48541/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4168) | Acc: (88.24%) (8824/10000)\n",
            "Epoch: 68 | Batch_idx: 0 |  Loss: (0.1386) | Acc: (94.53%) (121/128)\n",
            "Epoch: 68 | Batch_idx: 10 |  Loss: (0.0849) | Acc: (97.02%) (1366/1408)\n",
            "Epoch: 68 | Batch_idx: 20 |  Loss: (0.0751) | Acc: (97.47%) (2620/2688)\n",
            "Epoch: 68 | Batch_idx: 30 |  Loss: (0.0714) | Acc: (97.66%) (3875/3968)\n",
            "Epoch: 68 | Batch_idx: 40 |  Loss: (0.0715) | Acc: (97.73%) (5129/5248)\n",
            "Epoch: 68 | Batch_idx: 50 |  Loss: (0.0710) | Acc: (97.70%) (6378/6528)\n",
            "Epoch: 68 | Batch_idx: 60 |  Loss: (0.0710) | Acc: (97.64%) (7624/7808)\n",
            "Epoch: 68 | Batch_idx: 70 |  Loss: (0.0729) | Acc: (97.57%) (8867/9088)\n",
            "Epoch: 68 | Batch_idx: 80 |  Loss: (0.0729) | Acc: (97.58%) (10117/10368)\n",
            "Epoch: 68 | Batch_idx: 90 |  Loss: (0.0737) | Acc: (97.59%) (11367/11648)\n",
            "Epoch: 68 | Batch_idx: 100 |  Loss: (0.0739) | Acc: (97.59%) (12616/12928)\n",
            "Epoch: 68 | Batch_idx: 110 |  Loss: (0.0760) | Acc: (97.49%) (13851/14208)\n",
            "Epoch: 68 | Batch_idx: 120 |  Loss: (0.0757) | Acc: (97.49%) (15100/15488)\n",
            "Epoch: 68 | Batch_idx: 130 |  Loss: (0.0760) | Acc: (97.47%) (16344/16768)\n",
            "Epoch: 68 | Batch_idx: 140 |  Loss: (0.0763) | Acc: (97.45%) (17587/18048)\n",
            "Epoch: 68 | Batch_idx: 150 |  Loss: (0.0758) | Acc: (97.49%) (18842/19328)\n",
            "Epoch: 68 | Batch_idx: 160 |  Loss: (0.0753) | Acc: (97.52%) (20096/20608)\n",
            "Epoch: 68 | Batch_idx: 170 |  Loss: (0.0753) | Acc: (97.53%) (21348/21888)\n",
            "Epoch: 68 | Batch_idx: 180 |  Loss: (0.0750) | Acc: (97.54%) (22598/23168)\n",
            "Epoch: 68 | Batch_idx: 190 |  Loss: (0.0753) | Acc: (97.53%) (23843/24448)\n",
            "Epoch: 68 | Batch_idx: 200 |  Loss: (0.0756) | Acc: (97.49%) (25081/25728)\n",
            "Epoch: 68 | Batch_idx: 210 |  Loss: (0.0764) | Acc: (97.46%) (26323/27008)\n",
            "Epoch: 68 | Batch_idx: 220 |  Loss: (0.0760) | Acc: (97.47%) (27573/28288)\n",
            "Epoch: 68 | Batch_idx: 230 |  Loss: (0.0758) | Acc: (97.49%) (28827/29568)\n",
            "Epoch: 68 | Batch_idx: 240 |  Loss: (0.0755) | Acc: (97.52%) (30082/30848)\n",
            "Epoch: 68 | Batch_idx: 250 |  Loss: (0.0761) | Acc: (97.50%) (31325/32128)\n",
            "Epoch: 68 | Batch_idx: 260 |  Loss: (0.0764) | Acc: (97.47%) (32564/33408)\n",
            "Epoch: 68 | Batch_idx: 270 |  Loss: (0.0767) | Acc: (97.48%) (33813/34688)\n",
            "Epoch: 68 | Batch_idx: 280 |  Loss: (0.0763) | Acc: (97.50%) (35068/35968)\n",
            "Epoch: 68 | Batch_idx: 290 |  Loss: (0.0768) | Acc: (97.47%) (36304/37248)\n",
            "Epoch: 68 | Batch_idx: 300 |  Loss: (0.0771) | Acc: (97.45%) (37545/38528)\n",
            "Epoch: 68 | Batch_idx: 310 |  Loss: (0.0777) | Acc: (97.43%) (38786/39808)\n",
            "Epoch: 68 | Batch_idx: 320 |  Loss: (0.0781) | Acc: (97.42%) (40029/41088)\n",
            "Epoch: 68 | Batch_idx: 330 |  Loss: (0.0786) | Acc: (97.40%) (41266/42368)\n",
            "Epoch: 68 | Batch_idx: 340 |  Loss: (0.0788) | Acc: (97.38%) (42506/43648)\n",
            "Epoch: 68 | Batch_idx: 350 |  Loss: (0.0791) | Acc: (97.37%) (43745/44928)\n",
            "Epoch: 68 | Batch_idx: 360 |  Loss: (0.0794) | Acc: (97.37%) (44993/46208)\n",
            "Epoch: 68 | Batch_idx: 370 |  Loss: (0.0797) | Acc: (97.36%) (46232/47488)\n",
            "Epoch: 68 | Batch_idx: 380 |  Loss: (0.0803) | Acc: (97.34%) (47470/48768)\n",
            "Epoch: 68 | Batch_idx: 390 |  Loss: (0.0804) | Acc: (97.32%) (48660/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3359) | Acc: (90.21%) (9021/10000)\n",
            "Epoch: 69 | Batch_idx: 0 |  Loss: (0.0666) | Acc: (97.66%) (125/128)\n",
            "Epoch: 69 | Batch_idx: 10 |  Loss: (0.0753) | Acc: (97.66%) (1375/1408)\n",
            "Epoch: 69 | Batch_idx: 20 |  Loss: (0.0750) | Acc: (97.54%) (2622/2688)\n",
            "Epoch: 69 | Batch_idx: 30 |  Loss: (0.0761) | Acc: (97.56%) (3871/3968)\n",
            "Epoch: 69 | Batch_idx: 40 |  Loss: (0.0803) | Acc: (97.31%) (5107/5248)\n",
            "Epoch: 69 | Batch_idx: 50 |  Loss: (0.0779) | Acc: (97.41%) (6359/6528)\n",
            "Epoch: 69 | Batch_idx: 60 |  Loss: (0.0795) | Acc: (97.36%) (7602/7808)\n",
            "Epoch: 69 | Batch_idx: 70 |  Loss: (0.0792) | Acc: (97.40%) (8852/9088)\n",
            "Epoch: 69 | Batch_idx: 80 |  Loss: (0.0780) | Acc: (97.42%) (10100/10368)\n",
            "Epoch: 69 | Batch_idx: 90 |  Loss: (0.0801) | Acc: (97.30%) (11333/11648)\n",
            "Epoch: 69 | Batch_idx: 100 |  Loss: (0.0788) | Acc: (97.35%) (12586/12928)\n",
            "Epoch: 69 | Batch_idx: 110 |  Loss: (0.0777) | Acc: (97.46%) (13847/14208)\n",
            "Epoch: 69 | Batch_idx: 120 |  Loss: (0.0762) | Acc: (97.53%) (15106/15488)\n",
            "Epoch: 69 | Batch_idx: 130 |  Loss: (0.0761) | Acc: (97.55%) (16358/16768)\n",
            "Epoch: 69 | Batch_idx: 140 |  Loss: (0.0761) | Acc: (97.55%) (17606/18048)\n",
            "Epoch: 69 | Batch_idx: 150 |  Loss: (0.0763) | Acc: (97.52%) (18849/19328)\n",
            "Epoch: 69 | Batch_idx: 160 |  Loss: (0.0759) | Acc: (97.54%) (20102/20608)\n",
            "Epoch: 69 | Batch_idx: 170 |  Loss: (0.0757) | Acc: (97.55%) (21352/21888)\n",
            "Epoch: 69 | Batch_idx: 180 |  Loss: (0.0755) | Acc: (97.58%) (22607/23168)\n",
            "Epoch: 69 | Batch_idx: 190 |  Loss: (0.0747) | Acc: (97.61%) (23863/24448)\n",
            "Epoch: 69 | Batch_idx: 200 |  Loss: (0.0749) | Acc: (97.59%) (25107/25728)\n",
            "Epoch: 69 | Batch_idx: 210 |  Loss: (0.0759) | Acc: (97.52%) (26339/27008)\n",
            "Epoch: 69 | Batch_idx: 220 |  Loss: (0.0765) | Acc: (97.51%) (27583/28288)\n",
            "Epoch: 69 | Batch_idx: 230 |  Loss: (0.0768) | Acc: (97.48%) (28822/29568)\n",
            "Epoch: 69 | Batch_idx: 240 |  Loss: (0.0777) | Acc: (97.45%) (30060/30848)\n",
            "Epoch: 69 | Batch_idx: 250 |  Loss: (0.0782) | Acc: (97.44%) (31307/32128)\n",
            "Epoch: 69 | Batch_idx: 260 |  Loss: (0.0786) | Acc: (97.42%) (32546/33408)\n",
            "Epoch: 69 | Batch_idx: 270 |  Loss: (0.0782) | Acc: (97.44%) (33801/34688)\n",
            "Epoch: 69 | Batch_idx: 280 |  Loss: (0.0788) | Acc: (97.41%) (35035/35968)\n",
            "Epoch: 69 | Batch_idx: 290 |  Loss: (0.0799) | Acc: (97.37%) (36268/37248)\n",
            "Epoch: 69 | Batch_idx: 300 |  Loss: (0.0799) | Acc: (97.36%) (37509/38528)\n",
            "Epoch: 69 | Batch_idx: 310 |  Loss: (0.0799) | Acc: (97.35%) (38754/39808)\n",
            "Epoch: 69 | Batch_idx: 320 |  Loss: (0.0803) | Acc: (97.33%) (39993/41088)\n",
            "Epoch: 69 | Batch_idx: 330 |  Loss: (0.0804) | Acc: (97.32%) (41234/42368)\n",
            "Epoch: 69 | Batch_idx: 340 |  Loss: (0.0807) | Acc: (97.32%) (42477/43648)\n",
            "Epoch: 69 | Batch_idx: 350 |  Loss: (0.0811) | Acc: (97.30%) (43713/44928)\n",
            "Epoch: 69 | Batch_idx: 360 |  Loss: (0.0808) | Acc: (97.31%) (44967/46208)\n",
            "Epoch: 69 | Batch_idx: 370 |  Loss: (0.0807) | Acc: (97.32%) (46214/47488)\n",
            "Epoch: 69 | Batch_idx: 380 |  Loss: (0.0805) | Acc: (97.33%) (47466/48768)\n",
            "Epoch: 69 | Batch_idx: 390 |  Loss: (0.0803) | Acc: (97.34%) (48669/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3339) | Acc: (90.37%) (9037/10000)\n",
            "Epoch: 70 | Batch_idx: 0 |  Loss: (0.0905) | Acc: (98.44%) (126/128)\n",
            "Epoch: 70 | Batch_idx: 10 |  Loss: (0.0690) | Acc: (98.30%) (1384/1408)\n",
            "Epoch: 70 | Batch_idx: 20 |  Loss: (0.0718) | Acc: (97.69%) (2626/2688)\n",
            "Epoch: 70 | Batch_idx: 30 |  Loss: (0.0715) | Acc: (97.68%) (3876/3968)\n",
            "Epoch: 70 | Batch_idx: 40 |  Loss: (0.0707) | Acc: (97.77%) (5131/5248)\n",
            "Epoch: 70 | Batch_idx: 50 |  Loss: (0.0706) | Acc: (97.72%) (6379/6528)\n",
            "Epoch: 70 | Batch_idx: 60 |  Loss: (0.0708) | Acc: (97.72%) (7630/7808)\n",
            "Epoch: 70 | Batch_idx: 70 |  Loss: (0.0701) | Acc: (97.76%) (8884/9088)\n",
            "Epoch: 70 | Batch_idx: 80 |  Loss: (0.0719) | Acc: (97.66%) (10125/10368)\n",
            "Epoch: 70 | Batch_idx: 90 |  Loss: (0.0722) | Acc: (97.64%) (11373/11648)\n",
            "Epoch: 70 | Batch_idx: 100 |  Loss: (0.0734) | Acc: (97.62%) (12620/12928)\n",
            "Epoch: 70 | Batch_idx: 110 |  Loss: (0.0732) | Acc: (97.59%) (13865/14208)\n",
            "Epoch: 70 | Batch_idx: 120 |  Loss: (0.0729) | Acc: (97.62%) (15119/15488)\n",
            "Epoch: 70 | Batch_idx: 130 |  Loss: (0.0746) | Acc: (97.53%) (16353/16768)\n",
            "Epoch: 70 | Batch_idx: 140 |  Loss: (0.0755) | Acc: (97.49%) (17595/18048)\n",
            "Epoch: 70 | Batch_idx: 150 |  Loss: (0.0749) | Acc: (97.52%) (18848/19328)\n",
            "Epoch: 70 | Batch_idx: 160 |  Loss: (0.0747) | Acc: (97.52%) (20096/20608)\n",
            "Epoch: 70 | Batch_idx: 170 |  Loss: (0.0748) | Acc: (97.51%) (21344/21888)\n",
            "Epoch: 70 | Batch_idx: 180 |  Loss: (0.0747) | Acc: (97.54%) (22598/23168)\n",
            "Epoch: 70 | Batch_idx: 190 |  Loss: (0.0746) | Acc: (97.52%) (23842/24448)\n",
            "Epoch: 70 | Batch_idx: 200 |  Loss: (0.0754) | Acc: (97.48%) (25079/25728)\n",
            "Epoch: 70 | Batch_idx: 210 |  Loss: (0.0758) | Acc: (97.45%) (26320/27008)\n",
            "Epoch: 70 | Batch_idx: 220 |  Loss: (0.0762) | Acc: (97.43%) (27560/28288)\n",
            "Epoch: 70 | Batch_idx: 230 |  Loss: (0.0764) | Acc: (97.42%) (28805/29568)\n",
            "Epoch: 70 | Batch_idx: 240 |  Loss: (0.0768) | Acc: (97.41%) (30049/30848)\n",
            "Epoch: 70 | Batch_idx: 250 |  Loss: (0.0770) | Acc: (97.39%) (31288/32128)\n",
            "Epoch: 70 | Batch_idx: 260 |  Loss: (0.0771) | Acc: (97.38%) (32534/33408)\n",
            "Epoch: 70 | Batch_idx: 270 |  Loss: (0.0770) | Acc: (97.39%) (33782/34688)\n",
            "Epoch: 70 | Batch_idx: 280 |  Loss: (0.0773) | Acc: (97.37%) (35023/35968)\n",
            "Epoch: 70 | Batch_idx: 290 |  Loss: (0.0774) | Acc: (97.38%) (36272/37248)\n",
            "Epoch: 70 | Batch_idx: 300 |  Loss: (0.0775) | Acc: (97.38%) (37518/38528)\n",
            "Epoch: 70 | Batch_idx: 310 |  Loss: (0.0773) | Acc: (97.38%) (38767/39808)\n",
            "Epoch: 70 | Batch_idx: 320 |  Loss: (0.0775) | Acc: (97.39%) (40014/41088)\n",
            "Epoch: 70 | Batch_idx: 330 |  Loss: (0.0775) | Acc: (97.39%) (41261/42368)\n",
            "Epoch: 70 | Batch_idx: 340 |  Loss: (0.0775) | Acc: (97.39%) (42508/43648)\n",
            "Epoch: 70 | Batch_idx: 350 |  Loss: (0.0781) | Acc: (97.37%) (43747/44928)\n",
            "Epoch: 70 | Batch_idx: 360 |  Loss: (0.0781) | Acc: (97.36%) (44990/46208)\n",
            "Epoch: 70 | Batch_idx: 370 |  Loss: (0.0791) | Acc: (97.32%) (46217/47488)\n",
            "Epoch: 70 | Batch_idx: 380 |  Loss: (0.0802) | Acc: (97.29%) (47446/48768)\n",
            "Epoch: 70 | Batch_idx: 390 |  Loss: (0.0805) | Acc: (97.27%) (48634/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3920) | Acc: (88.66%) (8866/10000)\n",
            "Epoch: 71 | Batch_idx: 0 |  Loss: (0.0895) | Acc: (97.66%) (125/128)\n",
            "Epoch: 71 | Batch_idx: 10 |  Loss: (0.0876) | Acc: (96.73%) (1362/1408)\n",
            "Epoch: 71 | Batch_idx: 20 |  Loss: (0.0814) | Acc: (97.32%) (2616/2688)\n",
            "Epoch: 71 | Batch_idx: 30 |  Loss: (0.0819) | Acc: (97.30%) (3861/3968)\n",
            "Epoch: 71 | Batch_idx: 40 |  Loss: (0.0778) | Acc: (97.45%) (5114/5248)\n",
            "Epoch: 71 | Batch_idx: 50 |  Loss: (0.0800) | Acc: (97.37%) (6356/6528)\n",
            "Epoch: 71 | Batch_idx: 60 |  Loss: (0.0766) | Acc: (97.50%) (7613/7808)\n",
            "Epoch: 71 | Batch_idx: 70 |  Loss: (0.0783) | Acc: (97.45%) (8856/9088)\n",
            "Epoch: 71 | Batch_idx: 80 |  Loss: (0.0776) | Acc: (97.49%) (10108/10368)\n",
            "Epoch: 71 | Batch_idx: 90 |  Loss: (0.0769) | Acc: (97.50%) (11357/11648)\n",
            "Epoch: 71 | Batch_idx: 100 |  Loss: (0.0757) | Acc: (97.59%) (12616/12928)\n",
            "Epoch: 71 | Batch_idx: 110 |  Loss: (0.0764) | Acc: (97.55%) (13860/14208)\n",
            "Epoch: 71 | Batch_idx: 120 |  Loss: (0.0760) | Acc: (97.57%) (15111/15488)\n",
            "Epoch: 71 | Batch_idx: 130 |  Loss: (0.0777) | Acc: (97.48%) (16346/16768)\n",
            "Epoch: 71 | Batch_idx: 140 |  Loss: (0.0769) | Acc: (97.51%) (17598/18048)\n",
            "Epoch: 71 | Batch_idx: 150 |  Loss: (0.0773) | Acc: (97.45%) (18836/19328)\n",
            "Epoch: 71 | Batch_idx: 160 |  Loss: (0.0775) | Acc: (97.42%) (20077/20608)\n",
            "Epoch: 71 | Batch_idx: 170 |  Loss: (0.0781) | Acc: (97.39%) (21316/21888)\n",
            "Epoch: 71 | Batch_idx: 180 |  Loss: (0.0782) | Acc: (97.39%) (22563/23168)\n",
            "Epoch: 71 | Batch_idx: 190 |  Loss: (0.0780) | Acc: (97.37%) (23806/24448)\n",
            "Epoch: 71 | Batch_idx: 200 |  Loss: (0.0785) | Acc: (97.33%) (25042/25728)\n",
            "Epoch: 71 | Batch_idx: 210 |  Loss: (0.0780) | Acc: (97.35%) (26292/27008)\n",
            "Epoch: 71 | Batch_idx: 220 |  Loss: (0.0774) | Acc: (97.38%) (27546/28288)\n",
            "Epoch: 71 | Batch_idx: 230 |  Loss: (0.0768) | Acc: (97.40%) (28800/29568)\n",
            "Epoch: 71 | Batch_idx: 240 |  Loss: (0.0767) | Acc: (97.41%) (30048/30848)\n",
            "Epoch: 71 | Batch_idx: 250 |  Loss: (0.0768) | Acc: (97.41%) (31295/32128)\n",
            "Epoch: 71 | Batch_idx: 260 |  Loss: (0.0771) | Acc: (97.41%) (32542/33408)\n",
            "Epoch: 71 | Batch_idx: 270 |  Loss: (0.0776) | Acc: (97.39%) (33781/34688)\n",
            "Epoch: 71 | Batch_idx: 280 |  Loss: (0.0778) | Acc: (97.38%) (35025/35968)\n",
            "Epoch: 71 | Batch_idx: 290 |  Loss: (0.0779) | Acc: (97.37%) (36270/37248)\n",
            "Epoch: 71 | Batch_idx: 300 |  Loss: (0.0778) | Acc: (97.38%) (37518/38528)\n",
            "Epoch: 71 | Batch_idx: 310 |  Loss: (0.0777) | Acc: (97.39%) (38768/39808)\n",
            "Epoch: 71 | Batch_idx: 320 |  Loss: (0.0786) | Acc: (97.34%) (39997/41088)\n",
            "Epoch: 71 | Batch_idx: 330 |  Loss: (0.0789) | Acc: (97.33%) (41235/42368)\n",
            "Epoch: 71 | Batch_idx: 340 |  Loss: (0.0793) | Acc: (97.31%) (42475/43648)\n",
            "Epoch: 71 | Batch_idx: 350 |  Loss: (0.0798) | Acc: (97.30%) (43715/44928)\n",
            "Epoch: 71 | Batch_idx: 360 |  Loss: (0.0801) | Acc: (97.29%) (44957/46208)\n",
            "Epoch: 71 | Batch_idx: 370 |  Loss: (0.0801) | Acc: (97.30%) (46204/47488)\n",
            "Epoch: 71 | Batch_idx: 380 |  Loss: (0.0801) | Acc: (97.30%) (47450/48768)\n",
            "Epoch: 71 | Batch_idx: 390 |  Loss: (0.0801) | Acc: (97.29%) (48647/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3166) | Acc: (90.88%) (9088/10000)\n",
            "Epoch: 72 | Batch_idx: 0 |  Loss: (0.0836) | Acc: (96.88%) (124/128)\n",
            "Epoch: 72 | Batch_idx: 10 |  Loss: (0.0678) | Acc: (97.94%) (1379/1408)\n",
            "Epoch: 72 | Batch_idx: 20 |  Loss: (0.0692) | Acc: (97.58%) (2623/2688)\n",
            "Epoch: 72 | Batch_idx: 30 |  Loss: (0.0695) | Acc: (97.51%) (3869/3968)\n",
            "Epoch: 72 | Batch_idx: 40 |  Loss: (0.0669) | Acc: (97.64%) (5124/5248)\n",
            "Epoch: 72 | Batch_idx: 50 |  Loss: (0.0640) | Acc: (97.82%) (6386/6528)\n",
            "Epoch: 72 | Batch_idx: 60 |  Loss: (0.0657) | Acc: (97.73%) (7631/7808)\n",
            "Epoch: 72 | Batch_idx: 70 |  Loss: (0.0658) | Acc: (97.77%) (8885/9088)\n",
            "Epoch: 72 | Batch_idx: 80 |  Loss: (0.0675) | Acc: (97.66%) (10125/10368)\n",
            "Epoch: 72 | Batch_idx: 90 |  Loss: (0.0696) | Acc: (97.61%) (11370/11648)\n",
            "Epoch: 72 | Batch_idx: 100 |  Loss: (0.0701) | Acc: (97.66%) (12625/12928)\n",
            "Epoch: 72 | Batch_idx: 110 |  Loss: (0.0719) | Acc: (97.61%) (13869/14208)\n",
            "Epoch: 72 | Batch_idx: 120 |  Loss: (0.0707) | Acc: (97.67%) (15127/15488)\n",
            "Epoch: 72 | Batch_idx: 130 |  Loss: (0.0716) | Acc: (97.62%) (16369/16768)\n",
            "Epoch: 72 | Batch_idx: 140 |  Loss: (0.0726) | Acc: (97.57%) (17610/18048)\n",
            "Epoch: 72 | Batch_idx: 150 |  Loss: (0.0738) | Acc: (97.51%) (18846/19328)\n",
            "Epoch: 72 | Batch_idx: 160 |  Loss: (0.0743) | Acc: (97.48%) (20088/20608)\n",
            "Epoch: 72 | Batch_idx: 170 |  Loss: (0.0747) | Acc: (97.50%) (21340/21888)\n",
            "Epoch: 72 | Batch_idx: 180 |  Loss: (0.0748) | Acc: (97.50%) (22588/23168)\n",
            "Epoch: 72 | Batch_idx: 190 |  Loss: (0.0755) | Acc: (97.47%) (23830/24448)\n",
            "Epoch: 72 | Batch_idx: 200 |  Loss: (0.0757) | Acc: (97.45%) (25071/25728)\n",
            "Epoch: 72 | Batch_idx: 210 |  Loss: (0.0759) | Acc: (97.43%) (26313/27008)\n",
            "Epoch: 72 | Batch_idx: 220 |  Loss: (0.0766) | Acc: (97.42%) (27559/28288)\n",
            "Epoch: 72 | Batch_idx: 230 |  Loss: (0.0768) | Acc: (97.41%) (28802/29568)\n",
            "Epoch: 72 | Batch_idx: 240 |  Loss: (0.0767) | Acc: (97.40%) (30047/30848)\n",
            "Epoch: 72 | Batch_idx: 250 |  Loss: (0.0767) | Acc: (97.41%) (31297/32128)\n",
            "Epoch: 72 | Batch_idx: 260 |  Loss: (0.0770) | Acc: (97.40%) (32538/33408)\n",
            "Epoch: 72 | Batch_idx: 270 |  Loss: (0.0781) | Acc: (97.37%) (33774/34688)\n",
            "Epoch: 72 | Batch_idx: 280 |  Loss: (0.0781) | Acc: (97.38%) (35025/35968)\n",
            "Epoch: 72 | Batch_idx: 290 |  Loss: (0.0785) | Acc: (97.38%) (36272/37248)\n",
            "Epoch: 72 | Batch_idx: 300 |  Loss: (0.0785) | Acc: (97.38%) (37519/38528)\n",
            "Epoch: 72 | Batch_idx: 310 |  Loss: (0.0789) | Acc: (97.36%) (38758/39808)\n",
            "Epoch: 72 | Batch_idx: 320 |  Loss: (0.0791) | Acc: (97.35%) (39999/41088)\n",
            "Epoch: 72 | Batch_idx: 330 |  Loss: (0.0793) | Acc: (97.35%) (41244/42368)\n",
            "Epoch: 72 | Batch_idx: 340 |  Loss: (0.0798) | Acc: (97.33%) (42484/43648)\n",
            "Epoch: 72 | Batch_idx: 350 |  Loss: (0.0802) | Acc: (97.31%) (43719/44928)\n",
            "Epoch: 72 | Batch_idx: 360 |  Loss: (0.0803) | Acc: (97.30%) (44960/46208)\n",
            "Epoch: 72 | Batch_idx: 370 |  Loss: (0.0805) | Acc: (97.29%) (46201/47488)\n",
            "Epoch: 72 | Batch_idx: 380 |  Loss: (0.0809) | Acc: (97.27%) (47437/48768)\n",
            "Epoch: 72 | Batch_idx: 390 |  Loss: (0.0811) | Acc: (97.27%) (48635/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3680) | Acc: (89.82%) (8982/10000)\n",
            "Epoch: 73 | Batch_idx: 0 |  Loss: (0.1452) | Acc: (96.09%) (123/128)\n",
            "Epoch: 73 | Batch_idx: 10 |  Loss: (0.0717) | Acc: (97.73%) (1376/1408)\n",
            "Epoch: 73 | Batch_idx: 20 |  Loss: (0.0759) | Acc: (97.32%) (2616/2688)\n",
            "Epoch: 73 | Batch_idx: 30 |  Loss: (0.0816) | Acc: (97.20%) (3857/3968)\n",
            "Epoch: 73 | Batch_idx: 40 |  Loss: (0.0785) | Acc: (97.35%) (5109/5248)\n",
            "Epoch: 73 | Batch_idx: 50 |  Loss: (0.0765) | Acc: (97.46%) (6362/6528)\n",
            "Epoch: 73 | Batch_idx: 60 |  Loss: (0.0771) | Acc: (97.43%) (7607/7808)\n",
            "Epoch: 73 | Batch_idx: 70 |  Loss: (0.0736) | Acc: (97.61%) (8871/9088)\n",
            "Epoch: 73 | Batch_idx: 80 |  Loss: (0.0721) | Acc: (97.68%) (10127/10368)\n",
            "Epoch: 73 | Batch_idx: 90 |  Loss: (0.0728) | Acc: (97.67%) (11377/11648)\n",
            "Epoch: 73 | Batch_idx: 100 |  Loss: (0.0709) | Acc: (97.74%) (12636/12928)\n",
            "Epoch: 73 | Batch_idx: 110 |  Loss: (0.0707) | Acc: (97.73%) (13886/14208)\n",
            "Epoch: 73 | Batch_idx: 120 |  Loss: (0.0713) | Acc: (97.71%) (15134/15488)\n",
            "Epoch: 73 | Batch_idx: 130 |  Loss: (0.0705) | Acc: (97.74%) (16389/16768)\n",
            "Epoch: 73 | Batch_idx: 140 |  Loss: (0.0717) | Acc: (97.68%) (17630/18048)\n",
            "Epoch: 73 | Batch_idx: 150 |  Loss: (0.0725) | Acc: (97.63%) (18870/19328)\n",
            "Epoch: 73 | Batch_idx: 160 |  Loss: (0.0725) | Acc: (97.63%) (20120/20608)\n",
            "Epoch: 73 | Batch_idx: 170 |  Loss: (0.0732) | Acc: (97.58%) (21359/21888)\n",
            "Epoch: 73 | Batch_idx: 180 |  Loss: (0.0750) | Acc: (97.53%) (22595/23168)\n",
            "Epoch: 73 | Batch_idx: 190 |  Loss: (0.0754) | Acc: (97.53%) (23843/24448)\n",
            "Epoch: 73 | Batch_idx: 200 |  Loss: (0.0757) | Acc: (97.52%) (25090/25728)\n",
            "Epoch: 73 | Batch_idx: 210 |  Loss: (0.0766) | Acc: (97.48%) (26327/27008)\n",
            "Epoch: 73 | Batch_idx: 220 |  Loss: (0.0766) | Acc: (97.50%) (27580/28288)\n",
            "Epoch: 73 | Batch_idx: 230 |  Loss: (0.0769) | Acc: (97.48%) (28822/29568)\n",
            "Epoch: 73 | Batch_idx: 240 |  Loss: (0.0763) | Acc: (97.49%) (30075/30848)\n",
            "Epoch: 73 | Batch_idx: 250 |  Loss: (0.0759) | Acc: (97.53%) (31334/32128)\n",
            "Epoch: 73 | Batch_idx: 260 |  Loss: (0.0756) | Acc: (97.54%) (32585/33408)\n",
            "Epoch: 73 | Batch_idx: 270 |  Loss: (0.0758) | Acc: (97.54%) (33836/34688)\n",
            "Epoch: 73 | Batch_idx: 280 |  Loss: (0.0762) | Acc: (97.53%) (35079/35968)\n",
            "Epoch: 73 | Batch_idx: 290 |  Loss: (0.0767) | Acc: (97.51%) (36321/37248)\n",
            "Epoch: 73 | Batch_idx: 300 |  Loss: (0.0772) | Acc: (97.50%) (37566/38528)\n",
            "Epoch: 73 | Batch_idx: 310 |  Loss: (0.0771) | Acc: (97.50%) (38813/39808)\n",
            "Epoch: 73 | Batch_idx: 320 |  Loss: (0.0779) | Acc: (97.48%) (40054/41088)\n",
            "Epoch: 73 | Batch_idx: 330 |  Loss: (0.0784) | Acc: (97.47%) (41295/42368)\n",
            "Epoch: 73 | Batch_idx: 340 |  Loss: (0.0787) | Acc: (97.45%) (42535/43648)\n",
            "Epoch: 73 | Batch_idx: 350 |  Loss: (0.0792) | Acc: (97.42%) (43767/44928)\n",
            "Epoch: 73 | Batch_idx: 360 |  Loss: (0.0800) | Acc: (97.38%) (44999/46208)\n",
            "Epoch: 73 | Batch_idx: 370 |  Loss: (0.0803) | Acc: (97.37%) (46238/47488)\n",
            "Epoch: 73 | Batch_idx: 380 |  Loss: (0.0813) | Acc: (97.34%) (47469/48768)\n",
            "Epoch: 73 | Batch_idx: 390 |  Loss: (0.0816) | Acc: (97.33%) (48664/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3670) | Acc: (89.40%) (8940/10000)\n",
            "Epoch: 74 | Batch_idx: 0 |  Loss: (0.0799) | Acc: (97.66%) (125/128)\n",
            "Epoch: 74 | Batch_idx: 10 |  Loss: (0.0810) | Acc: (97.30%) (1370/1408)\n",
            "Epoch: 74 | Batch_idx: 20 |  Loss: (0.0842) | Acc: (97.02%) (2608/2688)\n",
            "Epoch: 74 | Batch_idx: 30 |  Loss: (0.0845) | Acc: (96.77%) (3840/3968)\n",
            "Epoch: 74 | Batch_idx: 40 |  Loss: (0.0862) | Acc: (96.78%) (5079/5248)\n",
            "Epoch: 74 | Batch_idx: 50 |  Loss: (0.0861) | Acc: (96.78%) (6318/6528)\n",
            "Epoch: 74 | Batch_idx: 60 |  Loss: (0.0849) | Acc: (96.89%) (7565/7808)\n",
            "Epoch: 74 | Batch_idx: 70 |  Loss: (0.0810) | Acc: (97.10%) (8824/9088)\n",
            "Epoch: 74 | Batch_idx: 80 |  Loss: (0.0797) | Acc: (97.14%) (10071/10368)\n",
            "Epoch: 74 | Batch_idx: 90 |  Loss: (0.0788) | Acc: (97.19%) (11321/11648)\n",
            "Epoch: 74 | Batch_idx: 100 |  Loss: (0.0784) | Acc: (97.23%) (12570/12928)\n",
            "Epoch: 74 | Batch_idx: 110 |  Loss: (0.0776) | Acc: (97.28%) (13822/14208)\n",
            "Epoch: 74 | Batch_idx: 120 |  Loss: (0.0776) | Acc: (97.29%) (15068/15488)\n",
            "Epoch: 74 | Batch_idx: 130 |  Loss: (0.0783) | Acc: (97.29%) (16313/16768)\n",
            "Epoch: 74 | Batch_idx: 140 |  Loss: (0.0779) | Acc: (97.32%) (17565/18048)\n",
            "Epoch: 74 | Batch_idx: 150 |  Loss: (0.0781) | Acc: (97.33%) (18811/19328)\n",
            "Epoch: 74 | Batch_idx: 160 |  Loss: (0.0785) | Acc: (97.31%) (20054/20608)\n",
            "Epoch: 74 | Batch_idx: 170 |  Loss: (0.0793) | Acc: (97.30%) (21298/21888)\n",
            "Epoch: 74 | Batch_idx: 180 |  Loss: (0.0795) | Acc: (97.29%) (22539/23168)\n",
            "Epoch: 74 | Batch_idx: 190 |  Loss: (0.0796) | Acc: (97.30%) (23787/24448)\n",
            "Epoch: 74 | Batch_idx: 200 |  Loss: (0.0796) | Acc: (97.30%) (25033/25728)\n",
            "Epoch: 74 | Batch_idx: 210 |  Loss: (0.0797) | Acc: (97.31%) (26281/27008)\n",
            "Epoch: 74 | Batch_idx: 220 |  Loss: (0.0797) | Acc: (97.31%) (27526/28288)\n",
            "Epoch: 74 | Batch_idx: 230 |  Loss: (0.0792) | Acc: (97.33%) (28779/29568)\n",
            "Epoch: 74 | Batch_idx: 240 |  Loss: (0.0793) | Acc: (97.33%) (30025/30848)\n",
            "Epoch: 74 | Batch_idx: 250 |  Loss: (0.0800) | Acc: (97.30%) (31262/32128)\n",
            "Epoch: 74 | Batch_idx: 260 |  Loss: (0.0799) | Acc: (97.31%) (32509/33408)\n",
            "Epoch: 74 | Batch_idx: 270 |  Loss: (0.0801) | Acc: (97.29%) (33749/34688)\n",
            "Epoch: 74 | Batch_idx: 280 |  Loss: (0.0811) | Acc: (97.25%) (34980/35968)\n",
            "Epoch: 74 | Batch_idx: 290 |  Loss: (0.0810) | Acc: (97.26%) (36229/37248)\n",
            "Epoch: 74 | Batch_idx: 300 |  Loss: (0.0811) | Acc: (97.26%) (37473/38528)\n",
            "Epoch: 74 | Batch_idx: 310 |  Loss: (0.0810) | Acc: (97.26%) (38716/39808)\n",
            "Epoch: 74 | Batch_idx: 320 |  Loss: (0.0810) | Acc: (97.25%) (39960/41088)\n",
            "Epoch: 74 | Batch_idx: 330 |  Loss: (0.0809) | Acc: (97.26%) (41206/42368)\n",
            "Epoch: 74 | Batch_idx: 340 |  Loss: (0.0814) | Acc: (97.24%) (42442/43648)\n",
            "Epoch: 74 | Batch_idx: 350 |  Loss: (0.0820) | Acc: (97.22%) (43678/44928)\n",
            "Epoch: 74 | Batch_idx: 360 |  Loss: (0.0825) | Acc: (97.19%) (44909/46208)\n",
            "Epoch: 74 | Batch_idx: 370 |  Loss: (0.0828) | Acc: (97.17%) (46145/47488)\n",
            "Epoch: 74 | Batch_idx: 380 |  Loss: (0.0828) | Acc: (97.19%) (47396/48768)\n",
            "Epoch: 74 | Batch_idx: 390 |  Loss: (0.0832) | Acc: (97.18%) (48590/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3783) | Acc: (89.31%) (8931/10000)\n",
            "Epoch: 75 | Batch_idx: 0 |  Loss: (0.0701) | Acc: (98.44%) (126/128)\n",
            "Epoch: 75 | Batch_idx: 10 |  Loss: (0.0802) | Acc: (97.09%) (1367/1408)\n",
            "Epoch: 75 | Batch_idx: 20 |  Loss: (0.0824) | Acc: (97.25%) (2614/2688)\n",
            "Epoch: 75 | Batch_idx: 30 |  Loss: (0.0788) | Acc: (97.38%) (3864/3968)\n",
            "Epoch: 75 | Batch_idx: 40 |  Loss: (0.0809) | Acc: (97.31%) (5107/5248)\n",
            "Epoch: 75 | Batch_idx: 50 |  Loss: (0.0779) | Acc: (97.50%) (6365/6528)\n",
            "Epoch: 75 | Batch_idx: 60 |  Loss: (0.0763) | Acc: (97.50%) (7613/7808)\n",
            "Epoch: 75 | Batch_idx: 70 |  Loss: (0.0756) | Acc: (97.49%) (8860/9088)\n",
            "Epoch: 75 | Batch_idx: 80 |  Loss: (0.0747) | Acc: (97.52%) (10111/10368)\n",
            "Epoch: 75 | Batch_idx: 90 |  Loss: (0.0753) | Acc: (97.51%) (11358/11648)\n",
            "Epoch: 75 | Batch_idx: 100 |  Loss: (0.0774) | Acc: (97.46%) (12599/12928)\n",
            "Epoch: 75 | Batch_idx: 110 |  Loss: (0.0789) | Acc: (97.41%) (13840/14208)\n",
            "Epoch: 75 | Batch_idx: 120 |  Loss: (0.0781) | Acc: (97.45%) (15093/15488)\n",
            "Epoch: 75 | Batch_idx: 130 |  Loss: (0.0776) | Acc: (97.47%) (16344/16768)\n",
            "Epoch: 75 | Batch_idx: 140 |  Loss: (0.0770) | Acc: (97.52%) (17601/18048)\n",
            "Epoch: 75 | Batch_idx: 150 |  Loss: (0.0759) | Acc: (97.58%) (18861/19328)\n",
            "Epoch: 75 | Batch_idx: 160 |  Loss: (0.0753) | Acc: (97.62%) (20118/20608)\n",
            "Epoch: 75 | Batch_idx: 170 |  Loss: (0.0756) | Acc: (97.58%) (21358/21888)\n",
            "Epoch: 75 | Batch_idx: 180 |  Loss: (0.0749) | Acc: (97.60%) (22612/23168)\n",
            "Epoch: 75 | Batch_idx: 190 |  Loss: (0.0743) | Acc: (97.62%) (23867/24448)\n",
            "Epoch: 75 | Batch_idx: 200 |  Loss: (0.0742) | Acc: (97.62%) (25116/25728)\n",
            "Epoch: 75 | Batch_idx: 210 |  Loss: (0.0739) | Acc: (97.63%) (26369/27008)\n",
            "Epoch: 75 | Batch_idx: 220 |  Loss: (0.0738) | Acc: (97.63%) (27618/28288)\n",
            "Epoch: 75 | Batch_idx: 230 |  Loss: (0.0737) | Acc: (97.63%) (28867/29568)\n",
            "Epoch: 75 | Batch_idx: 240 |  Loss: (0.0733) | Acc: (97.65%) (30124/30848)\n",
            "Epoch: 75 | Batch_idx: 250 |  Loss: (0.0740) | Acc: (97.63%) (31365/32128)\n",
            "Epoch: 75 | Batch_idx: 260 |  Loss: (0.0739) | Acc: (97.62%) (32613/33408)\n",
            "Epoch: 75 | Batch_idx: 270 |  Loss: (0.0744) | Acc: (97.61%) (33859/34688)\n",
            "Epoch: 75 | Batch_idx: 280 |  Loss: (0.0747) | Acc: (97.60%) (35103/35968)\n",
            "Epoch: 75 | Batch_idx: 290 |  Loss: (0.0746) | Acc: (97.60%) (36354/37248)\n",
            "Epoch: 75 | Batch_idx: 300 |  Loss: (0.0749) | Acc: (97.58%) (37597/38528)\n",
            "Epoch: 75 | Batch_idx: 310 |  Loss: (0.0751) | Acc: (97.59%) (38848/39808)\n",
            "Epoch: 75 | Batch_idx: 320 |  Loss: (0.0755) | Acc: (97.57%) (40090/41088)\n",
            "Epoch: 75 | Batch_idx: 330 |  Loss: (0.0754) | Acc: (97.57%) (41339/42368)\n",
            "Epoch: 75 | Batch_idx: 340 |  Loss: (0.0757) | Acc: (97.58%) (42590/43648)\n",
            "Epoch: 75 | Batch_idx: 350 |  Loss: (0.0760) | Acc: (97.56%) (43832/44928)\n",
            "Epoch: 75 | Batch_idx: 360 |  Loss: (0.0765) | Acc: (97.54%) (45072/46208)\n",
            "Epoch: 75 | Batch_idx: 370 |  Loss: (0.0769) | Acc: (97.53%) (46314/47488)\n",
            "Epoch: 75 | Batch_idx: 380 |  Loss: (0.0770) | Acc: (97.51%) (47555/48768)\n",
            "Epoch: 75 | Batch_idx: 390 |  Loss: (0.0771) | Acc: (97.51%) (48753/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3518) | Acc: (90.01%) (9001/10000)\n",
            "Epoch: 76 | Batch_idx: 0 |  Loss: (0.0624) | Acc: (97.66%) (125/128)\n",
            "Epoch: 76 | Batch_idx: 10 |  Loss: (0.0724) | Acc: (97.87%) (1378/1408)\n",
            "Epoch: 76 | Batch_idx: 20 |  Loss: (0.0776) | Acc: (97.54%) (2622/2688)\n",
            "Epoch: 76 | Batch_idx: 30 |  Loss: (0.0743) | Acc: (97.58%) (3872/3968)\n",
            "Epoch: 76 | Batch_idx: 40 |  Loss: (0.0699) | Acc: (97.75%) (5130/5248)\n",
            "Epoch: 76 | Batch_idx: 50 |  Loss: (0.0691) | Acc: (97.73%) (6380/6528)\n",
            "Epoch: 76 | Batch_idx: 60 |  Loss: (0.0684) | Acc: (97.78%) (7635/7808)\n",
            "Epoch: 76 | Batch_idx: 70 |  Loss: (0.0687) | Acc: (97.78%) (8886/9088)\n",
            "Epoch: 76 | Batch_idx: 80 |  Loss: (0.0699) | Acc: (97.69%) (10128/10368)\n",
            "Epoch: 76 | Batch_idx: 90 |  Loss: (0.0692) | Acc: (97.71%) (11381/11648)\n",
            "Epoch: 76 | Batch_idx: 100 |  Loss: (0.0687) | Acc: (97.75%) (12637/12928)\n",
            "Epoch: 76 | Batch_idx: 110 |  Loss: (0.0695) | Acc: (97.76%) (13890/14208)\n",
            "Epoch: 76 | Batch_idx: 120 |  Loss: (0.0693) | Acc: (97.75%) (15140/15488)\n",
            "Epoch: 76 | Batch_idx: 130 |  Loss: (0.0695) | Acc: (97.78%) (16396/16768)\n",
            "Epoch: 76 | Batch_idx: 140 |  Loss: (0.0696) | Acc: (97.78%) (17647/18048)\n",
            "Epoch: 76 | Batch_idx: 150 |  Loss: (0.0703) | Acc: (97.76%) (18895/19328)\n",
            "Epoch: 76 | Batch_idx: 160 |  Loss: (0.0705) | Acc: (97.75%) (20145/20608)\n",
            "Epoch: 76 | Batch_idx: 170 |  Loss: (0.0704) | Acc: (97.77%) (21400/21888)\n",
            "Epoch: 76 | Batch_idx: 180 |  Loss: (0.0709) | Acc: (97.74%) (22645/23168)\n",
            "Epoch: 76 | Batch_idx: 190 |  Loss: (0.0714) | Acc: (97.73%) (23894/24448)\n",
            "Epoch: 76 | Batch_idx: 200 |  Loss: (0.0710) | Acc: (97.76%) (25151/25728)\n",
            "Epoch: 76 | Batch_idx: 210 |  Loss: (0.0713) | Acc: (97.76%) (26402/27008)\n",
            "Epoch: 76 | Batch_idx: 220 |  Loss: (0.0711) | Acc: (97.76%) (27653/28288)\n",
            "Epoch: 76 | Batch_idx: 230 |  Loss: (0.0712) | Acc: (97.75%) (28904/29568)\n",
            "Epoch: 76 | Batch_idx: 240 |  Loss: (0.0712) | Acc: (97.72%) (30146/30848)\n",
            "Epoch: 76 | Batch_idx: 250 |  Loss: (0.0723) | Acc: (97.69%) (31386/32128)\n",
            "Epoch: 76 | Batch_idx: 260 |  Loss: (0.0723) | Acc: (97.69%) (32637/33408)\n",
            "Epoch: 76 | Batch_idx: 270 |  Loss: (0.0723) | Acc: (97.69%) (33887/34688)\n",
            "Epoch: 76 | Batch_idx: 280 |  Loss: (0.0730) | Acc: (97.66%) (35126/35968)\n",
            "Epoch: 76 | Batch_idx: 290 |  Loss: (0.0732) | Acc: (97.66%) (36376/37248)\n",
            "Epoch: 76 | Batch_idx: 300 |  Loss: (0.0736) | Acc: (97.64%) (37619/38528)\n",
            "Epoch: 76 | Batch_idx: 310 |  Loss: (0.0738) | Acc: (97.63%) (38863/39808)\n",
            "Epoch: 76 | Batch_idx: 320 |  Loss: (0.0737) | Acc: (97.63%) (40114/41088)\n",
            "Epoch: 76 | Batch_idx: 330 |  Loss: (0.0738) | Acc: (97.63%) (41363/42368)\n",
            "Epoch: 76 | Batch_idx: 340 |  Loss: (0.0742) | Acc: (97.62%) (42607/43648)\n",
            "Epoch: 76 | Batch_idx: 350 |  Loss: (0.0745) | Acc: (97.62%) (43859/44928)\n",
            "Epoch: 76 | Batch_idx: 360 |  Loss: (0.0749) | Acc: (97.60%) (45097/46208)\n",
            "Epoch: 76 | Batch_idx: 370 |  Loss: (0.0749) | Acc: (97.59%) (46342/47488)\n",
            "Epoch: 76 | Batch_idx: 380 |  Loss: (0.0756) | Acc: (97.55%) (47573/48768)\n",
            "Epoch: 76 | Batch_idx: 390 |  Loss: (0.0761) | Acc: (97.52%) (48760/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3960) | Acc: (89.07%) (8907/10000)\n",
            "Epoch: 77 | Batch_idx: 0 |  Loss: (0.0625) | Acc: (97.66%) (125/128)\n",
            "Epoch: 77 | Batch_idx: 10 |  Loss: (0.0978) | Acc: (96.95%) (1365/1408)\n",
            "Epoch: 77 | Batch_idx: 20 |  Loss: (0.0882) | Acc: (97.14%) (2611/2688)\n",
            "Epoch: 77 | Batch_idx: 30 |  Loss: (0.0882) | Acc: (97.13%) (3854/3968)\n",
            "Epoch: 77 | Batch_idx: 40 |  Loss: (0.0864) | Acc: (97.12%) (5097/5248)\n",
            "Epoch: 77 | Batch_idx: 50 |  Loss: (0.0840) | Acc: (97.21%) (6346/6528)\n",
            "Epoch: 77 | Batch_idx: 60 |  Loss: (0.0822) | Acc: (97.25%) (7593/7808)\n",
            "Epoch: 77 | Batch_idx: 70 |  Loss: (0.0807) | Acc: (97.23%) (8836/9088)\n",
            "Epoch: 77 | Batch_idx: 80 |  Loss: (0.0797) | Acc: (97.30%) (10088/10368)\n",
            "Epoch: 77 | Batch_idx: 90 |  Loss: (0.0794) | Acc: (97.28%) (11331/11648)\n",
            "Epoch: 77 | Batch_idx: 100 |  Loss: (0.0787) | Acc: (97.29%) (12578/12928)\n",
            "Epoch: 77 | Batch_idx: 110 |  Loss: (0.0782) | Acc: (97.32%) (13827/14208)\n",
            "Epoch: 77 | Batch_idx: 120 |  Loss: (0.0779) | Acc: (97.32%) (15073/15488)\n",
            "Epoch: 77 | Batch_idx: 130 |  Loss: (0.0766) | Acc: (97.36%) (16326/16768)\n",
            "Epoch: 77 | Batch_idx: 140 |  Loss: (0.0764) | Acc: (97.40%) (17578/18048)\n",
            "Epoch: 77 | Batch_idx: 150 |  Loss: (0.0759) | Acc: (97.42%) (18830/19328)\n",
            "Epoch: 77 | Batch_idx: 160 |  Loss: (0.0756) | Acc: (97.47%) (20086/20608)\n",
            "Epoch: 77 | Batch_idx: 170 |  Loss: (0.0755) | Acc: (97.48%) (21336/21888)\n",
            "Epoch: 77 | Batch_idx: 180 |  Loss: (0.0745) | Acc: (97.52%) (22593/23168)\n",
            "Epoch: 77 | Batch_idx: 190 |  Loss: (0.0738) | Acc: (97.56%) (23852/24448)\n",
            "Epoch: 77 | Batch_idx: 200 |  Loss: (0.0731) | Acc: (97.61%) (25114/25728)\n",
            "Epoch: 77 | Batch_idx: 210 |  Loss: (0.0725) | Acc: (97.65%) (26372/27008)\n",
            "Epoch: 77 | Batch_idx: 220 |  Loss: (0.0730) | Acc: (97.64%) (27619/28288)\n",
            "Epoch: 77 | Batch_idx: 230 |  Loss: (0.0733) | Acc: (97.65%) (28873/29568)\n",
            "Epoch: 77 | Batch_idx: 240 |  Loss: (0.0733) | Acc: (97.64%) (30120/30848)\n",
            "Epoch: 77 | Batch_idx: 250 |  Loss: (0.0738) | Acc: (97.61%) (31360/32128)\n",
            "Epoch: 77 | Batch_idx: 260 |  Loss: (0.0735) | Acc: (97.63%) (32616/33408)\n",
            "Epoch: 77 | Batch_idx: 270 |  Loss: (0.0735) | Acc: (97.63%) (33867/34688)\n",
            "Epoch: 77 | Batch_idx: 280 |  Loss: (0.0735) | Acc: (97.65%) (35121/35968)\n",
            "Epoch: 77 | Batch_idx: 290 |  Loss: (0.0740) | Acc: (97.62%) (36362/37248)\n",
            "Epoch: 77 | Batch_idx: 300 |  Loss: (0.0740) | Acc: (97.62%) (37611/38528)\n",
            "Epoch: 77 | Batch_idx: 310 |  Loss: (0.0743) | Acc: (97.60%) (38853/39808)\n",
            "Epoch: 77 | Batch_idx: 320 |  Loss: (0.0742) | Acc: (97.60%) (40103/41088)\n",
            "Epoch: 77 | Batch_idx: 330 |  Loss: (0.0751) | Acc: (97.57%) (41337/42368)\n",
            "Epoch: 77 | Batch_idx: 340 |  Loss: (0.0754) | Acc: (97.56%) (42581/43648)\n",
            "Epoch: 77 | Batch_idx: 350 |  Loss: (0.0760) | Acc: (97.54%) (43821/44928)\n",
            "Epoch: 77 | Batch_idx: 360 |  Loss: (0.0770) | Acc: (97.51%) (45058/46208)\n",
            "Epoch: 77 | Batch_idx: 370 |  Loss: (0.0772) | Acc: (97.50%) (46302/47488)\n",
            "Epoch: 77 | Batch_idx: 380 |  Loss: (0.0782) | Acc: (97.47%) (47536/48768)\n",
            "Epoch: 77 | Batch_idx: 390 |  Loss: (0.0777) | Acc: (97.49%) (48743/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3802) | Acc: (89.60%) (8960/10000)\n",
            "Epoch: 78 | Batch_idx: 0 |  Loss: (0.0651) | Acc: (97.66%) (125/128)\n",
            "Epoch: 78 | Batch_idx: 10 |  Loss: (0.0695) | Acc: (97.66%) (1375/1408)\n",
            "Epoch: 78 | Batch_idx: 20 |  Loss: (0.0679) | Acc: (97.69%) (2626/2688)\n",
            "Epoch: 78 | Batch_idx: 30 |  Loss: (0.0660) | Acc: (97.76%) (3879/3968)\n",
            "Epoch: 78 | Batch_idx: 40 |  Loss: (0.0641) | Acc: (97.77%) (5131/5248)\n",
            "Epoch: 78 | Batch_idx: 50 |  Loss: (0.0649) | Acc: (97.78%) (6383/6528)\n",
            "Epoch: 78 | Batch_idx: 60 |  Loss: (0.0666) | Acc: (97.69%) (7628/7808)\n",
            "Epoch: 78 | Batch_idx: 70 |  Loss: (0.0666) | Acc: (97.71%) (8880/9088)\n",
            "Epoch: 78 | Batch_idx: 80 |  Loss: (0.0672) | Acc: (97.78%) (10138/10368)\n",
            "Epoch: 78 | Batch_idx: 90 |  Loss: (0.0673) | Acc: (97.78%) (11389/11648)\n",
            "Epoch: 78 | Batch_idx: 100 |  Loss: (0.0686) | Acc: (97.77%) (12640/12928)\n",
            "Epoch: 78 | Batch_idx: 110 |  Loss: (0.0677) | Acc: (97.79%) (13894/14208)\n",
            "Epoch: 78 | Batch_idx: 120 |  Loss: (0.0690) | Acc: (97.73%) (15137/15488)\n",
            "Epoch: 78 | Batch_idx: 130 |  Loss: (0.0692) | Acc: (97.72%) (16385/16768)\n",
            "Epoch: 78 | Batch_idx: 140 |  Loss: (0.0696) | Acc: (97.67%) (17628/18048)\n",
            "Epoch: 78 | Batch_idx: 150 |  Loss: (0.0700) | Acc: (97.68%) (18879/19328)\n",
            "Epoch: 78 | Batch_idx: 160 |  Loss: (0.0700) | Acc: (97.65%) (20123/20608)\n",
            "Epoch: 78 | Batch_idx: 170 |  Loss: (0.0702) | Acc: (97.63%) (21370/21888)\n",
            "Epoch: 78 | Batch_idx: 180 |  Loss: (0.0708) | Acc: (97.60%) (22612/23168)\n",
            "Epoch: 78 | Batch_idx: 190 |  Loss: (0.0711) | Acc: (97.60%) (23862/24448)\n",
            "Epoch: 78 | Batch_idx: 200 |  Loss: (0.0712) | Acc: (97.60%) (25111/25728)\n",
            "Epoch: 78 | Batch_idx: 210 |  Loss: (0.0714) | Acc: (97.57%) (26353/27008)\n",
            "Epoch: 78 | Batch_idx: 220 |  Loss: (0.0720) | Acc: (97.58%) (27604/28288)\n",
            "Epoch: 78 | Batch_idx: 230 |  Loss: (0.0719) | Acc: (97.60%) (28857/29568)\n",
            "Epoch: 78 | Batch_idx: 240 |  Loss: (0.0727) | Acc: (97.56%) (30095/30848)\n",
            "Epoch: 78 | Batch_idx: 250 |  Loss: (0.0728) | Acc: (97.57%) (31346/32128)\n",
            "Epoch: 78 | Batch_idx: 260 |  Loss: (0.0735) | Acc: (97.54%) (32585/33408)\n",
            "Epoch: 78 | Batch_idx: 270 |  Loss: (0.0737) | Acc: (97.55%) (33837/34688)\n",
            "Epoch: 78 | Batch_idx: 280 |  Loss: (0.0737) | Acc: (97.55%) (35085/35968)\n",
            "Epoch: 78 | Batch_idx: 290 |  Loss: (0.0739) | Acc: (97.55%) (36336/37248)\n",
            "Epoch: 78 | Batch_idx: 300 |  Loss: (0.0741) | Acc: (97.55%) (37583/38528)\n",
            "Epoch: 78 | Batch_idx: 310 |  Loss: (0.0741) | Acc: (97.55%) (38833/39808)\n",
            "Epoch: 78 | Batch_idx: 320 |  Loss: (0.0738) | Acc: (97.57%) (40089/41088)\n",
            "Epoch: 78 | Batch_idx: 330 |  Loss: (0.0742) | Acc: (97.55%) (41332/42368)\n",
            "Epoch: 78 | Batch_idx: 340 |  Loss: (0.0744) | Acc: (97.54%) (42574/43648)\n",
            "Epoch: 78 | Batch_idx: 350 |  Loss: (0.0745) | Acc: (97.54%) (43825/44928)\n",
            "Epoch: 78 | Batch_idx: 360 |  Loss: (0.0750) | Acc: (97.53%) (45068/46208)\n",
            "Epoch: 78 | Batch_idx: 370 |  Loss: (0.0753) | Acc: (97.52%) (46309/47488)\n",
            "Epoch: 78 | Batch_idx: 380 |  Loss: (0.0756) | Acc: (97.51%) (47554/48768)\n",
            "Epoch: 78 | Batch_idx: 390 |  Loss: (0.0759) | Acc: (97.50%) (48749/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.4235) | Acc: (88.63%) (8863/10000)\n",
            "Epoch: 79 | Batch_idx: 0 |  Loss: (0.0726) | Acc: (96.88%) (124/128)\n",
            "Epoch: 79 | Batch_idx: 10 |  Loss: (0.0684) | Acc: (97.80%) (1377/1408)\n",
            "Epoch: 79 | Batch_idx: 20 |  Loss: (0.0662) | Acc: (97.77%) (2628/2688)\n",
            "Epoch: 79 | Batch_idx: 30 |  Loss: (0.0642) | Acc: (97.93%) (3886/3968)\n",
            "Epoch: 79 | Batch_idx: 40 |  Loss: (0.0633) | Acc: (98.04%) (5145/5248)\n",
            "Epoch: 79 | Batch_idx: 50 |  Loss: (0.0641) | Acc: (98.05%) (6401/6528)\n",
            "Epoch: 79 | Batch_idx: 60 |  Loss: (0.0639) | Acc: (98.00%) (7652/7808)\n",
            "Epoch: 79 | Batch_idx: 70 |  Loss: (0.0661) | Acc: (97.92%) (8899/9088)\n",
            "Epoch: 79 | Batch_idx: 80 |  Loss: (0.0667) | Acc: (97.89%) (10149/10368)\n",
            "Epoch: 79 | Batch_idx: 90 |  Loss: (0.0656) | Acc: (97.91%) (11405/11648)\n",
            "Epoch: 79 | Batch_idx: 100 |  Loss: (0.0656) | Acc: (97.87%) (12652/12928)\n",
            "Epoch: 79 | Batch_idx: 110 |  Loss: (0.0662) | Acc: (97.84%) (13901/14208)\n",
            "Epoch: 79 | Batch_idx: 120 |  Loss: (0.0664) | Acc: (97.83%) (15152/15488)\n",
            "Epoch: 79 | Batch_idx: 130 |  Loss: (0.0684) | Acc: (97.74%) (16389/16768)\n",
            "Epoch: 79 | Batch_idx: 140 |  Loss: (0.0689) | Acc: (97.71%) (17634/18048)\n",
            "Epoch: 79 | Batch_idx: 150 |  Loss: (0.0697) | Acc: (97.68%) (18880/19328)\n",
            "Epoch: 79 | Batch_idx: 160 |  Loss: (0.0698) | Acc: (97.67%) (20127/20608)\n",
            "Epoch: 79 | Batch_idx: 170 |  Loss: (0.0698) | Acc: (97.67%) (21379/21888)\n",
            "Epoch: 79 | Batch_idx: 180 |  Loss: (0.0703) | Acc: (97.66%) (22625/23168)\n",
            "Epoch: 79 | Batch_idx: 190 |  Loss: (0.0703) | Acc: (97.67%) (23879/24448)\n",
            "Epoch: 79 | Batch_idx: 200 |  Loss: (0.0698) | Acc: (97.70%) (25136/25728)\n",
            "Epoch: 79 | Batch_idx: 210 |  Loss: (0.0704) | Acc: (97.67%) (26379/27008)\n",
            "Epoch: 79 | Batch_idx: 220 |  Loss: (0.0709) | Acc: (97.65%) (27622/28288)\n",
            "Epoch: 79 | Batch_idx: 230 |  Loss: (0.0719) | Acc: (97.60%) (28857/29568)\n",
            "Epoch: 79 | Batch_idx: 240 |  Loss: (0.0719) | Acc: (97.60%) (30108/30848)\n",
            "Epoch: 79 | Batch_idx: 250 |  Loss: (0.0722) | Acc: (97.59%) (31353/32128)\n",
            "Epoch: 79 | Batch_idx: 260 |  Loss: (0.0722) | Acc: (97.59%) (32603/33408)\n",
            "Epoch: 79 | Batch_idx: 270 |  Loss: (0.0722) | Acc: (97.59%) (33852/34688)\n",
            "Epoch: 79 | Batch_idx: 280 |  Loss: (0.0722) | Acc: (97.60%) (35105/35968)\n",
            "Epoch: 79 | Batch_idx: 290 |  Loss: (0.0723) | Acc: (97.60%) (36354/37248)\n",
            "Epoch: 79 | Batch_idx: 300 |  Loss: (0.0722) | Acc: (97.61%) (37607/38528)\n",
            "Epoch: 79 | Batch_idx: 310 |  Loss: (0.0727) | Acc: (97.59%) (38849/39808)\n",
            "Epoch: 79 | Batch_idx: 320 |  Loss: (0.0732) | Acc: (97.57%) (40089/41088)\n",
            "Epoch: 79 | Batch_idx: 330 |  Loss: (0.0741) | Acc: (97.54%) (41325/42368)\n",
            "Epoch: 79 | Batch_idx: 340 |  Loss: (0.0746) | Acc: (97.51%) (42561/43648)\n",
            "Epoch: 79 | Batch_idx: 350 |  Loss: (0.0754) | Acc: (97.48%) (43797/44928)\n",
            "Epoch: 79 | Batch_idx: 360 |  Loss: (0.0754) | Acc: (97.48%) (45043/46208)\n",
            "Epoch: 79 | Batch_idx: 370 |  Loss: (0.0758) | Acc: (97.46%) (46284/47488)\n",
            "Epoch: 79 | Batch_idx: 380 |  Loss: (0.0760) | Acc: (97.46%) (47531/48768)\n",
            "Epoch: 79 | Batch_idx: 390 |  Loss: (0.0764) | Acc: (97.46%) (48730/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.3619) | Acc: (90.20%) (9020/10000)\n",
            "Epoch: 80 | Batch_idx: 0 |  Loss: (0.0591) | Acc: (98.44%) (126/128)\n",
            "Epoch: 80 | Batch_idx: 10 |  Loss: (0.0705) | Acc: (97.51%) (1373/1408)\n",
            "Epoch: 80 | Batch_idx: 20 |  Loss: (0.0665) | Acc: (97.73%) (2627/2688)\n",
            "Epoch: 80 | Batch_idx: 30 |  Loss: (0.0656) | Acc: (97.86%) (3883/3968)\n",
            "Epoch: 80 | Batch_idx: 40 |  Loss: (0.0637) | Acc: (97.92%) (5139/5248)\n",
            "Epoch: 80 | Batch_idx: 50 |  Loss: (0.0617) | Acc: (98.01%) (6398/6528)\n",
            "Epoch: 80 | Batch_idx: 60 |  Loss: (0.0585) | Acc: (98.09%) (7659/7808)\n",
            "Epoch: 80 | Batch_idx: 70 |  Loss: (0.0566) | Acc: (98.20%) (8924/9088)\n",
            "Epoch: 80 | Batch_idx: 80 |  Loss: (0.0555) | Acc: (98.29%) (10191/10368)\n",
            "Epoch: 80 | Batch_idx: 90 |  Loss: (0.0545) | Acc: (98.35%) (11456/11648)\n",
            "Epoch: 80 | Batch_idx: 100 |  Loss: (0.0534) | Acc: (98.39%) (12720/12928)\n",
            "Epoch: 80 | Batch_idx: 110 |  Loss: (0.0533) | Acc: (98.40%) (13980/14208)\n",
            "Epoch: 80 | Batch_idx: 120 |  Loss: (0.0517) | Acc: (98.46%) (15250/15488)\n",
            "Epoch: 80 | Batch_idx: 130 |  Loss: (0.0515) | Acc: (98.46%) (16509/16768)\n",
            "Epoch: 80 | Batch_idx: 140 |  Loss: (0.0502) | Acc: (98.53%) (17782/18048)\n",
            "Epoch: 80 | Batch_idx: 150 |  Loss: (0.0496) | Acc: (98.56%) (19049/19328)\n",
            "Epoch: 80 | Batch_idx: 160 |  Loss: (0.0491) | Acc: (98.57%) (20313/20608)\n",
            "Epoch: 80 | Batch_idx: 170 |  Loss: (0.0486) | Acc: (98.59%) (21579/21888)\n",
            "Epoch: 80 | Batch_idx: 180 |  Loss: (0.0478) | Acc: (98.62%) (22848/23168)\n",
            "Epoch: 80 | Batch_idx: 190 |  Loss: (0.0473) | Acc: (98.63%) (24114/24448)\n",
            "Epoch: 80 | Batch_idx: 200 |  Loss: (0.0471) | Acc: (98.64%) (25378/25728)\n",
            "Epoch: 80 | Batch_idx: 210 |  Loss: (0.0464) | Acc: (98.67%) (26649/27008)\n",
            "Epoch: 80 | Batch_idx: 220 |  Loss: (0.0461) | Acc: (98.70%) (27919/28288)\n",
            "Epoch: 80 | Batch_idx: 230 |  Loss: (0.0460) | Acc: (98.70%) (29184/29568)\n",
            "Epoch: 80 | Batch_idx: 240 |  Loss: (0.0454) | Acc: (98.73%) (30457/30848)\n",
            "Epoch: 80 | Batch_idx: 250 |  Loss: (0.0449) | Acc: (98.73%) (31721/32128)\n",
            "Epoch: 80 | Batch_idx: 260 |  Loss: (0.0444) | Acc: (98.75%) (32992/33408)\n",
            "Epoch: 80 | Batch_idx: 270 |  Loss: (0.0442) | Acc: (98.75%) (34256/34688)\n",
            "Epoch: 80 | Batch_idx: 280 |  Loss: (0.0440) | Acc: (98.76%) (35522/35968)\n",
            "Epoch: 80 | Batch_idx: 290 |  Loss: (0.0439) | Acc: (98.77%) (36789/37248)\n",
            "Epoch: 80 | Batch_idx: 300 |  Loss: (0.0434) | Acc: (98.79%) (38062/38528)\n",
            "Epoch: 80 | Batch_idx: 310 |  Loss: (0.0428) | Acc: (98.81%) (39336/39808)\n",
            "Epoch: 80 | Batch_idx: 320 |  Loss: (0.0424) | Acc: (98.83%) (40607/41088)\n",
            "Epoch: 80 | Batch_idx: 330 |  Loss: (0.0422) | Acc: (98.84%) (41877/42368)\n",
            "Epoch: 80 | Batch_idx: 340 |  Loss: (0.0419) | Acc: (98.85%) (43146/43648)\n",
            "Epoch: 80 | Batch_idx: 350 |  Loss: (0.0419) | Acc: (98.85%) (44410/44928)\n",
            "Epoch: 80 | Batch_idx: 360 |  Loss: (0.0415) | Acc: (98.86%) (45681/46208)\n",
            "Epoch: 80 | Batch_idx: 370 |  Loss: (0.0412) | Acc: (98.87%) (46953/47488)\n",
            "Epoch: 80 | Batch_idx: 380 |  Loss: (0.0409) | Acc: (98.89%) (48227/48768)\n",
            "Epoch: 80 | Batch_idx: 390 |  Loss: (0.0406) | Acc: (98.90%) (49449/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2801) | Acc: (92.15%) (9215/10000)\n",
            "Epoch: 81 | Batch_idx: 0 |  Loss: (0.0347) | Acc: (100.00%) (128/128)\n",
            "Epoch: 81 | Batch_idx: 10 |  Loss: (0.0254) | Acc: (99.50%) (1401/1408)\n",
            "Epoch: 81 | Batch_idx: 20 |  Loss: (0.0256) | Acc: (99.52%) (2675/2688)\n",
            "Epoch: 81 | Batch_idx: 30 |  Loss: (0.0247) | Acc: (99.57%) (3951/3968)\n",
            "Epoch: 81 | Batch_idx: 40 |  Loss: (0.0237) | Acc: (99.66%) (5230/5248)\n",
            "Epoch: 81 | Batch_idx: 50 |  Loss: (0.0240) | Acc: (99.59%) (6501/6528)\n",
            "Epoch: 81 | Batch_idx: 60 |  Loss: (0.0252) | Acc: (99.50%) (7769/7808)\n",
            "Epoch: 81 | Batch_idx: 70 |  Loss: (0.0260) | Acc: (99.47%) (9040/9088)\n",
            "Epoch: 81 | Batch_idx: 80 |  Loss: (0.0273) | Acc: (99.40%) (10306/10368)\n",
            "Epoch: 81 | Batch_idx: 90 |  Loss: (0.0276) | Acc: (99.38%) (11576/11648)\n",
            "Epoch: 81 | Batch_idx: 100 |  Loss: (0.0271) | Acc: (99.40%) (12851/12928)\n",
            "Epoch: 81 | Batch_idx: 110 |  Loss: (0.0273) | Acc: (99.42%) (14125/14208)\n",
            "Epoch: 81 | Batch_idx: 120 |  Loss: (0.0275) | Acc: (99.40%) (15395/15488)\n",
            "Epoch: 81 | Batch_idx: 130 |  Loss: (0.0279) | Acc: (99.37%) (16663/16768)\n",
            "Epoch: 81 | Batch_idx: 140 |  Loss: (0.0281) | Acc: (99.37%) (17934/18048)\n",
            "Epoch: 81 | Batch_idx: 150 |  Loss: (0.0283) | Acc: (99.36%) (19204/19328)\n",
            "Epoch: 81 | Batch_idx: 160 |  Loss: (0.0280) | Acc: (99.37%) (20478/20608)\n",
            "Epoch: 81 | Batch_idx: 170 |  Loss: (0.0287) | Acc: (99.34%) (21743/21888)\n",
            "Epoch: 81 | Batch_idx: 180 |  Loss: (0.0283) | Acc: (99.34%) (23016/23168)\n",
            "Epoch: 81 | Batch_idx: 190 |  Loss: (0.0283) | Acc: (99.35%) (24288/24448)\n",
            "Epoch: 81 | Batch_idx: 200 |  Loss: (0.0279) | Acc: (99.36%) (25564/25728)\n",
            "Epoch: 81 | Batch_idx: 210 |  Loss: (0.0280) | Acc: (99.34%) (26831/27008)\n",
            "Epoch: 81 | Batch_idx: 220 |  Loss: (0.0281) | Acc: (99.35%) (28104/28288)\n",
            "Epoch: 81 | Batch_idx: 230 |  Loss: (0.0283) | Acc: (99.35%) (29376/29568)\n",
            "Epoch: 81 | Batch_idx: 240 |  Loss: (0.0281) | Acc: (99.36%) (30650/30848)\n",
            "Epoch: 81 | Batch_idx: 250 |  Loss: (0.0284) | Acc: (99.35%) (31918/32128)\n",
            "Epoch: 81 | Batch_idx: 260 |  Loss: (0.0283) | Acc: (99.35%) (33190/33408)\n",
            "Epoch: 81 | Batch_idx: 270 |  Loss: (0.0284) | Acc: (99.35%) (34463/34688)\n",
            "Epoch: 81 | Batch_idx: 280 |  Loss: (0.0285) | Acc: (99.35%) (35733/35968)\n",
            "Epoch: 81 | Batch_idx: 290 |  Loss: (0.0285) | Acc: (99.34%) (37003/37248)\n",
            "Epoch: 81 | Batch_idx: 300 |  Loss: (0.0284) | Acc: (99.35%) (38279/38528)\n",
            "Epoch: 81 | Batch_idx: 310 |  Loss: (0.0286) | Acc: (99.35%) (39548/39808)\n",
            "Epoch: 81 | Batch_idx: 320 |  Loss: (0.0285) | Acc: (99.34%) (40818/41088)\n",
            "Epoch: 81 | Batch_idx: 330 |  Loss: (0.0285) | Acc: (99.35%) (42093/42368)\n",
            "Epoch: 81 | Batch_idx: 340 |  Loss: (0.0284) | Acc: (99.35%) (43363/43648)\n",
            "Epoch: 81 | Batch_idx: 350 |  Loss: (0.0282) | Acc: (99.35%) (44638/44928)\n",
            "Epoch: 81 | Batch_idx: 360 |  Loss: (0.0282) | Acc: (99.35%) (45909/46208)\n",
            "Epoch: 81 | Batch_idx: 370 |  Loss: (0.0281) | Acc: (99.36%) (47182/47488)\n",
            "Epoch: 81 | Batch_idx: 380 |  Loss: (0.0279) | Acc: (99.36%) (48457/48768)\n",
            "Epoch: 81 | Batch_idx: 390 |  Loss: (0.0279) | Acc: (99.36%) (49680/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2691) | Acc: (92.26%) (9226/10000)\n",
            "Epoch: 82 | Batch_idx: 0 |  Loss: (0.0293) | Acc: (100.00%) (128/128)\n",
            "Epoch: 82 | Batch_idx: 10 |  Loss: (0.0232) | Acc: (99.50%) (1401/1408)\n",
            "Epoch: 82 | Batch_idx: 20 |  Loss: (0.0217) | Acc: (99.59%) (2677/2688)\n",
            "Epoch: 82 | Batch_idx: 30 |  Loss: (0.0202) | Acc: (99.62%) (3953/3968)\n",
            "Epoch: 82 | Batch_idx: 40 |  Loss: (0.0208) | Acc: (99.64%) (5229/5248)\n",
            "Epoch: 82 | Batch_idx: 50 |  Loss: (0.0216) | Acc: (99.63%) (6504/6528)\n",
            "Epoch: 82 | Batch_idx: 60 |  Loss: (0.0216) | Acc: (99.65%) (7781/7808)\n",
            "Epoch: 82 | Batch_idx: 70 |  Loss: (0.0217) | Acc: (99.65%) (9056/9088)\n",
            "Epoch: 82 | Batch_idx: 80 |  Loss: (0.0229) | Acc: (99.59%) (10326/10368)\n",
            "Epoch: 82 | Batch_idx: 90 |  Loss: (0.0228) | Acc: (99.61%) (11602/11648)\n",
            "Epoch: 82 | Batch_idx: 100 |  Loss: (0.0228) | Acc: (99.61%) (12878/12928)\n",
            "Epoch: 82 | Batch_idx: 110 |  Loss: (0.0223) | Acc: (99.63%) (14156/14208)\n",
            "Epoch: 82 | Batch_idx: 120 |  Loss: (0.0226) | Acc: (99.63%) (15431/15488)\n",
            "Epoch: 82 | Batch_idx: 130 |  Loss: (0.0230) | Acc: (99.61%) (16703/16768)\n",
            "Epoch: 82 | Batch_idx: 140 |  Loss: (0.0228) | Acc: (99.61%) (17977/18048)\n",
            "Epoch: 82 | Batch_idx: 150 |  Loss: (0.0227) | Acc: (99.62%) (19254/19328)\n",
            "Epoch: 82 | Batch_idx: 160 |  Loss: (0.0228) | Acc: (99.58%) (20522/20608)\n",
            "Epoch: 82 | Batch_idx: 170 |  Loss: (0.0231) | Acc: (99.57%) (21793/21888)\n",
            "Epoch: 82 | Batch_idx: 180 |  Loss: (0.0233) | Acc: (99.55%) (23064/23168)\n",
            "Epoch: 82 | Batch_idx: 190 |  Loss: (0.0233) | Acc: (99.55%) (24339/24448)\n",
            "Epoch: 82 | Batch_idx: 200 |  Loss: (0.0233) | Acc: (99.56%) (25614/25728)\n",
            "Epoch: 82 | Batch_idx: 210 |  Loss: (0.0234) | Acc: (99.54%) (26885/27008)\n",
            "Epoch: 82 | Batch_idx: 220 |  Loss: (0.0234) | Acc: (99.54%) (28159/28288)\n",
            "Epoch: 82 | Batch_idx: 230 |  Loss: (0.0235) | Acc: (99.55%) (29434/29568)\n",
            "Epoch: 82 | Batch_idx: 240 |  Loss: (0.0236) | Acc: (99.54%) (30707/30848)\n",
            "Epoch: 82 | Batch_idx: 250 |  Loss: (0.0234) | Acc: (99.55%) (31985/32128)\n",
            "Epoch: 82 | Batch_idx: 260 |  Loss: (0.0235) | Acc: (99.56%) (33260/33408)\n",
            "Epoch: 82 | Batch_idx: 270 |  Loss: (0.0235) | Acc: (99.56%) (34535/34688)\n",
            "Epoch: 82 | Batch_idx: 280 |  Loss: (0.0233) | Acc: (99.57%) (35813/35968)\n",
            "Epoch: 82 | Batch_idx: 290 |  Loss: (0.0233) | Acc: (99.56%) (37085/37248)\n",
            "Epoch: 82 | Batch_idx: 300 |  Loss: (0.0232) | Acc: (99.57%) (38361/38528)\n",
            "Epoch: 82 | Batch_idx: 310 |  Loss: (0.0233) | Acc: (99.56%) (39634/39808)\n",
            "Epoch: 82 | Batch_idx: 320 |  Loss: (0.0231) | Acc: (99.57%) (40913/41088)\n",
            "Epoch: 82 | Batch_idx: 330 |  Loss: (0.0234) | Acc: (99.56%) (42182/42368)\n",
            "Epoch: 82 | Batch_idx: 340 |  Loss: (0.0233) | Acc: (99.57%) (43459/43648)\n",
            "Epoch: 82 | Batch_idx: 350 |  Loss: (0.0233) | Acc: (99.57%) (44735/44928)\n",
            "Epoch: 82 | Batch_idx: 360 |  Loss: (0.0234) | Acc: (99.57%) (46008/46208)\n",
            "Epoch: 82 | Batch_idx: 370 |  Loss: (0.0235) | Acc: (99.56%) (47278/47488)\n",
            "Epoch: 82 | Batch_idx: 380 |  Loss: (0.0235) | Acc: (99.55%) (48548/48768)\n",
            "Epoch: 82 | Batch_idx: 390 |  Loss: (0.0238) | Acc: (99.53%) (49766/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2668) | Acc: (92.69%) (9269/10000)\n",
            "Epoch: 83 | Batch_idx: 0 |  Loss: (0.0143) | Acc: (100.00%) (128/128)\n",
            "Epoch: 83 | Batch_idx: 10 |  Loss: (0.0191) | Acc: (99.79%) (1405/1408)\n",
            "Epoch: 83 | Batch_idx: 20 |  Loss: (0.0213) | Acc: (99.63%) (2678/2688)\n",
            "Epoch: 83 | Batch_idx: 30 |  Loss: (0.0210) | Acc: (99.70%) (3956/3968)\n",
            "Epoch: 83 | Batch_idx: 40 |  Loss: (0.0212) | Acc: (99.66%) (5230/5248)\n",
            "Epoch: 83 | Batch_idx: 50 |  Loss: (0.0210) | Acc: (99.63%) (6504/6528)\n",
            "Epoch: 83 | Batch_idx: 60 |  Loss: (0.0213) | Acc: (99.62%) (7778/7808)\n",
            "Epoch: 83 | Batch_idx: 70 |  Loss: (0.0206) | Acc: (99.66%) (9057/9088)\n",
            "Epoch: 83 | Batch_idx: 80 |  Loss: (0.0207) | Acc: (99.66%) (10333/10368)\n",
            "Epoch: 83 | Batch_idx: 90 |  Loss: (0.0208) | Acc: (99.63%) (11605/11648)\n",
            "Epoch: 83 | Batch_idx: 100 |  Loss: (0.0209) | Acc: (99.64%) (12881/12928)\n",
            "Epoch: 83 | Batch_idx: 110 |  Loss: (0.0211) | Acc: (99.63%) (14156/14208)\n",
            "Epoch: 83 | Batch_idx: 120 |  Loss: (0.0210) | Acc: (99.65%) (15434/15488)\n",
            "Epoch: 83 | Batch_idx: 130 |  Loss: (0.0210) | Acc: (99.65%) (16709/16768)\n",
            "Epoch: 83 | Batch_idx: 140 |  Loss: (0.0208) | Acc: (99.65%) (17985/18048)\n",
            "Epoch: 83 | Batch_idx: 150 |  Loss: (0.0205) | Acc: (99.65%) (19261/19328)\n",
            "Epoch: 83 | Batch_idx: 160 |  Loss: (0.0207) | Acc: (99.65%) (20536/20608)\n",
            "Epoch: 83 | Batch_idx: 170 |  Loss: (0.0209) | Acc: (99.65%) (21811/21888)\n",
            "Epoch: 83 | Batch_idx: 180 |  Loss: (0.0205) | Acc: (99.66%) (23090/23168)\n",
            "Epoch: 83 | Batch_idx: 190 |  Loss: (0.0206) | Acc: (99.66%) (24365/24448)\n",
            "Epoch: 83 | Batch_idx: 200 |  Loss: (0.0204) | Acc: (99.67%) (25642/25728)\n",
            "Epoch: 83 | Batch_idx: 210 |  Loss: (0.0205) | Acc: (99.65%) (26913/27008)\n",
            "Epoch: 83 | Batch_idx: 220 |  Loss: (0.0205) | Acc: (99.65%) (28189/28288)\n",
            "Epoch: 83 | Batch_idx: 230 |  Loss: (0.0205) | Acc: (99.65%) (29465/29568)\n",
            "Epoch: 83 | Batch_idx: 240 |  Loss: (0.0203) | Acc: (99.65%) (30741/30848)\n",
            "Epoch: 83 | Batch_idx: 250 |  Loss: (0.0205) | Acc: (99.65%) (32015/32128)\n",
            "Epoch: 83 | Batch_idx: 260 |  Loss: (0.0204) | Acc: (99.66%) (33293/33408)\n",
            "Epoch: 83 | Batch_idx: 270 |  Loss: (0.0208) | Acc: (99.64%) (34564/34688)\n",
            "Epoch: 83 | Batch_idx: 280 |  Loss: (0.0207) | Acc: (99.64%) (35838/35968)\n",
            "Epoch: 83 | Batch_idx: 290 |  Loss: (0.0210) | Acc: (99.63%) (37110/37248)\n",
            "Epoch: 83 | Batch_idx: 300 |  Loss: (0.0209) | Acc: (99.64%) (38388/38528)\n",
            "Epoch: 83 | Batch_idx: 310 |  Loss: (0.0209) | Acc: (99.63%) (39662/39808)\n",
            "Epoch: 83 | Batch_idx: 320 |  Loss: (0.0210) | Acc: (99.62%) (40931/41088)\n",
            "Epoch: 83 | Batch_idx: 330 |  Loss: (0.0211) | Acc: (99.62%) (42205/42368)\n",
            "Epoch: 83 | Batch_idx: 340 |  Loss: (0.0210) | Acc: (99.61%) (43479/43648)\n",
            "Epoch: 83 | Batch_idx: 350 |  Loss: (0.0210) | Acc: (99.61%) (44754/44928)\n",
            "Epoch: 83 | Batch_idx: 360 |  Loss: (0.0210) | Acc: (99.61%) (46030/46208)\n",
            "Epoch: 83 | Batch_idx: 370 |  Loss: (0.0210) | Acc: (99.61%) (47302/47488)\n",
            "Epoch: 83 | Batch_idx: 380 |  Loss: (0.0210) | Acc: (99.61%) (48576/48768)\n",
            "Epoch: 83 | Batch_idx: 390 |  Loss: (0.0211) | Acc: (99.60%) (49799/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2651) | Acc: (92.53%) (9253/10000)\n",
            "Epoch: 84 | Batch_idx: 0 |  Loss: (0.0206) | Acc: (100.00%) (128/128)\n",
            "Epoch: 84 | Batch_idx: 10 |  Loss: (0.0216) | Acc: (99.57%) (1402/1408)\n",
            "Epoch: 84 | Batch_idx: 20 |  Loss: (0.0217) | Acc: (99.67%) (2679/2688)\n",
            "Epoch: 84 | Batch_idx: 30 |  Loss: (0.0202) | Acc: (99.62%) (3953/3968)\n",
            "Epoch: 84 | Batch_idx: 40 |  Loss: (0.0208) | Acc: (99.58%) (5226/5248)\n",
            "Epoch: 84 | Batch_idx: 50 |  Loss: (0.0209) | Acc: (99.57%) (6500/6528)\n",
            "Epoch: 84 | Batch_idx: 60 |  Loss: (0.0203) | Acc: (99.59%) (7776/7808)\n",
            "Epoch: 84 | Batch_idx: 70 |  Loss: (0.0211) | Acc: (99.55%) (9047/9088)\n",
            "Epoch: 84 | Batch_idx: 80 |  Loss: (0.0210) | Acc: (99.56%) (10322/10368)\n",
            "Epoch: 84 | Batch_idx: 90 |  Loss: (0.0206) | Acc: (99.56%) (11597/11648)\n",
            "Epoch: 84 | Batch_idx: 100 |  Loss: (0.0207) | Acc: (99.57%) (12872/12928)\n",
            "Epoch: 84 | Batch_idx: 110 |  Loss: (0.0207) | Acc: (99.56%) (14146/14208)\n",
            "Epoch: 84 | Batch_idx: 120 |  Loss: (0.0205) | Acc: (99.58%) (15423/15488)\n",
            "Epoch: 84 | Batch_idx: 130 |  Loss: (0.0205) | Acc: (99.56%) (16695/16768)\n",
            "Epoch: 84 | Batch_idx: 140 |  Loss: (0.0202) | Acc: (99.56%) (17969/18048)\n",
            "Epoch: 84 | Batch_idx: 150 |  Loss: (0.0206) | Acc: (99.56%) (19242/19328)\n",
            "Epoch: 84 | Batch_idx: 160 |  Loss: (0.0205) | Acc: (99.57%) (20519/20608)\n",
            "Epoch: 84 | Batch_idx: 170 |  Loss: (0.0207) | Acc: (99.56%) (21791/21888)\n",
            "Epoch: 84 | Batch_idx: 180 |  Loss: (0.0207) | Acc: (99.56%) (23067/23168)\n",
            "Epoch: 84 | Batch_idx: 190 |  Loss: (0.0210) | Acc: (99.56%) (24341/24448)\n",
            "Epoch: 84 | Batch_idx: 200 |  Loss: (0.0207) | Acc: (99.56%) (25616/25728)\n",
            "Epoch: 84 | Batch_idx: 210 |  Loss: (0.0205) | Acc: (99.58%) (26894/27008)\n",
            "Epoch: 84 | Batch_idx: 220 |  Loss: (0.0203) | Acc: (99.58%) (28170/28288)\n",
            "Epoch: 84 | Batch_idx: 230 |  Loss: (0.0203) | Acc: (99.59%) (29447/29568)\n",
            "Epoch: 84 | Batch_idx: 240 |  Loss: (0.0201) | Acc: (99.60%) (30725/30848)\n",
            "Epoch: 84 | Batch_idx: 250 |  Loss: (0.0200) | Acc: (99.61%) (32002/32128)\n",
            "Epoch: 84 | Batch_idx: 260 |  Loss: (0.0201) | Acc: (99.60%) (33275/33408)\n",
            "Epoch: 84 | Batch_idx: 270 |  Loss: (0.0202) | Acc: (99.59%) (34547/34688)\n",
            "Epoch: 84 | Batch_idx: 280 |  Loss: (0.0201) | Acc: (99.59%) (35821/35968)\n",
            "Epoch: 84 | Batch_idx: 290 |  Loss: (0.0201) | Acc: (99.59%) (37094/37248)\n",
            "Epoch: 84 | Batch_idx: 300 |  Loss: (0.0201) | Acc: (99.59%) (38371/38528)\n",
            "Epoch: 84 | Batch_idx: 310 |  Loss: (0.0200) | Acc: (99.60%) (39649/39808)\n",
            "Epoch: 84 | Batch_idx: 320 |  Loss: (0.0200) | Acc: (99.59%) (40921/41088)\n",
            "Epoch: 84 | Batch_idx: 330 |  Loss: (0.0199) | Acc: (99.60%) (42197/42368)\n",
            "Epoch: 84 | Batch_idx: 340 |  Loss: (0.0199) | Acc: (99.60%) (43473/43648)\n",
            "Epoch: 84 | Batch_idx: 350 |  Loss: (0.0198) | Acc: (99.60%) (44748/44928)\n",
            "Epoch: 84 | Batch_idx: 360 |  Loss: (0.0198) | Acc: (99.60%) (46024/46208)\n",
            "Epoch: 84 | Batch_idx: 370 |  Loss: (0.0198) | Acc: (99.60%) (47300/47488)\n",
            "Epoch: 84 | Batch_idx: 380 |  Loss: (0.0197) | Acc: (99.61%) (48577/48768)\n",
            "Epoch: 84 | Batch_idx: 390 |  Loss: (0.0196) | Acc: (99.61%) (49805/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2631) | Acc: (92.59%) (9259/10000)\n",
            "Epoch: 85 | Batch_idx: 0 |  Loss: (0.0171) | Acc: (99.22%) (127/128)\n",
            "Epoch: 85 | Batch_idx: 10 |  Loss: (0.0170) | Acc: (99.79%) (1405/1408)\n",
            "Epoch: 85 | Batch_idx: 20 |  Loss: (0.0165) | Acc: (99.85%) (2684/2688)\n",
            "Epoch: 85 | Batch_idx: 30 |  Loss: (0.0170) | Acc: (99.85%) (3962/3968)\n",
            "Epoch: 85 | Batch_idx: 40 |  Loss: (0.0163) | Acc: (99.87%) (5241/5248)\n",
            "Epoch: 85 | Batch_idx: 50 |  Loss: (0.0168) | Acc: (99.85%) (6518/6528)\n",
            "Epoch: 85 | Batch_idx: 60 |  Loss: (0.0179) | Acc: (99.81%) (7793/7808)\n",
            "Epoch: 85 | Batch_idx: 70 |  Loss: (0.0175) | Acc: (99.81%) (9071/9088)\n",
            "Epoch: 85 | Batch_idx: 80 |  Loss: (0.0172) | Acc: (99.79%) (10346/10368)\n",
            "Epoch: 85 | Batch_idx: 90 |  Loss: (0.0171) | Acc: (99.78%) (11622/11648)\n",
            "Epoch: 85 | Batch_idx: 100 |  Loss: (0.0178) | Acc: (99.74%) (12895/12928)\n",
            "Epoch: 85 | Batch_idx: 110 |  Loss: (0.0176) | Acc: (99.75%) (14172/14208)\n",
            "Epoch: 85 | Batch_idx: 120 |  Loss: (0.0177) | Acc: (99.75%) (15449/15488)\n",
            "Epoch: 85 | Batch_idx: 130 |  Loss: (0.0177) | Acc: (99.74%) (16724/16768)\n",
            "Epoch: 85 | Batch_idx: 140 |  Loss: (0.0174) | Acc: (99.74%) (18001/18048)\n",
            "Epoch: 85 | Batch_idx: 150 |  Loss: (0.0175) | Acc: (99.73%) (19276/19328)\n",
            "Epoch: 85 | Batch_idx: 160 |  Loss: (0.0178) | Acc: (99.72%) (20551/20608)\n",
            "Epoch: 85 | Batch_idx: 170 |  Loss: (0.0178) | Acc: (99.73%) (21828/21888)\n",
            "Epoch: 85 | Batch_idx: 180 |  Loss: (0.0175) | Acc: (99.74%) (23107/23168)\n",
            "Epoch: 85 | Batch_idx: 190 |  Loss: (0.0177) | Acc: (99.73%) (24381/24448)\n",
            "Epoch: 85 | Batch_idx: 200 |  Loss: (0.0178) | Acc: (99.72%) (25655/25728)\n",
            "Epoch: 85 | Batch_idx: 210 |  Loss: (0.0177) | Acc: (99.72%) (26933/27008)\n",
            "Epoch: 85 | Batch_idx: 220 |  Loss: (0.0177) | Acc: (99.72%) (28209/28288)\n",
            "Epoch: 85 | Batch_idx: 230 |  Loss: (0.0175) | Acc: (99.73%) (29487/29568)\n",
            "Epoch: 85 | Batch_idx: 240 |  Loss: (0.0174) | Acc: (99.73%) (30764/30848)\n",
            "Epoch: 85 | Batch_idx: 250 |  Loss: (0.0173) | Acc: (99.74%) (32043/32128)\n",
            "Epoch: 85 | Batch_idx: 260 |  Loss: (0.0171) | Acc: (99.74%) (33322/33408)\n",
            "Epoch: 85 | Batch_idx: 270 |  Loss: (0.0170) | Acc: (99.75%) (34601/34688)\n",
            "Epoch: 85 | Batch_idx: 280 |  Loss: (0.0173) | Acc: (99.74%) (35874/35968)\n",
            "Epoch: 85 | Batch_idx: 290 |  Loss: (0.0172) | Acc: (99.74%) (37151/37248)\n",
            "Epoch: 85 | Batch_idx: 300 |  Loss: (0.0172) | Acc: (99.74%) (38428/38528)\n",
            "Epoch: 85 | Batch_idx: 310 |  Loss: (0.0172) | Acc: (99.74%) (39705/39808)\n",
            "Epoch: 85 | Batch_idx: 320 |  Loss: (0.0172) | Acc: (99.74%) (40980/41088)\n",
            "Epoch: 85 | Batch_idx: 330 |  Loss: (0.0172) | Acc: (99.74%) (42257/42368)\n",
            "Epoch: 85 | Batch_idx: 340 |  Loss: (0.0172) | Acc: (99.74%) (43534/43648)\n",
            "Epoch: 85 | Batch_idx: 350 |  Loss: (0.0172) | Acc: (99.74%) (44810/44928)\n",
            "Epoch: 85 | Batch_idx: 360 |  Loss: (0.0172) | Acc: (99.74%) (46086/46208)\n",
            "Epoch: 85 | Batch_idx: 370 |  Loss: (0.0172) | Acc: (99.73%) (47361/47488)\n",
            "Epoch: 85 | Batch_idx: 380 |  Loss: (0.0172) | Acc: (99.73%) (48637/48768)\n",
            "Epoch: 85 | Batch_idx: 390 |  Loss: (0.0172) | Acc: (99.73%) (49865/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2630) | Acc: (92.55%) (9255/10000)\n",
            "Epoch: 86 | Batch_idx: 0 |  Loss: (0.0198) | Acc: (99.22%) (127/128)\n",
            "Epoch: 86 | Batch_idx: 10 |  Loss: (0.0191) | Acc: (99.72%) (1404/1408)\n",
            "Epoch: 86 | Batch_idx: 20 |  Loss: (0.0180) | Acc: (99.74%) (2681/2688)\n",
            "Epoch: 86 | Batch_idx: 30 |  Loss: (0.0185) | Acc: (99.67%) (3955/3968)\n",
            "Epoch: 86 | Batch_idx: 40 |  Loss: (0.0183) | Acc: (99.64%) (5229/5248)\n",
            "Epoch: 86 | Batch_idx: 50 |  Loss: (0.0179) | Acc: (99.66%) (6506/6528)\n",
            "Epoch: 86 | Batch_idx: 60 |  Loss: (0.0169) | Acc: (99.71%) (7785/7808)\n",
            "Epoch: 86 | Batch_idx: 70 |  Loss: (0.0176) | Acc: (99.68%) (9059/9088)\n",
            "Epoch: 86 | Batch_idx: 80 |  Loss: (0.0171) | Acc: (99.71%) (10338/10368)\n",
            "Epoch: 86 | Batch_idx: 90 |  Loss: (0.0170) | Acc: (99.71%) (11614/11648)\n",
            "Epoch: 86 | Batch_idx: 100 |  Loss: (0.0169) | Acc: (99.68%) (12887/12928)\n",
            "Epoch: 86 | Batch_idx: 110 |  Loss: (0.0168) | Acc: (99.68%) (14163/14208)\n",
            "Epoch: 86 | Batch_idx: 120 |  Loss: (0.0168) | Acc: (99.69%) (15440/15488)\n",
            "Epoch: 86 | Batch_idx: 130 |  Loss: (0.0167) | Acc: (99.71%) (16719/16768)\n",
            "Epoch: 86 | Batch_idx: 140 |  Loss: (0.0167) | Acc: (99.71%) (17996/18048)\n",
            "Epoch: 86 | Batch_idx: 150 |  Loss: (0.0169) | Acc: (99.70%) (19270/19328)\n",
            "Epoch: 86 | Batch_idx: 160 |  Loss: (0.0169) | Acc: (99.70%) (20546/20608)\n",
            "Epoch: 86 | Batch_idx: 170 |  Loss: (0.0170) | Acc: (99.69%) (21821/21888)\n",
            "Epoch: 86 | Batch_idx: 180 |  Loss: (0.0172) | Acc: (99.68%) (23093/23168)\n",
            "Epoch: 86 | Batch_idx: 190 |  Loss: (0.0172) | Acc: (99.69%) (24371/24448)\n",
            "Epoch: 86 | Batch_idx: 200 |  Loss: (0.0169) | Acc: (99.70%) (25650/25728)\n",
            "Epoch: 86 | Batch_idx: 210 |  Loss: (0.0170) | Acc: (99.70%) (26926/27008)\n",
            "Epoch: 86 | Batch_idx: 220 |  Loss: (0.0173) | Acc: (99.68%) (28197/28288)\n",
            "Epoch: 86 | Batch_idx: 230 |  Loss: (0.0172) | Acc: (99.69%) (29476/29568)\n",
            "Epoch: 86 | Batch_idx: 240 |  Loss: (0.0172) | Acc: (99.70%) (30754/30848)\n",
            "Epoch: 86 | Batch_idx: 250 |  Loss: (0.0173) | Acc: (99.69%) (32027/32128)\n",
            "Epoch: 86 | Batch_idx: 260 |  Loss: (0.0171) | Acc: (99.69%) (33306/33408)\n",
            "Epoch: 86 | Batch_idx: 270 |  Loss: (0.0171) | Acc: (99.69%) (34582/34688)\n",
            "Epoch: 86 | Batch_idx: 280 |  Loss: (0.0170) | Acc: (99.69%) (35858/35968)\n",
            "Epoch: 86 | Batch_idx: 290 |  Loss: (0.0171) | Acc: (99.70%) (37135/37248)\n",
            "Epoch: 86 | Batch_idx: 300 |  Loss: (0.0171) | Acc: (99.70%) (38411/38528)\n",
            "Epoch: 86 | Batch_idx: 310 |  Loss: (0.0171) | Acc: (99.70%) (39687/39808)\n",
            "Epoch: 86 | Batch_idx: 320 |  Loss: (0.0170) | Acc: (99.70%) (40966/41088)\n",
            "Epoch: 86 | Batch_idx: 330 |  Loss: (0.0170) | Acc: (99.70%) (42242/42368)\n",
            "Epoch: 86 | Batch_idx: 340 |  Loss: (0.0172) | Acc: (99.70%) (43517/43648)\n",
            "Epoch: 86 | Batch_idx: 350 |  Loss: (0.0171) | Acc: (99.70%) (44794/44928)\n",
            "Epoch: 86 | Batch_idx: 360 |  Loss: (0.0171) | Acc: (99.71%) (46072/46208)\n",
            "Epoch: 86 | Batch_idx: 370 |  Loss: (0.0170) | Acc: (99.71%) (47349/47488)\n",
            "Epoch: 86 | Batch_idx: 380 |  Loss: (0.0168) | Acc: (99.71%) (48627/48768)\n",
            "Epoch: 86 | Batch_idx: 390 |  Loss: (0.0167) | Acc: (99.71%) (49856/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2626) | Acc: (92.64%) (9264/10000)\n",
            "Epoch: 87 | Batch_idx: 0 |  Loss: (0.0087) | Acc: (100.00%) (128/128)\n",
            "Epoch: 87 | Batch_idx: 10 |  Loss: (0.0142) | Acc: (99.79%) (1405/1408)\n",
            "Epoch: 87 | Batch_idx: 20 |  Loss: (0.0158) | Acc: (99.67%) (2679/2688)\n",
            "Epoch: 87 | Batch_idx: 30 |  Loss: (0.0152) | Acc: (99.72%) (3957/3968)\n",
            "Epoch: 87 | Batch_idx: 40 |  Loss: (0.0150) | Acc: (99.73%) (5234/5248)\n",
            "Epoch: 87 | Batch_idx: 50 |  Loss: (0.0156) | Acc: (99.74%) (6511/6528)\n",
            "Epoch: 87 | Batch_idx: 60 |  Loss: (0.0161) | Acc: (99.72%) (7786/7808)\n",
            "Epoch: 87 | Batch_idx: 70 |  Loss: (0.0160) | Acc: (99.71%) (9062/9088)\n",
            "Epoch: 87 | Batch_idx: 80 |  Loss: (0.0153) | Acc: (99.75%) (10342/10368)\n",
            "Epoch: 87 | Batch_idx: 90 |  Loss: (0.0153) | Acc: (99.75%) (11619/11648)\n",
            "Epoch: 87 | Batch_idx: 100 |  Loss: (0.0155) | Acc: (99.74%) (12895/12928)\n",
            "Epoch: 87 | Batch_idx: 110 |  Loss: (0.0155) | Acc: (99.74%) (14171/14208)\n",
            "Epoch: 87 | Batch_idx: 120 |  Loss: (0.0158) | Acc: (99.72%) (15445/15488)\n",
            "Epoch: 87 | Batch_idx: 130 |  Loss: (0.0158) | Acc: (99.73%) (16723/16768)\n",
            "Epoch: 87 | Batch_idx: 140 |  Loss: (0.0160) | Acc: (99.72%) (17998/18048)\n",
            "Epoch: 87 | Batch_idx: 150 |  Loss: (0.0160) | Acc: (99.73%) (19275/19328)\n",
            "Epoch: 87 | Batch_idx: 160 |  Loss: (0.0159) | Acc: (99.74%) (20555/20608)\n",
            "Epoch: 87 | Batch_idx: 170 |  Loss: (0.0158) | Acc: (99.74%) (21832/21888)\n",
            "Epoch: 87 | Batch_idx: 180 |  Loss: (0.0159) | Acc: (99.75%) (23109/23168)\n",
            "Epoch: 87 | Batch_idx: 190 |  Loss: (0.0159) | Acc: (99.74%) (24384/24448)\n",
            "Epoch: 87 | Batch_idx: 200 |  Loss: (0.0158) | Acc: (99.74%) (25661/25728)\n",
            "Epoch: 87 | Batch_idx: 210 |  Loss: (0.0156) | Acc: (99.74%) (26939/27008)\n",
            "Epoch: 87 | Batch_idx: 220 |  Loss: (0.0154) | Acc: (99.75%) (28218/28288)\n",
            "Epoch: 87 | Batch_idx: 230 |  Loss: (0.0154) | Acc: (99.76%) (29496/29568)\n",
            "Epoch: 87 | Batch_idx: 240 |  Loss: (0.0155) | Acc: (99.75%) (30771/30848)\n",
            "Epoch: 87 | Batch_idx: 250 |  Loss: (0.0155) | Acc: (99.75%) (32048/32128)\n",
            "Epoch: 87 | Batch_idx: 260 |  Loss: (0.0154) | Acc: (99.75%) (33326/33408)\n",
            "Epoch: 87 | Batch_idx: 270 |  Loss: (0.0153) | Acc: (99.76%) (34604/34688)\n",
            "Epoch: 87 | Batch_idx: 280 |  Loss: (0.0152) | Acc: (99.76%) (35883/35968)\n",
            "Epoch: 87 | Batch_idx: 290 |  Loss: (0.0153) | Acc: (99.76%) (37159/37248)\n",
            "Epoch: 87 | Batch_idx: 300 |  Loss: (0.0154) | Acc: (99.76%) (38436/38528)\n",
            "Epoch: 87 | Batch_idx: 310 |  Loss: (0.0154) | Acc: (99.76%) (39711/39808)\n",
            "Epoch: 87 | Batch_idx: 320 |  Loss: (0.0154) | Acc: (99.76%) (40988/41088)\n",
            "Epoch: 87 | Batch_idx: 330 |  Loss: (0.0154) | Acc: (99.76%) (42266/42368)\n",
            "Epoch: 87 | Batch_idx: 340 |  Loss: (0.0155) | Acc: (99.76%) (43542/43648)\n",
            "Epoch: 87 | Batch_idx: 350 |  Loss: (0.0155) | Acc: (99.76%) (44820/44928)\n",
            "Epoch: 87 | Batch_idx: 360 |  Loss: (0.0155) | Acc: (99.76%) (46097/46208)\n",
            "Epoch: 87 | Batch_idx: 370 |  Loss: (0.0156) | Acc: (99.76%) (47372/47488)\n",
            "Epoch: 87 | Batch_idx: 380 |  Loss: (0.0155) | Acc: (99.76%) (48650/48768)\n",
            "Epoch: 87 | Batch_idx: 390 |  Loss: (0.0154) | Acc: (99.76%) (49880/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2639) | Acc: (92.60%) (9260/10000)\n",
            "Epoch: 88 | Batch_idx: 0 |  Loss: (0.0216) | Acc: (99.22%) (127/128)\n",
            "Epoch: 88 | Batch_idx: 10 |  Loss: (0.0141) | Acc: (99.79%) (1405/1408)\n",
            "Epoch: 88 | Batch_idx: 20 |  Loss: (0.0146) | Acc: (99.78%) (2682/2688)\n",
            "Epoch: 88 | Batch_idx: 30 |  Loss: (0.0152) | Acc: (99.75%) (3958/3968)\n",
            "Epoch: 88 | Batch_idx: 40 |  Loss: (0.0156) | Acc: (99.73%) (5234/5248)\n",
            "Epoch: 88 | Batch_idx: 50 |  Loss: (0.0165) | Acc: (99.71%) (6509/6528)\n",
            "Epoch: 88 | Batch_idx: 60 |  Loss: (0.0157) | Acc: (99.74%) (7788/7808)\n",
            "Epoch: 88 | Batch_idx: 70 |  Loss: (0.0155) | Acc: (99.75%) (9065/9088)\n",
            "Epoch: 88 | Batch_idx: 80 |  Loss: (0.0155) | Acc: (99.76%) (10343/10368)\n",
            "Epoch: 88 | Batch_idx: 90 |  Loss: (0.0160) | Acc: (99.73%) (11617/11648)\n",
            "Epoch: 88 | Batch_idx: 100 |  Loss: (0.0160) | Acc: (99.73%) (12893/12928)\n",
            "Epoch: 88 | Batch_idx: 110 |  Loss: (0.0154) | Acc: (99.75%) (14172/14208)\n",
            "Epoch: 88 | Batch_idx: 120 |  Loss: (0.0155) | Acc: (99.73%) (15446/15488)\n",
            "Epoch: 88 | Batch_idx: 130 |  Loss: (0.0155) | Acc: (99.73%) (16723/16768)\n",
            "Epoch: 88 | Batch_idx: 140 |  Loss: (0.0156) | Acc: (99.71%) (17996/18048)\n",
            "Epoch: 88 | Batch_idx: 150 |  Loss: (0.0154) | Acc: (99.73%) (19275/19328)\n",
            "Epoch: 88 | Batch_idx: 160 |  Loss: (0.0155) | Acc: (99.71%) (20549/20608)\n",
            "Epoch: 88 | Batch_idx: 170 |  Loss: (0.0155) | Acc: (99.71%) (21825/21888)\n",
            "Epoch: 88 | Batch_idx: 180 |  Loss: (0.0155) | Acc: (99.72%) (23102/23168)\n",
            "Epoch: 88 | Batch_idx: 190 |  Loss: (0.0155) | Acc: (99.71%) (24378/24448)\n",
            "Epoch: 88 | Batch_idx: 200 |  Loss: (0.0155) | Acc: (99.72%) (25657/25728)\n",
            "Epoch: 88 | Batch_idx: 210 |  Loss: (0.0154) | Acc: (99.73%) (26936/27008)\n",
            "Epoch: 88 | Batch_idx: 220 |  Loss: (0.0154) | Acc: (99.74%) (28215/28288)\n",
            "Epoch: 88 | Batch_idx: 230 |  Loss: (0.0152) | Acc: (99.75%) (29494/29568)\n",
            "Epoch: 88 | Batch_idx: 240 |  Loss: (0.0152) | Acc: (99.75%) (30771/30848)\n",
            "Epoch: 88 | Batch_idx: 250 |  Loss: (0.0151) | Acc: (99.75%) (32047/32128)\n",
            "Epoch: 88 | Batch_idx: 260 |  Loss: (0.0149) | Acc: (99.76%) (33327/33408)\n",
            "Epoch: 88 | Batch_idx: 270 |  Loss: (0.0151) | Acc: (99.74%) (34599/34688)\n",
            "Epoch: 88 | Batch_idx: 280 |  Loss: (0.0151) | Acc: (99.74%) (35874/35968)\n",
            "Epoch: 88 | Batch_idx: 290 |  Loss: (0.0151) | Acc: (99.73%) (37149/37248)\n",
            "Epoch: 88 | Batch_idx: 300 |  Loss: (0.0151) | Acc: (99.73%) (38425/38528)\n",
            "Epoch: 88 | Batch_idx: 310 |  Loss: (0.0150) | Acc: (99.74%) (39703/39808)\n",
            "Epoch: 88 | Batch_idx: 320 |  Loss: (0.0151) | Acc: (99.73%) (40979/41088)\n",
            "Epoch: 88 | Batch_idx: 330 |  Loss: (0.0150) | Acc: (99.73%) (42255/42368)\n",
            "Epoch: 88 | Batch_idx: 340 |  Loss: (0.0149) | Acc: (99.74%) (43535/43648)\n",
            "Epoch: 88 | Batch_idx: 350 |  Loss: (0.0150) | Acc: (99.74%) (44813/44928)\n",
            "Epoch: 88 | Batch_idx: 360 |  Loss: (0.0150) | Acc: (99.74%) (46089/46208)\n",
            "Epoch: 88 | Batch_idx: 370 |  Loss: (0.0150) | Acc: (99.74%) (47366/47488)\n",
            "Epoch: 88 | Batch_idx: 380 |  Loss: (0.0151) | Acc: (99.74%) (48643/48768)\n",
            "Epoch: 88 | Batch_idx: 390 |  Loss: (0.0150) | Acc: (99.75%) (49873/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2632) | Acc: (92.76%) (9276/10000)\n",
            "Epoch: 89 | Batch_idx: 0 |  Loss: (0.0132) | Acc: (100.00%) (128/128)\n",
            "Epoch: 89 | Batch_idx: 10 |  Loss: (0.0146) | Acc: (99.79%) (1405/1408)\n",
            "Epoch: 89 | Batch_idx: 20 |  Loss: (0.0143) | Acc: (99.78%) (2682/2688)\n",
            "Epoch: 89 | Batch_idx: 30 |  Loss: (0.0131) | Acc: (99.80%) (3960/3968)\n",
            "Epoch: 89 | Batch_idx: 40 |  Loss: (0.0134) | Acc: (99.81%) (5238/5248)\n",
            "Epoch: 89 | Batch_idx: 50 |  Loss: (0.0139) | Acc: (99.80%) (6515/6528)\n",
            "Epoch: 89 | Batch_idx: 60 |  Loss: (0.0140) | Acc: (99.77%) (7790/7808)\n",
            "Epoch: 89 | Batch_idx: 70 |  Loss: (0.0143) | Acc: (99.76%) (9066/9088)\n",
            "Epoch: 89 | Batch_idx: 80 |  Loss: (0.0144) | Acc: (99.76%) (10343/10368)\n",
            "Epoch: 89 | Batch_idx: 90 |  Loss: (0.0144) | Acc: (99.78%) (11622/11648)\n",
            "Epoch: 89 | Batch_idx: 100 |  Loss: (0.0145) | Acc: (99.78%) (12900/12928)\n",
            "Epoch: 89 | Batch_idx: 110 |  Loss: (0.0147) | Acc: (99.77%) (14176/14208)\n",
            "Epoch: 89 | Batch_idx: 120 |  Loss: (0.0147) | Acc: (99.77%) (15452/15488)\n",
            "Epoch: 89 | Batch_idx: 130 |  Loss: (0.0146) | Acc: (99.77%) (16730/16768)\n",
            "Epoch: 89 | Batch_idx: 140 |  Loss: (0.0146) | Acc: (99.78%) (18008/18048)\n",
            "Epoch: 89 | Batch_idx: 150 |  Loss: (0.0146) | Acc: (99.78%) (19285/19328)\n",
            "Epoch: 89 | Batch_idx: 160 |  Loss: (0.0146) | Acc: (99.77%) (20560/20608)\n",
            "Epoch: 89 | Batch_idx: 170 |  Loss: (0.0145) | Acc: (99.77%) (21838/21888)\n",
            "Epoch: 89 | Batch_idx: 180 |  Loss: (0.0144) | Acc: (99.78%) (23117/23168)\n",
            "Epoch: 89 | Batch_idx: 190 |  Loss: (0.0143) | Acc: (99.79%) (24396/24448)\n",
            "Epoch: 89 | Batch_idx: 200 |  Loss: (0.0144) | Acc: (99.78%) (25671/25728)\n",
            "Epoch: 89 | Batch_idx: 210 |  Loss: (0.0145) | Acc: (99.77%) (26947/27008)\n",
            "Epoch: 89 | Batch_idx: 220 |  Loss: (0.0145) | Acc: (99.77%) (28224/28288)\n",
            "Epoch: 89 | Batch_idx: 230 |  Loss: (0.0145) | Acc: (99.77%) (29499/29568)\n",
            "Epoch: 89 | Batch_idx: 240 |  Loss: (0.0145) | Acc: (99.77%) (30777/30848)\n",
            "Epoch: 89 | Batch_idx: 250 |  Loss: (0.0145) | Acc: (99.77%) (32053/32128)\n",
            "Epoch: 89 | Batch_idx: 260 |  Loss: (0.0145) | Acc: (99.77%) (33331/33408)\n",
            "Epoch: 89 | Batch_idx: 270 |  Loss: (0.0144) | Acc: (99.77%) (34609/34688)\n",
            "Epoch: 89 | Batch_idx: 280 |  Loss: (0.0144) | Acc: (99.78%) (35888/35968)\n",
            "Epoch: 89 | Batch_idx: 290 |  Loss: (0.0144) | Acc: (99.78%) (37167/37248)\n",
            "Epoch: 89 | Batch_idx: 300 |  Loss: (0.0143) | Acc: (99.79%) (38446/38528)\n",
            "Epoch: 89 | Batch_idx: 310 |  Loss: (0.0143) | Acc: (99.79%) (39725/39808)\n",
            "Epoch: 89 | Batch_idx: 320 |  Loss: (0.0143) | Acc: (99.79%) (41003/41088)\n",
            "Epoch: 89 | Batch_idx: 330 |  Loss: (0.0142) | Acc: (99.79%) (42280/42368)\n",
            "Epoch: 89 | Batch_idx: 340 |  Loss: (0.0142) | Acc: (99.79%) (43556/43648)\n",
            "Epoch: 89 | Batch_idx: 350 |  Loss: (0.0143) | Acc: (99.79%) (44835/44928)\n",
            "Epoch: 89 | Batch_idx: 360 |  Loss: (0.0142) | Acc: (99.80%) (46114/46208)\n",
            "Epoch: 89 | Batch_idx: 370 |  Loss: (0.0143) | Acc: (99.79%) (47388/47488)\n",
            "Epoch: 89 | Batch_idx: 380 |  Loss: (0.0144) | Acc: (99.79%) (48666/48768)\n",
            "Epoch: 89 | Batch_idx: 390 |  Loss: (0.0143) | Acc: (99.79%) (49897/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2626) | Acc: (92.78%) (9278/10000)\n",
            "Epoch: 90 | Batch_idx: 0 |  Loss: (0.0217) | Acc: (99.22%) (127/128)\n",
            "Epoch: 90 | Batch_idx: 10 |  Loss: (0.0132) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 90 | Batch_idx: 20 |  Loss: (0.0139) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 90 | Batch_idx: 30 |  Loss: (0.0132) | Acc: (99.90%) (3964/3968)\n",
            "Epoch: 90 | Batch_idx: 40 |  Loss: (0.0143) | Acc: (99.83%) (5239/5248)\n",
            "Epoch: 90 | Batch_idx: 50 |  Loss: (0.0137) | Acc: (99.83%) (6517/6528)\n",
            "Epoch: 90 | Batch_idx: 60 |  Loss: (0.0141) | Acc: (99.80%) (7792/7808)\n",
            "Epoch: 90 | Batch_idx: 70 |  Loss: (0.0141) | Acc: (99.78%) (9068/9088)\n",
            "Epoch: 90 | Batch_idx: 80 |  Loss: (0.0139) | Acc: (99.80%) (10347/10368)\n",
            "Epoch: 90 | Batch_idx: 90 |  Loss: (0.0137) | Acc: (99.80%) (11625/11648)\n",
            "Epoch: 90 | Batch_idx: 100 |  Loss: (0.0136) | Acc: (99.81%) (12903/12928)\n",
            "Epoch: 90 | Batch_idx: 110 |  Loss: (0.0137) | Acc: (99.78%) (14177/14208)\n",
            "Epoch: 90 | Batch_idx: 120 |  Loss: (0.0136) | Acc: (99.79%) (15456/15488)\n",
            "Epoch: 90 | Batch_idx: 130 |  Loss: (0.0135) | Acc: (99.81%) (16736/16768)\n",
            "Epoch: 90 | Batch_idx: 140 |  Loss: (0.0134) | Acc: (99.81%) (18014/18048)\n",
            "Epoch: 90 | Batch_idx: 150 |  Loss: (0.0138) | Acc: (99.78%) (19286/19328)\n",
            "Epoch: 90 | Batch_idx: 160 |  Loss: (0.0135) | Acc: (99.79%) (20565/20608)\n",
            "Epoch: 90 | Batch_idx: 170 |  Loss: (0.0133) | Acc: (99.80%) (21844/21888)\n",
            "Epoch: 90 | Batch_idx: 180 |  Loss: (0.0132) | Acc: (99.80%) (23122/23168)\n",
            "Epoch: 90 | Batch_idx: 190 |  Loss: (0.0132) | Acc: (99.79%) (24397/24448)\n",
            "Epoch: 90 | Batch_idx: 200 |  Loss: (0.0133) | Acc: (99.80%) (25676/25728)\n",
            "Epoch: 90 | Batch_idx: 210 |  Loss: (0.0134) | Acc: (99.80%) (26954/27008)\n",
            "Epoch: 90 | Batch_idx: 220 |  Loss: (0.0133) | Acc: (99.80%) (28232/28288)\n",
            "Epoch: 90 | Batch_idx: 230 |  Loss: (0.0133) | Acc: (99.80%) (29509/29568)\n",
            "Epoch: 90 | Batch_idx: 240 |  Loss: (0.0133) | Acc: (99.81%) (30788/30848)\n",
            "Epoch: 90 | Batch_idx: 250 |  Loss: (0.0133) | Acc: (99.81%) (32066/32128)\n",
            "Epoch: 90 | Batch_idx: 260 |  Loss: (0.0132) | Acc: (99.81%) (33343/33408)\n",
            "Epoch: 90 | Batch_idx: 270 |  Loss: (0.0133) | Acc: (99.80%) (34619/34688)\n",
            "Epoch: 90 | Batch_idx: 280 |  Loss: (0.0135) | Acc: (99.79%) (35892/35968)\n",
            "Epoch: 90 | Batch_idx: 290 |  Loss: (0.0135) | Acc: (99.79%) (37171/37248)\n",
            "Epoch: 90 | Batch_idx: 300 |  Loss: (0.0133) | Acc: (99.80%) (38451/38528)\n",
            "Epoch: 90 | Batch_idx: 310 |  Loss: (0.0132) | Acc: (99.81%) (39731/39808)\n",
            "Epoch: 90 | Batch_idx: 320 |  Loss: (0.0131) | Acc: (99.81%) (41010/41088)\n",
            "Epoch: 90 | Batch_idx: 330 |  Loss: (0.0132) | Acc: (99.81%) (42286/42368)\n",
            "Epoch: 90 | Batch_idx: 340 |  Loss: (0.0132) | Acc: (99.81%) (43566/43648)\n",
            "Epoch: 90 | Batch_idx: 350 |  Loss: (0.0132) | Acc: (99.81%) (44844/44928)\n",
            "Epoch: 90 | Batch_idx: 360 |  Loss: (0.0132) | Acc: (99.82%) (46124/46208)\n",
            "Epoch: 90 | Batch_idx: 370 |  Loss: (0.0132) | Acc: (99.82%) (47401/47488)\n",
            "Epoch: 90 | Batch_idx: 380 |  Loss: (0.0132) | Acc: (99.82%) (48680/48768)\n",
            "Epoch: 90 | Batch_idx: 390 |  Loss: (0.0132) | Acc: (99.82%) (49909/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2615) | Acc: (92.84%) (9284/10000)\n",
            "Epoch: 91 | Batch_idx: 0 |  Loss: (0.0254) | Acc: (99.22%) (127/128)\n",
            "Epoch: 91 | Batch_idx: 10 |  Loss: (0.0133) | Acc: (99.79%) (1405/1408)\n",
            "Epoch: 91 | Batch_idx: 20 |  Loss: (0.0133) | Acc: (99.81%) (2683/2688)\n",
            "Epoch: 91 | Batch_idx: 30 |  Loss: (0.0133) | Acc: (99.85%) (3962/3968)\n",
            "Epoch: 91 | Batch_idx: 40 |  Loss: (0.0128) | Acc: (99.89%) (5242/5248)\n",
            "Epoch: 91 | Batch_idx: 50 |  Loss: (0.0132) | Acc: (99.85%) (6518/6528)\n",
            "Epoch: 91 | Batch_idx: 60 |  Loss: (0.0134) | Acc: (99.83%) (7795/7808)\n",
            "Epoch: 91 | Batch_idx: 70 |  Loss: (0.0131) | Acc: (99.86%) (9075/9088)\n",
            "Epoch: 91 | Batch_idx: 80 |  Loss: (0.0130) | Acc: (99.86%) (10353/10368)\n",
            "Epoch: 91 | Batch_idx: 90 |  Loss: (0.0133) | Acc: (99.84%) (11629/11648)\n",
            "Epoch: 91 | Batch_idx: 100 |  Loss: (0.0130) | Acc: (99.85%) (12909/12928)\n",
            "Epoch: 91 | Batch_idx: 110 |  Loss: (0.0130) | Acc: (99.86%) (14188/14208)\n",
            "Epoch: 91 | Batch_idx: 120 |  Loss: (0.0131) | Acc: (99.85%) (15464/15488)\n",
            "Epoch: 91 | Batch_idx: 130 |  Loss: (0.0129) | Acc: (99.86%) (16744/16768)\n",
            "Epoch: 91 | Batch_idx: 140 |  Loss: (0.0128) | Acc: (99.87%) (18024/18048)\n",
            "Epoch: 91 | Batch_idx: 150 |  Loss: (0.0129) | Acc: (99.86%) (19301/19328)\n",
            "Epoch: 91 | Batch_idx: 160 |  Loss: (0.0127) | Acc: (99.86%) (20580/20608)\n",
            "Epoch: 91 | Batch_idx: 170 |  Loss: (0.0126) | Acc: (99.86%) (21858/21888)\n",
            "Epoch: 91 | Batch_idx: 180 |  Loss: (0.0128) | Acc: (99.85%) (23133/23168)\n",
            "Epoch: 91 | Batch_idx: 190 |  Loss: (0.0127) | Acc: (99.85%) (24412/24448)\n",
            "Epoch: 91 | Batch_idx: 200 |  Loss: (0.0128) | Acc: (99.85%) (25689/25728)\n",
            "Epoch: 91 | Batch_idx: 210 |  Loss: (0.0128) | Acc: (99.85%) (26967/27008)\n",
            "Epoch: 91 | Batch_idx: 220 |  Loss: (0.0129) | Acc: (99.84%) (28243/28288)\n",
            "Epoch: 91 | Batch_idx: 230 |  Loss: (0.0129) | Acc: (99.83%) (29519/29568)\n",
            "Epoch: 91 | Batch_idx: 240 |  Loss: (0.0129) | Acc: (99.83%) (30795/30848)\n",
            "Epoch: 91 | Batch_idx: 250 |  Loss: (0.0130) | Acc: (99.83%) (32073/32128)\n",
            "Epoch: 91 | Batch_idx: 260 |  Loss: (0.0129) | Acc: (99.83%) (33352/33408)\n",
            "Epoch: 91 | Batch_idx: 270 |  Loss: (0.0129) | Acc: (99.83%) (34630/34688)\n",
            "Epoch: 91 | Batch_idx: 280 |  Loss: (0.0129) | Acc: (99.84%) (35909/35968)\n",
            "Epoch: 91 | Batch_idx: 290 |  Loss: (0.0129) | Acc: (99.83%) (37186/37248)\n",
            "Epoch: 91 | Batch_idx: 300 |  Loss: (0.0129) | Acc: (99.83%) (38462/38528)\n",
            "Epoch: 91 | Batch_idx: 310 |  Loss: (0.0129) | Acc: (99.82%) (39738/39808)\n",
            "Epoch: 91 | Batch_idx: 320 |  Loss: (0.0130) | Acc: (99.82%) (41014/41088)\n",
            "Epoch: 91 | Batch_idx: 330 |  Loss: (0.0130) | Acc: (99.82%) (42293/42368)\n",
            "Epoch: 91 | Batch_idx: 340 |  Loss: (0.0130) | Acc: (99.83%) (43572/43648)\n",
            "Epoch: 91 | Batch_idx: 350 |  Loss: (0.0130) | Acc: (99.82%) (44849/44928)\n",
            "Epoch: 91 | Batch_idx: 360 |  Loss: (0.0131) | Acc: (99.82%) (46124/46208)\n",
            "Epoch: 91 | Batch_idx: 370 |  Loss: (0.0130) | Acc: (99.82%) (47404/47488)\n",
            "Epoch: 91 | Batch_idx: 380 |  Loss: (0.0131) | Acc: (99.82%) (48679/48768)\n",
            "Epoch: 91 | Batch_idx: 390 |  Loss: (0.0131) | Acc: (99.82%) (49911/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2610) | Acc: (92.90%) (9290/10000)\n",
            "Epoch: 92 | Batch_idx: 0 |  Loss: (0.0157) | Acc: (100.00%) (128/128)\n",
            "Epoch: 92 | Batch_idx: 10 |  Loss: (0.0137) | Acc: (99.72%) (1404/1408)\n",
            "Epoch: 92 | Batch_idx: 20 |  Loss: (0.0128) | Acc: (99.81%) (2683/2688)\n",
            "Epoch: 92 | Batch_idx: 30 |  Loss: (0.0127) | Acc: (99.85%) (3962/3968)\n",
            "Epoch: 92 | Batch_idx: 40 |  Loss: (0.0131) | Acc: (99.81%) (5238/5248)\n",
            "Epoch: 92 | Batch_idx: 50 |  Loss: (0.0128) | Acc: (99.82%) (6516/6528)\n",
            "Epoch: 92 | Batch_idx: 60 |  Loss: (0.0126) | Acc: (99.83%) (7795/7808)\n",
            "Epoch: 92 | Batch_idx: 70 |  Loss: (0.0126) | Acc: (99.82%) (9072/9088)\n",
            "Epoch: 92 | Batch_idx: 80 |  Loss: (0.0124) | Acc: (99.83%) (10350/10368)\n",
            "Epoch: 92 | Batch_idx: 90 |  Loss: (0.0123) | Acc: (99.85%) (11630/11648)\n",
            "Epoch: 92 | Batch_idx: 100 |  Loss: (0.0123) | Acc: (99.85%) (12908/12928)\n",
            "Epoch: 92 | Batch_idx: 110 |  Loss: (0.0123) | Acc: (99.85%) (14187/14208)\n",
            "Epoch: 92 | Batch_idx: 120 |  Loss: (0.0124) | Acc: (99.85%) (15464/15488)\n",
            "Epoch: 92 | Batch_idx: 130 |  Loss: (0.0124) | Acc: (99.84%) (16742/16768)\n",
            "Epoch: 92 | Batch_idx: 140 |  Loss: (0.0127) | Acc: (99.83%) (18018/18048)\n",
            "Epoch: 92 | Batch_idx: 150 |  Loss: (0.0128) | Acc: (99.83%) (19295/19328)\n",
            "Epoch: 92 | Batch_idx: 160 |  Loss: (0.0128) | Acc: (99.83%) (20573/20608)\n",
            "Epoch: 92 | Batch_idx: 170 |  Loss: (0.0127) | Acc: (99.83%) (21850/21888)\n",
            "Epoch: 92 | Batch_idx: 180 |  Loss: (0.0127) | Acc: (99.84%) (23130/23168)\n",
            "Epoch: 92 | Batch_idx: 190 |  Loss: (0.0126) | Acc: (99.84%) (24410/24448)\n",
            "Epoch: 92 | Batch_idx: 200 |  Loss: (0.0127) | Acc: (99.84%) (25688/25728)\n",
            "Epoch: 92 | Batch_idx: 210 |  Loss: (0.0127) | Acc: (99.84%) (26965/27008)\n",
            "Epoch: 92 | Batch_idx: 220 |  Loss: (0.0129) | Acc: (99.83%) (28241/28288)\n",
            "Epoch: 92 | Batch_idx: 230 |  Loss: (0.0129) | Acc: (99.84%) (29520/29568)\n",
            "Epoch: 92 | Batch_idx: 240 |  Loss: (0.0130) | Acc: (99.83%) (30797/30848)\n",
            "Epoch: 92 | Batch_idx: 250 |  Loss: (0.0130) | Acc: (99.84%) (32076/32128)\n",
            "Epoch: 92 | Batch_idx: 260 |  Loss: (0.0131) | Acc: (99.84%) (33354/33408)\n",
            "Epoch: 92 | Batch_idx: 270 |  Loss: (0.0131) | Acc: (99.84%) (34631/34688)\n",
            "Epoch: 92 | Batch_idx: 280 |  Loss: (0.0131) | Acc: (99.83%) (35908/35968)\n",
            "Epoch: 92 | Batch_idx: 290 |  Loss: (0.0132) | Acc: (99.83%) (37185/37248)\n",
            "Epoch: 92 | Batch_idx: 300 |  Loss: (0.0132) | Acc: (99.83%) (38462/38528)\n",
            "Epoch: 92 | Batch_idx: 310 |  Loss: (0.0131) | Acc: (99.83%) (39741/39808)\n",
            "Epoch: 92 | Batch_idx: 320 |  Loss: (0.0130) | Acc: (99.83%) (41020/41088)\n",
            "Epoch: 92 | Batch_idx: 330 |  Loss: (0.0132) | Acc: (99.82%) (42292/42368)\n",
            "Epoch: 92 | Batch_idx: 340 |  Loss: (0.0131) | Acc: (99.82%) (43571/43648)\n",
            "Epoch: 92 | Batch_idx: 350 |  Loss: (0.0130) | Acc: (99.83%) (44851/44928)\n",
            "Epoch: 92 | Batch_idx: 360 |  Loss: (0.0130) | Acc: (99.83%) (46129/46208)\n",
            "Epoch: 92 | Batch_idx: 370 |  Loss: (0.0130) | Acc: (99.83%) (47406/47488)\n",
            "Epoch: 92 | Batch_idx: 380 |  Loss: (0.0129) | Acc: (99.83%) (48684/48768)\n",
            "Epoch: 92 | Batch_idx: 390 |  Loss: (0.0129) | Acc: (99.83%) (49914/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2610) | Acc: (92.78%) (9278/10000)\n",
            "Epoch: 93 | Batch_idx: 0 |  Loss: (0.0131) | Acc: (100.00%) (128/128)\n",
            "Epoch: 93 | Batch_idx: 10 |  Loss: (0.0113) | Acc: (99.86%) (1406/1408)\n",
            "Epoch: 93 | Batch_idx: 20 |  Loss: (0.0113) | Acc: (99.85%) (2684/2688)\n",
            "Epoch: 93 | Batch_idx: 30 |  Loss: (0.0111) | Acc: (99.85%) (3962/3968)\n",
            "Epoch: 93 | Batch_idx: 40 |  Loss: (0.0108) | Acc: (99.87%) (5241/5248)\n",
            "Epoch: 93 | Batch_idx: 50 |  Loss: (0.0109) | Acc: (99.85%) (6518/6528)\n",
            "Epoch: 93 | Batch_idx: 60 |  Loss: (0.0110) | Acc: (99.85%) (7796/7808)\n",
            "Epoch: 93 | Batch_idx: 70 |  Loss: (0.0107) | Acc: (99.85%) (9074/9088)\n",
            "Epoch: 93 | Batch_idx: 80 |  Loss: (0.0107) | Acc: (99.86%) (10353/10368)\n",
            "Epoch: 93 | Batch_idx: 90 |  Loss: (0.0106) | Acc: (99.86%) (11632/11648)\n",
            "Epoch: 93 | Batch_idx: 100 |  Loss: (0.0110) | Acc: (99.83%) (12906/12928)\n",
            "Epoch: 93 | Batch_idx: 110 |  Loss: (0.0113) | Acc: (99.83%) (14184/14208)\n",
            "Epoch: 93 | Batch_idx: 120 |  Loss: (0.0116) | Acc: (99.83%) (15461/15488)\n",
            "Epoch: 93 | Batch_idx: 130 |  Loss: (0.0115) | Acc: (99.83%) (16740/16768)\n",
            "Epoch: 93 | Batch_idx: 140 |  Loss: (0.0116) | Acc: (99.83%) (18018/18048)\n",
            "Epoch: 93 | Batch_idx: 150 |  Loss: (0.0117) | Acc: (99.83%) (19296/19328)\n",
            "Epoch: 93 | Batch_idx: 160 |  Loss: (0.0118) | Acc: (99.84%) (20574/20608)\n",
            "Epoch: 93 | Batch_idx: 170 |  Loss: (0.0116) | Acc: (99.84%) (21854/21888)\n",
            "Epoch: 93 | Batch_idx: 180 |  Loss: (0.0117) | Acc: (99.84%) (23131/23168)\n",
            "Epoch: 93 | Batch_idx: 190 |  Loss: (0.0119) | Acc: (99.83%) (24407/24448)\n",
            "Epoch: 93 | Batch_idx: 200 |  Loss: (0.0118) | Acc: (99.84%) (25686/25728)\n",
            "Epoch: 93 | Batch_idx: 210 |  Loss: (0.0119) | Acc: (99.83%) (26963/27008)\n",
            "Epoch: 93 | Batch_idx: 220 |  Loss: (0.0120) | Acc: (99.83%) (28241/28288)\n",
            "Epoch: 93 | Batch_idx: 230 |  Loss: (0.0120) | Acc: (99.83%) (29519/29568)\n",
            "Epoch: 93 | Batch_idx: 240 |  Loss: (0.0119) | Acc: (99.84%) (30798/30848)\n",
            "Epoch: 93 | Batch_idx: 250 |  Loss: (0.0120) | Acc: (99.83%) (32074/32128)\n",
            "Epoch: 93 | Batch_idx: 260 |  Loss: (0.0120) | Acc: (99.83%) (33350/33408)\n",
            "Epoch: 93 | Batch_idx: 270 |  Loss: (0.0120) | Acc: (99.83%) (34629/34688)\n",
            "Epoch: 93 | Batch_idx: 280 |  Loss: (0.0121) | Acc: (99.83%) (35906/35968)\n",
            "Epoch: 93 | Batch_idx: 290 |  Loss: (0.0121) | Acc: (99.82%) (37182/37248)\n",
            "Epoch: 93 | Batch_idx: 300 |  Loss: (0.0121) | Acc: (99.83%) (38461/38528)\n",
            "Epoch: 93 | Batch_idx: 310 |  Loss: (0.0120) | Acc: (99.83%) (39740/39808)\n",
            "Epoch: 93 | Batch_idx: 320 |  Loss: (0.0120) | Acc: (99.83%) (41019/41088)\n",
            "Epoch: 93 | Batch_idx: 330 |  Loss: (0.0120) | Acc: (99.83%) (42297/42368)\n",
            "Epoch: 93 | Batch_idx: 340 |  Loss: (0.0120) | Acc: (99.83%) (43574/43648)\n",
            "Epoch: 93 | Batch_idx: 350 |  Loss: (0.0121) | Acc: (99.82%) (44848/44928)\n",
            "Epoch: 93 | Batch_idx: 360 |  Loss: (0.0121) | Acc: (99.82%) (46126/46208)\n",
            "Epoch: 93 | Batch_idx: 370 |  Loss: (0.0121) | Acc: (99.82%) (47404/47488)\n",
            "Epoch: 93 | Batch_idx: 380 |  Loss: (0.0121) | Acc: (99.83%) (48683/48768)\n",
            "Epoch: 93 | Batch_idx: 390 |  Loss: (0.0122) | Acc: (99.83%) (49914/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2606) | Acc: (92.77%) (9277/10000)\n",
            "Epoch: 94 | Batch_idx: 0 |  Loss: (0.0159) | Acc: (99.22%) (127/128)\n",
            "Epoch: 94 | Batch_idx: 10 |  Loss: (0.0120) | Acc: (99.79%) (1405/1408)\n",
            "Epoch: 94 | Batch_idx: 20 |  Loss: (0.0128) | Acc: (99.81%) (2683/2688)\n",
            "Epoch: 94 | Batch_idx: 30 |  Loss: (0.0126) | Acc: (99.82%) (3961/3968)\n",
            "Epoch: 94 | Batch_idx: 40 |  Loss: (0.0127) | Acc: (99.83%) (5239/5248)\n",
            "Epoch: 94 | Batch_idx: 50 |  Loss: (0.0121) | Acc: (99.85%) (6518/6528)\n",
            "Epoch: 94 | Batch_idx: 60 |  Loss: (0.0122) | Acc: (99.85%) (7796/7808)\n",
            "Epoch: 94 | Batch_idx: 70 |  Loss: (0.0122) | Acc: (99.83%) (9073/9088)\n",
            "Epoch: 94 | Batch_idx: 80 |  Loss: (0.0125) | Acc: (99.83%) (10350/10368)\n",
            "Epoch: 94 | Batch_idx: 90 |  Loss: (0.0124) | Acc: (99.82%) (11627/11648)\n",
            "Epoch: 94 | Batch_idx: 100 |  Loss: (0.0127) | Acc: (99.81%) (12903/12928)\n",
            "Epoch: 94 | Batch_idx: 110 |  Loss: (0.0127) | Acc: (99.80%) (14180/14208)\n",
            "Epoch: 94 | Batch_idx: 120 |  Loss: (0.0125) | Acc: (99.82%) (15460/15488)\n",
            "Epoch: 94 | Batch_idx: 130 |  Loss: (0.0124) | Acc: (99.82%) (16737/16768)\n",
            "Epoch: 94 | Batch_idx: 140 |  Loss: (0.0124) | Acc: (99.82%) (18015/18048)\n",
            "Epoch: 94 | Batch_idx: 150 |  Loss: (0.0124) | Acc: (99.81%) (19292/19328)\n",
            "Epoch: 94 | Batch_idx: 160 |  Loss: (0.0122) | Acc: (99.83%) (20572/20608)\n",
            "Epoch: 94 | Batch_idx: 170 |  Loss: (0.0121) | Acc: (99.84%) (21852/21888)\n",
            "Epoch: 94 | Batch_idx: 180 |  Loss: (0.0120) | Acc: (99.84%) (23131/23168)\n",
            "Epoch: 94 | Batch_idx: 190 |  Loss: (0.0120) | Acc: (99.84%) (24409/24448)\n",
            "Epoch: 94 | Batch_idx: 200 |  Loss: (0.0121) | Acc: (99.83%) (25685/25728)\n",
            "Epoch: 94 | Batch_idx: 210 |  Loss: (0.0119) | Acc: (99.84%) (26965/27008)\n",
            "Epoch: 94 | Batch_idx: 220 |  Loss: (0.0119) | Acc: (99.83%) (28241/28288)\n",
            "Epoch: 94 | Batch_idx: 230 |  Loss: (0.0120) | Acc: (99.83%) (29519/29568)\n",
            "Epoch: 94 | Batch_idx: 240 |  Loss: (0.0120) | Acc: (99.84%) (30798/30848)\n",
            "Epoch: 94 | Batch_idx: 250 |  Loss: (0.0119) | Acc: (99.84%) (32076/32128)\n",
            "Epoch: 94 | Batch_idx: 260 |  Loss: (0.0118) | Acc: (99.84%) (33356/33408)\n",
            "Epoch: 94 | Batch_idx: 270 |  Loss: (0.0119) | Acc: (99.84%) (34634/34688)\n",
            "Epoch: 94 | Batch_idx: 280 |  Loss: (0.0119) | Acc: (99.84%) (35912/35968)\n",
            "Epoch: 94 | Batch_idx: 290 |  Loss: (0.0118) | Acc: (99.85%) (37192/37248)\n",
            "Epoch: 94 | Batch_idx: 300 |  Loss: (0.0119) | Acc: (99.84%) (38468/38528)\n",
            "Epoch: 94 | Batch_idx: 310 |  Loss: (0.0119) | Acc: (99.84%) (39746/39808)\n",
            "Epoch: 94 | Batch_idx: 320 |  Loss: (0.0119) | Acc: (99.85%) (41025/41088)\n",
            "Epoch: 94 | Batch_idx: 330 |  Loss: (0.0118) | Acc: (99.85%) (42303/42368)\n",
            "Epoch: 94 | Batch_idx: 340 |  Loss: (0.0119) | Acc: (99.84%) (43579/43648)\n",
            "Epoch: 94 | Batch_idx: 350 |  Loss: (0.0120) | Acc: (99.84%) (44857/44928)\n",
            "Epoch: 94 | Batch_idx: 360 |  Loss: (0.0121) | Acc: (99.84%) (46133/46208)\n",
            "Epoch: 94 | Batch_idx: 370 |  Loss: (0.0120) | Acc: (99.84%) (47413/47488)\n",
            "Epoch: 94 | Batch_idx: 380 |  Loss: (0.0120) | Acc: (99.84%) (48689/48768)\n",
            "Epoch: 94 | Batch_idx: 390 |  Loss: (0.0120) | Acc: (99.84%) (49920/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2616) | Acc: (92.79%) (9279/10000)\n",
            "Epoch: 95 | Batch_idx: 0 |  Loss: (0.0135) | Acc: (100.00%) (128/128)\n",
            "Epoch: 95 | Batch_idx: 10 |  Loss: (0.0105) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 95 | Batch_idx: 20 |  Loss: (0.0126) | Acc: (99.81%) (2683/2688)\n",
            "Epoch: 95 | Batch_idx: 30 |  Loss: (0.0128) | Acc: (99.77%) (3959/3968)\n",
            "Epoch: 95 | Batch_idx: 40 |  Loss: (0.0121) | Acc: (99.83%) (5239/5248)\n",
            "Epoch: 95 | Batch_idx: 50 |  Loss: (0.0117) | Acc: (99.86%) (6519/6528)\n",
            "Epoch: 95 | Batch_idx: 60 |  Loss: (0.0118) | Acc: (99.87%) (7798/7808)\n",
            "Epoch: 95 | Batch_idx: 70 |  Loss: (0.0121) | Acc: (99.87%) (9076/9088)\n",
            "Epoch: 95 | Batch_idx: 80 |  Loss: (0.0122) | Acc: (99.87%) (10355/10368)\n",
            "Epoch: 95 | Batch_idx: 90 |  Loss: (0.0123) | Acc: (99.87%) (11633/11648)\n",
            "Epoch: 95 | Batch_idx: 100 |  Loss: (0.0121) | Acc: (99.87%) (12911/12928)\n",
            "Epoch: 95 | Batch_idx: 110 |  Loss: (0.0123) | Acc: (99.85%) (14187/14208)\n",
            "Epoch: 95 | Batch_idx: 120 |  Loss: (0.0125) | Acc: (99.84%) (15463/15488)\n",
            "Epoch: 95 | Batch_idx: 130 |  Loss: (0.0130) | Acc: (99.81%) (16736/16768)\n",
            "Epoch: 95 | Batch_idx: 140 |  Loss: (0.0127) | Acc: (99.82%) (18015/18048)\n",
            "Epoch: 95 | Batch_idx: 150 |  Loss: (0.0127) | Acc: (99.82%) (19294/19328)\n",
            "Epoch: 95 | Batch_idx: 160 |  Loss: (0.0126) | Acc: (99.83%) (20572/20608)\n",
            "Epoch: 95 | Batch_idx: 170 |  Loss: (0.0125) | Acc: (99.83%) (21850/21888)\n",
            "Epoch: 95 | Batch_idx: 180 |  Loss: (0.0124) | Acc: (99.83%) (23129/23168)\n",
            "Epoch: 95 | Batch_idx: 190 |  Loss: (0.0123) | Acc: (99.83%) (24407/24448)\n",
            "Epoch: 95 | Batch_idx: 200 |  Loss: (0.0125) | Acc: (99.82%) (25681/25728)\n",
            "Epoch: 95 | Batch_idx: 210 |  Loss: (0.0123) | Acc: (99.82%) (26960/27008)\n",
            "Epoch: 95 | Batch_idx: 220 |  Loss: (0.0123) | Acc: (99.83%) (28239/28288)\n",
            "Epoch: 95 | Batch_idx: 230 |  Loss: (0.0124) | Acc: (99.82%) (29516/29568)\n",
            "Epoch: 95 | Batch_idx: 240 |  Loss: (0.0122) | Acc: (99.83%) (30795/30848)\n",
            "Epoch: 95 | Batch_idx: 250 |  Loss: (0.0121) | Acc: (99.84%) (32075/32128)\n",
            "Epoch: 95 | Batch_idx: 260 |  Loss: (0.0120) | Acc: (99.84%) (33353/33408)\n",
            "Epoch: 95 | Batch_idx: 270 |  Loss: (0.0121) | Acc: (99.83%) (34629/34688)\n",
            "Epoch: 95 | Batch_idx: 280 |  Loss: (0.0120) | Acc: (99.83%) (35906/35968)\n",
            "Epoch: 95 | Batch_idx: 290 |  Loss: (0.0121) | Acc: (99.83%) (37183/37248)\n",
            "Epoch: 95 | Batch_idx: 300 |  Loss: (0.0119) | Acc: (99.83%) (38463/38528)\n",
            "Epoch: 95 | Batch_idx: 310 |  Loss: (0.0119) | Acc: (99.83%) (39739/39808)\n",
            "Epoch: 95 | Batch_idx: 320 |  Loss: (0.0119) | Acc: (99.82%) (41016/41088)\n",
            "Epoch: 95 | Batch_idx: 330 |  Loss: (0.0119) | Acc: (99.82%) (42293/42368)\n",
            "Epoch: 95 | Batch_idx: 340 |  Loss: (0.0118) | Acc: (99.83%) (43572/43648)\n",
            "Epoch: 95 | Batch_idx: 350 |  Loss: (0.0119) | Acc: (99.83%) (44850/44928)\n",
            "Epoch: 95 | Batch_idx: 360 |  Loss: (0.0119) | Acc: (99.83%) (46129/46208)\n",
            "Epoch: 95 | Batch_idx: 370 |  Loss: (0.0118) | Acc: (99.83%) (47407/47488)\n",
            "Epoch: 95 | Batch_idx: 380 |  Loss: (0.0118) | Acc: (99.83%) (48685/48768)\n",
            "Epoch: 95 | Batch_idx: 390 |  Loss: (0.0119) | Acc: (99.83%) (49914/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2573) | Acc: (92.82%) (9282/10000)\n",
            "Epoch: 96 | Batch_idx: 0 |  Loss: (0.0080) | Acc: (100.00%) (128/128)\n",
            "Epoch: 96 | Batch_idx: 10 |  Loss: (0.0092) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 96 | Batch_idx: 20 |  Loss: (0.0095) | Acc: (99.89%) (2685/2688)\n",
            "Epoch: 96 | Batch_idx: 30 |  Loss: (0.0094) | Acc: (99.92%) (3965/3968)\n",
            "Epoch: 96 | Batch_idx: 40 |  Loss: (0.0098) | Acc: (99.92%) (5244/5248)\n",
            "Epoch: 96 | Batch_idx: 50 |  Loss: (0.0102) | Acc: (99.91%) (6522/6528)\n",
            "Epoch: 96 | Batch_idx: 60 |  Loss: (0.0107) | Acc: (99.88%) (7799/7808)\n",
            "Epoch: 96 | Batch_idx: 70 |  Loss: (0.0109) | Acc: (99.88%) (9077/9088)\n",
            "Epoch: 96 | Batch_idx: 80 |  Loss: (0.0103) | Acc: (99.89%) (10357/10368)\n",
            "Epoch: 96 | Batch_idx: 90 |  Loss: (0.0107) | Acc: (99.89%) (11635/11648)\n",
            "Epoch: 96 | Batch_idx: 100 |  Loss: (0.0107) | Acc: (99.88%) (12913/12928)\n",
            "Epoch: 96 | Batch_idx: 110 |  Loss: (0.0108) | Acc: (99.88%) (14191/14208)\n",
            "Epoch: 96 | Batch_idx: 120 |  Loss: (0.0108) | Acc: (99.88%) (15470/15488)\n",
            "Epoch: 96 | Batch_idx: 130 |  Loss: (0.0107) | Acc: (99.89%) (16749/16768)\n",
            "Epoch: 96 | Batch_idx: 140 |  Loss: (0.0107) | Acc: (99.89%) (18028/18048)\n",
            "Epoch: 96 | Batch_idx: 150 |  Loss: (0.0107) | Acc: (99.90%) (19308/19328)\n",
            "Epoch: 96 | Batch_idx: 160 |  Loss: (0.0107) | Acc: (99.89%) (20586/20608)\n",
            "Epoch: 96 | Batch_idx: 170 |  Loss: (0.0111) | Acc: (99.89%) (21863/21888)\n",
            "Epoch: 96 | Batch_idx: 180 |  Loss: (0.0110) | Acc: (99.88%) (23140/23168)\n",
            "Epoch: 96 | Batch_idx: 190 |  Loss: (0.0108) | Acc: (99.88%) (24419/24448)\n",
            "Epoch: 96 | Batch_idx: 200 |  Loss: (0.0109) | Acc: (99.88%) (25698/25728)\n",
            "Epoch: 96 | Batch_idx: 210 |  Loss: (0.0108) | Acc: (99.89%) (26978/27008)\n",
            "Epoch: 96 | Batch_idx: 220 |  Loss: (0.0107) | Acc: (99.89%) (28258/28288)\n",
            "Epoch: 96 | Batch_idx: 230 |  Loss: (0.0107) | Acc: (99.90%) (29538/29568)\n",
            "Epoch: 96 | Batch_idx: 240 |  Loss: (0.0105) | Acc: (99.90%) (30818/30848)\n",
            "Epoch: 96 | Batch_idx: 250 |  Loss: (0.0106) | Acc: (99.90%) (32096/32128)\n",
            "Epoch: 96 | Batch_idx: 260 |  Loss: (0.0106) | Acc: (99.90%) (33375/33408)\n",
            "Epoch: 96 | Batch_idx: 270 |  Loss: (0.0106) | Acc: (99.90%) (34654/34688)\n",
            "Epoch: 96 | Batch_idx: 280 |  Loss: (0.0107) | Acc: (99.90%) (35932/35968)\n",
            "Epoch: 96 | Batch_idx: 290 |  Loss: (0.0108) | Acc: (99.90%) (37209/37248)\n",
            "Epoch: 96 | Batch_idx: 300 |  Loss: (0.0108) | Acc: (99.89%) (38487/38528)\n",
            "Epoch: 96 | Batch_idx: 310 |  Loss: (0.0108) | Acc: (99.89%) (39766/39808)\n",
            "Epoch: 96 | Batch_idx: 320 |  Loss: (0.0107) | Acc: (99.90%) (41046/41088)\n",
            "Epoch: 96 | Batch_idx: 330 |  Loss: (0.0107) | Acc: (99.90%) (42325/42368)\n",
            "Epoch: 96 | Batch_idx: 340 |  Loss: (0.0107) | Acc: (99.90%) (43603/43648)\n",
            "Epoch: 96 | Batch_idx: 350 |  Loss: (0.0107) | Acc: (99.90%) (44881/44928)\n",
            "Epoch: 96 | Batch_idx: 360 |  Loss: (0.0107) | Acc: (99.90%) (46161/46208)\n",
            "Epoch: 96 | Batch_idx: 370 |  Loss: (0.0107) | Acc: (99.90%) (47439/47488)\n",
            "Epoch: 96 | Batch_idx: 380 |  Loss: (0.0107) | Acc: (99.90%) (48719/48768)\n",
            "Epoch: 96 | Batch_idx: 390 |  Loss: (0.0108) | Acc: (99.90%) (49948/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2597) | Acc: (92.87%) (9287/10000)\n",
            "Epoch: 97 | Batch_idx: 0 |  Loss: (0.0091) | Acc: (100.00%) (128/128)\n",
            "Epoch: 97 | Batch_idx: 10 |  Loss: (0.0122) | Acc: (99.72%) (1404/1408)\n",
            "Epoch: 97 | Batch_idx: 20 |  Loss: (0.0115) | Acc: (99.78%) (2682/2688)\n",
            "Epoch: 97 | Batch_idx: 30 |  Loss: (0.0102) | Acc: (99.85%) (3962/3968)\n",
            "Epoch: 97 | Batch_idx: 40 |  Loss: (0.0117) | Acc: (99.79%) (5237/5248)\n",
            "Epoch: 97 | Batch_idx: 50 |  Loss: (0.0114) | Acc: (99.83%) (6517/6528)\n",
            "Epoch: 97 | Batch_idx: 60 |  Loss: (0.0115) | Acc: (99.83%) (7795/7808)\n",
            "Epoch: 97 | Batch_idx: 70 |  Loss: (0.0113) | Acc: (99.85%) (9074/9088)\n",
            "Epoch: 97 | Batch_idx: 80 |  Loss: (0.0109) | Acc: (99.86%) (10353/10368)\n",
            "Epoch: 97 | Batch_idx: 90 |  Loss: (0.0109) | Acc: (99.86%) (11632/11648)\n",
            "Epoch: 97 | Batch_idx: 100 |  Loss: (0.0112) | Acc: (99.85%) (12908/12928)\n",
            "Epoch: 97 | Batch_idx: 110 |  Loss: (0.0112) | Acc: (99.85%) (14186/14208)\n",
            "Epoch: 97 | Batch_idx: 120 |  Loss: (0.0114) | Acc: (99.85%) (15465/15488)\n",
            "Epoch: 97 | Batch_idx: 130 |  Loss: (0.0114) | Acc: (99.86%) (16744/16768)\n",
            "Epoch: 97 | Batch_idx: 140 |  Loss: (0.0120) | Acc: (99.83%) (18018/18048)\n",
            "Epoch: 97 | Batch_idx: 150 |  Loss: (0.0119) | Acc: (99.83%) (19295/19328)\n",
            "Epoch: 97 | Batch_idx: 160 |  Loss: (0.0119) | Acc: (99.83%) (20572/20608)\n",
            "Epoch: 97 | Batch_idx: 170 |  Loss: (0.0117) | Acc: (99.83%) (21851/21888)\n",
            "Epoch: 97 | Batch_idx: 180 |  Loss: (0.0116) | Acc: (99.83%) (23129/23168)\n",
            "Epoch: 97 | Batch_idx: 190 |  Loss: (0.0115) | Acc: (99.83%) (24406/24448)\n",
            "Epoch: 97 | Batch_idx: 200 |  Loss: (0.0115) | Acc: (99.83%) (25683/25728)\n",
            "Epoch: 97 | Batch_idx: 210 |  Loss: (0.0114) | Acc: (99.83%) (26962/27008)\n",
            "Epoch: 97 | Batch_idx: 220 |  Loss: (0.0113) | Acc: (99.83%) (28240/28288)\n",
            "Epoch: 97 | Batch_idx: 230 |  Loss: (0.0112) | Acc: (99.83%) (29519/29568)\n",
            "Epoch: 97 | Batch_idx: 240 |  Loss: (0.0113) | Acc: (99.83%) (30797/30848)\n",
            "Epoch: 97 | Batch_idx: 250 |  Loss: (0.0112) | Acc: (99.84%) (32077/32128)\n",
            "Epoch: 97 | Batch_idx: 260 |  Loss: (0.0112) | Acc: (99.84%) (33356/33408)\n",
            "Epoch: 97 | Batch_idx: 270 |  Loss: (0.0112) | Acc: (99.85%) (34636/34688)\n",
            "Epoch: 97 | Batch_idx: 280 |  Loss: (0.0111) | Acc: (99.85%) (35914/35968)\n",
            "Epoch: 97 | Batch_idx: 290 |  Loss: (0.0113) | Acc: (99.84%) (37190/37248)\n",
            "Epoch: 97 | Batch_idx: 300 |  Loss: (0.0113) | Acc: (99.84%) (38467/38528)\n",
            "Epoch: 97 | Batch_idx: 310 |  Loss: (0.0114) | Acc: (99.84%) (39743/39808)\n",
            "Epoch: 97 | Batch_idx: 320 |  Loss: (0.0113) | Acc: (99.84%) (41022/41088)\n",
            "Epoch: 97 | Batch_idx: 330 |  Loss: (0.0113) | Acc: (99.84%) (42301/42368)\n",
            "Epoch: 97 | Batch_idx: 340 |  Loss: (0.0112) | Acc: (99.84%) (43580/43648)\n",
            "Epoch: 97 | Batch_idx: 350 |  Loss: (0.0113) | Acc: (99.84%) (44857/44928)\n",
            "Epoch: 97 | Batch_idx: 360 |  Loss: (0.0112) | Acc: (99.85%) (46137/46208)\n",
            "Epoch: 97 | Batch_idx: 370 |  Loss: (0.0112) | Acc: (99.85%) (47417/47488)\n",
            "Epoch: 97 | Batch_idx: 380 |  Loss: (0.0112) | Acc: (99.85%) (48696/48768)\n",
            "Epoch: 97 | Batch_idx: 390 |  Loss: (0.0112) | Acc: (99.85%) (49927/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2601) | Acc: (92.70%) (9270/10000)\n",
            "Epoch: 98 | Batch_idx: 0 |  Loss: (0.0094) | Acc: (100.00%) (128/128)\n",
            "Epoch: 98 | Batch_idx: 10 |  Loss: (0.0094) | Acc: (99.86%) (1406/1408)\n",
            "Epoch: 98 | Batch_idx: 20 |  Loss: (0.0103) | Acc: (99.85%) (2684/2688)\n",
            "Epoch: 98 | Batch_idx: 30 |  Loss: (0.0100) | Acc: (99.87%) (3963/3968)\n",
            "Epoch: 98 | Batch_idx: 40 |  Loss: (0.0104) | Acc: (99.85%) (5240/5248)\n",
            "Epoch: 98 | Batch_idx: 50 |  Loss: (0.0102) | Acc: (99.86%) (6519/6528)\n",
            "Epoch: 98 | Batch_idx: 60 |  Loss: (0.0099) | Acc: (99.87%) (7798/7808)\n",
            "Epoch: 98 | Batch_idx: 70 |  Loss: (0.0099) | Acc: (99.88%) (9077/9088)\n",
            "Epoch: 98 | Batch_idx: 80 |  Loss: (0.0102) | Acc: (99.88%) (10356/10368)\n",
            "Epoch: 98 | Batch_idx: 90 |  Loss: (0.0101) | Acc: (99.89%) (11635/11648)\n",
            "Epoch: 98 | Batch_idx: 100 |  Loss: (0.0100) | Acc: (99.90%) (12915/12928)\n",
            "Epoch: 98 | Batch_idx: 110 |  Loss: (0.0104) | Acc: (99.89%) (14192/14208)\n",
            "Epoch: 98 | Batch_idx: 120 |  Loss: (0.0103) | Acc: (99.89%) (15471/15488)\n",
            "Epoch: 98 | Batch_idx: 130 |  Loss: (0.0103) | Acc: (99.88%) (16748/16768)\n",
            "Epoch: 98 | Batch_idx: 140 |  Loss: (0.0102) | Acc: (99.89%) (18028/18048)\n",
            "Epoch: 98 | Batch_idx: 150 |  Loss: (0.0102) | Acc: (99.89%) (19307/19328)\n",
            "Epoch: 98 | Batch_idx: 160 |  Loss: (0.0102) | Acc: (99.89%) (20585/20608)\n",
            "Epoch: 98 | Batch_idx: 170 |  Loss: (0.0103) | Acc: (99.89%) (21863/21888)\n",
            "Epoch: 98 | Batch_idx: 180 |  Loss: (0.0105) | Acc: (99.87%) (23138/23168)\n",
            "Epoch: 98 | Batch_idx: 190 |  Loss: (0.0104) | Acc: (99.87%) (24417/24448)\n",
            "Epoch: 98 | Batch_idx: 200 |  Loss: (0.0104) | Acc: (99.88%) (25696/25728)\n",
            "Epoch: 98 | Batch_idx: 210 |  Loss: (0.0103) | Acc: (99.88%) (26975/27008)\n",
            "Epoch: 98 | Batch_idx: 220 |  Loss: (0.0103) | Acc: (99.88%) (28253/28288)\n",
            "Epoch: 98 | Batch_idx: 230 |  Loss: (0.0103) | Acc: (99.87%) (29531/29568)\n",
            "Epoch: 98 | Batch_idx: 240 |  Loss: (0.0103) | Acc: (99.88%) (30810/30848)\n",
            "Epoch: 98 | Batch_idx: 250 |  Loss: (0.0103) | Acc: (99.88%) (32089/32128)\n",
            "Epoch: 98 | Batch_idx: 260 |  Loss: (0.0103) | Acc: (99.88%) (33367/33408)\n",
            "Epoch: 98 | Batch_idx: 270 |  Loss: (0.0103) | Acc: (99.88%) (34645/34688)\n",
            "Epoch: 98 | Batch_idx: 280 |  Loss: (0.0103) | Acc: (99.87%) (35922/35968)\n",
            "Epoch: 98 | Batch_idx: 290 |  Loss: (0.0104) | Acc: (99.87%) (37201/37248)\n",
            "Epoch: 98 | Batch_idx: 300 |  Loss: (0.0104) | Acc: (99.88%) (38480/38528)\n",
            "Epoch: 98 | Batch_idx: 310 |  Loss: (0.0104) | Acc: (99.88%) (39759/39808)\n",
            "Epoch: 98 | Batch_idx: 320 |  Loss: (0.0104) | Acc: (99.88%) (41037/41088)\n",
            "Epoch: 98 | Batch_idx: 330 |  Loss: (0.0104) | Acc: (99.88%) (42316/42368)\n",
            "Epoch: 98 | Batch_idx: 340 |  Loss: (0.0105) | Acc: (99.87%) (43593/43648)\n",
            "Epoch: 98 | Batch_idx: 350 |  Loss: (0.0104) | Acc: (99.88%) (44873/44928)\n",
            "Epoch: 98 | Batch_idx: 360 |  Loss: (0.0105) | Acc: (99.88%) (46151/46208)\n",
            "Epoch: 98 | Batch_idx: 370 |  Loss: (0.0104) | Acc: (99.88%) (47431/47488)\n",
            "Epoch: 98 | Batch_idx: 380 |  Loss: (0.0105) | Acc: (99.88%) (48708/48768)\n",
            "Epoch: 98 | Batch_idx: 390 |  Loss: (0.0106) | Acc: (99.87%) (49936/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2631) | Acc: (92.64%) (9264/10000)\n",
            "Epoch: 99 | Batch_idx: 0 |  Loss: (0.0125) | Acc: (100.00%) (128/128)\n",
            "Epoch: 99 | Batch_idx: 10 |  Loss: (0.0101) | Acc: (99.86%) (1406/1408)\n",
            "Epoch: 99 | Batch_idx: 20 |  Loss: (0.0102) | Acc: (99.89%) (2685/2688)\n",
            "Epoch: 99 | Batch_idx: 30 |  Loss: (0.0104) | Acc: (99.90%) (3964/3968)\n",
            "Epoch: 99 | Batch_idx: 40 |  Loss: (0.0108) | Acc: (99.90%) (5243/5248)\n",
            "Epoch: 99 | Batch_idx: 50 |  Loss: (0.0107) | Acc: (99.89%) (6521/6528)\n",
            "Epoch: 99 | Batch_idx: 60 |  Loss: (0.0104) | Acc: (99.91%) (7801/7808)\n",
            "Epoch: 99 | Batch_idx: 70 |  Loss: (0.0104) | Acc: (99.91%) (9080/9088)\n",
            "Epoch: 99 | Batch_idx: 80 |  Loss: (0.0105) | Acc: (99.89%) (10357/10368)\n",
            "Epoch: 99 | Batch_idx: 90 |  Loss: (0.0105) | Acc: (99.90%) (11636/11648)\n",
            "Epoch: 99 | Batch_idx: 100 |  Loss: (0.0105) | Acc: (99.90%) (12915/12928)\n",
            "Epoch: 99 | Batch_idx: 110 |  Loss: (0.0106) | Acc: (99.89%) (14193/14208)\n",
            "Epoch: 99 | Batch_idx: 120 |  Loss: (0.0105) | Acc: (99.90%) (15473/15488)\n",
            "Epoch: 99 | Batch_idx: 130 |  Loss: (0.0105) | Acc: (99.90%) (16752/16768)\n",
            "Epoch: 99 | Batch_idx: 140 |  Loss: (0.0104) | Acc: (99.91%) (18031/18048)\n",
            "Epoch: 99 | Batch_idx: 150 |  Loss: (0.0104) | Acc: (99.91%) (19311/19328)\n",
            "Epoch: 99 | Batch_idx: 160 |  Loss: (0.0102) | Acc: (99.92%) (20591/20608)\n",
            "Epoch: 99 | Batch_idx: 170 |  Loss: (0.0101) | Acc: (99.91%) (21869/21888)\n",
            "Epoch: 99 | Batch_idx: 180 |  Loss: (0.0102) | Acc: (99.91%) (23148/23168)\n",
            "Epoch: 99 | Batch_idx: 190 |  Loss: (0.0102) | Acc: (99.91%) (24426/24448)\n",
            "Epoch: 99 | Batch_idx: 200 |  Loss: (0.0101) | Acc: (99.91%) (25706/25728)\n",
            "Epoch: 99 | Batch_idx: 210 |  Loss: (0.0102) | Acc: (99.91%) (26984/27008)\n",
            "Epoch: 99 | Batch_idx: 220 |  Loss: (0.0103) | Acc: (99.92%) (28264/28288)\n",
            "Epoch: 99 | Batch_idx: 230 |  Loss: (0.0103) | Acc: (99.91%) (29541/29568)\n",
            "Epoch: 99 | Batch_idx: 240 |  Loss: (0.0104) | Acc: (99.91%) (30819/30848)\n",
            "Epoch: 99 | Batch_idx: 250 |  Loss: (0.0104) | Acc: (99.90%) (32096/32128)\n",
            "Epoch: 99 | Batch_idx: 260 |  Loss: (0.0104) | Acc: (99.90%) (33374/33408)\n",
            "Epoch: 99 | Batch_idx: 270 |  Loss: (0.0105) | Acc: (99.90%) (34652/34688)\n",
            "Epoch: 99 | Batch_idx: 280 |  Loss: (0.0104) | Acc: (99.90%) (35931/35968)\n",
            "Epoch: 99 | Batch_idx: 290 |  Loss: (0.0104) | Acc: (99.90%) (37211/37248)\n",
            "Epoch: 99 | Batch_idx: 300 |  Loss: (0.0103) | Acc: (99.90%) (38491/38528)\n",
            "Epoch: 99 | Batch_idx: 310 |  Loss: (0.0103) | Acc: (99.90%) (39770/39808)\n",
            "Epoch: 99 | Batch_idx: 320 |  Loss: (0.0103) | Acc: (99.91%) (41049/41088)\n",
            "Epoch: 99 | Batch_idx: 330 |  Loss: (0.0103) | Acc: (99.91%) (42328/42368)\n",
            "Epoch: 99 | Batch_idx: 340 |  Loss: (0.0103) | Acc: (99.90%) (43606/43648)\n",
            "Epoch: 99 | Batch_idx: 350 |  Loss: (0.0103) | Acc: (99.90%) (44885/44928)\n",
            "Epoch: 99 | Batch_idx: 360 |  Loss: (0.0103) | Acc: (99.90%) (46164/46208)\n",
            "Epoch: 99 | Batch_idx: 370 |  Loss: (0.0102) | Acc: (99.91%) (47443/47488)\n",
            "Epoch: 99 | Batch_idx: 380 |  Loss: (0.0103) | Acc: (99.90%) (48720/48768)\n",
            "Epoch: 99 | Batch_idx: 390 |  Loss: (0.0102) | Acc: (99.90%) (49951/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2611) | Acc: (92.87%) (9287/10000)\n",
            "Epoch: 100 | Batch_idx: 0 |  Loss: (0.0075) | Acc: (100.00%) (128/128)\n",
            "Epoch: 100 | Batch_idx: 10 |  Loss: (0.0101) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 100 | Batch_idx: 20 |  Loss: (0.0107) | Acc: (99.89%) (2685/2688)\n",
            "Epoch: 100 | Batch_idx: 30 |  Loss: (0.0118) | Acc: (99.85%) (3962/3968)\n",
            "Epoch: 100 | Batch_idx: 40 |  Loss: (0.0113) | Acc: (99.89%) (5242/5248)\n",
            "Epoch: 100 | Batch_idx: 50 |  Loss: (0.0110) | Acc: (99.89%) (6521/6528)\n",
            "Epoch: 100 | Batch_idx: 60 |  Loss: (0.0109) | Acc: (99.90%) (7800/7808)\n",
            "Epoch: 100 | Batch_idx: 70 |  Loss: (0.0103) | Acc: (99.91%) (9080/9088)\n",
            "Epoch: 100 | Batch_idx: 80 |  Loss: (0.0105) | Acc: (99.89%) (10357/10368)\n",
            "Epoch: 100 | Batch_idx: 90 |  Loss: (0.0104) | Acc: (99.91%) (11637/11648)\n",
            "Epoch: 100 | Batch_idx: 100 |  Loss: (0.0104) | Acc: (99.90%) (12915/12928)\n",
            "Epoch: 100 | Batch_idx: 110 |  Loss: (0.0104) | Acc: (99.90%) (14194/14208)\n",
            "Epoch: 100 | Batch_idx: 120 |  Loss: (0.0102) | Acc: (99.91%) (15474/15488)\n",
            "Epoch: 100 | Batch_idx: 130 |  Loss: (0.0102) | Acc: (99.91%) (16753/16768)\n",
            "Epoch: 100 | Batch_idx: 140 |  Loss: (0.0102) | Acc: (99.92%) (18033/18048)\n",
            "Epoch: 100 | Batch_idx: 150 |  Loss: (0.0101) | Acc: (99.91%) (19311/19328)\n",
            "Epoch: 100 | Batch_idx: 160 |  Loss: (0.0102) | Acc: (99.92%) (20591/20608)\n",
            "Epoch: 100 | Batch_idx: 170 |  Loss: (0.0102) | Acc: (99.92%) (21871/21888)\n",
            "Epoch: 100 | Batch_idx: 180 |  Loss: (0.0101) | Acc: (99.93%) (23151/23168)\n",
            "Epoch: 100 | Batch_idx: 190 |  Loss: (0.0100) | Acc: (99.93%) (24430/24448)\n",
            "Epoch: 100 | Batch_idx: 200 |  Loss: (0.0100) | Acc: (99.92%) (25708/25728)\n",
            "Epoch: 100 | Batch_idx: 210 |  Loss: (0.0101) | Acc: (99.92%) (26987/27008)\n",
            "Epoch: 100 | Batch_idx: 220 |  Loss: (0.0100) | Acc: (99.93%) (28267/28288)\n",
            "Epoch: 100 | Batch_idx: 230 |  Loss: (0.0099) | Acc: (99.93%) (29547/29568)\n",
            "Epoch: 100 | Batch_idx: 240 |  Loss: (0.0100) | Acc: (99.93%) (30825/30848)\n",
            "Epoch: 100 | Batch_idx: 250 |  Loss: (0.0099) | Acc: (99.93%) (32104/32128)\n",
            "Epoch: 100 | Batch_idx: 260 |  Loss: (0.0098) | Acc: (99.93%) (33384/33408)\n",
            "Epoch: 100 | Batch_idx: 270 |  Loss: (0.0098) | Acc: (99.93%) (34663/34688)\n",
            "Epoch: 100 | Batch_idx: 280 |  Loss: (0.0099) | Acc: (99.92%) (35941/35968)\n",
            "Epoch: 100 | Batch_idx: 290 |  Loss: (0.0101) | Acc: (99.92%) (37217/37248)\n",
            "Epoch: 100 | Batch_idx: 300 |  Loss: (0.0100) | Acc: (99.92%) (38497/38528)\n",
            "Epoch: 100 | Batch_idx: 310 |  Loss: (0.0100) | Acc: (99.92%) (39776/39808)\n",
            "Epoch: 100 | Batch_idx: 320 |  Loss: (0.0100) | Acc: (99.92%) (41054/41088)\n",
            "Epoch: 100 | Batch_idx: 330 |  Loss: (0.0100) | Acc: (99.92%) (42332/42368)\n",
            "Epoch: 100 | Batch_idx: 340 |  Loss: (0.0101) | Acc: (99.91%) (43610/43648)\n",
            "Epoch: 100 | Batch_idx: 350 |  Loss: (0.0101) | Acc: (99.91%) (44888/44928)\n",
            "Epoch: 100 | Batch_idx: 360 |  Loss: (0.0100) | Acc: (99.91%) (46165/46208)\n",
            "Epoch: 100 | Batch_idx: 370 |  Loss: (0.0101) | Acc: (99.91%) (47443/47488)\n",
            "Epoch: 100 | Batch_idx: 380 |  Loss: (0.0101) | Acc: (99.90%) (48720/48768)\n",
            "Epoch: 100 | Batch_idx: 390 |  Loss: (0.0101) | Acc: (99.90%) (49951/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2624) | Acc: (92.85%) (9285/10000)\n",
            "Epoch: 101 | Batch_idx: 0 |  Loss: (0.0095) | Acc: (100.00%) (128/128)\n",
            "Epoch: 101 | Batch_idx: 10 |  Loss: (0.0083) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 101 | Batch_idx: 20 |  Loss: (0.0088) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 101 | Batch_idx: 30 |  Loss: (0.0097) | Acc: (99.92%) (3965/3968)\n",
            "Epoch: 101 | Batch_idx: 40 |  Loss: (0.0095) | Acc: (99.92%) (5244/5248)\n",
            "Epoch: 101 | Batch_idx: 50 |  Loss: (0.0098) | Acc: (99.86%) (6519/6528)\n",
            "Epoch: 101 | Batch_idx: 60 |  Loss: (0.0099) | Acc: (99.86%) (7797/7808)\n",
            "Epoch: 101 | Batch_idx: 70 |  Loss: (0.0098) | Acc: (99.85%) (9074/9088)\n",
            "Epoch: 101 | Batch_idx: 80 |  Loss: (0.0097) | Acc: (99.86%) (10353/10368)\n",
            "Epoch: 101 | Batch_idx: 90 |  Loss: (0.0100) | Acc: (99.85%) (11630/11648)\n",
            "Epoch: 101 | Batch_idx: 100 |  Loss: (0.0102) | Acc: (99.85%) (12908/12928)\n",
            "Epoch: 101 | Batch_idx: 110 |  Loss: (0.0100) | Acc: (99.85%) (14187/14208)\n",
            "Epoch: 101 | Batch_idx: 120 |  Loss: (0.0100) | Acc: (99.86%) (15466/15488)\n",
            "Epoch: 101 | Batch_idx: 130 |  Loss: (0.0098) | Acc: (99.87%) (16746/16768)\n",
            "Epoch: 101 | Batch_idx: 140 |  Loss: (0.0100) | Acc: (99.86%) (18023/18048)\n",
            "Epoch: 101 | Batch_idx: 150 |  Loss: (0.0097) | Acc: (99.87%) (19303/19328)\n",
            "Epoch: 101 | Batch_idx: 160 |  Loss: (0.0098) | Acc: (99.86%) (20580/20608)\n",
            "Epoch: 101 | Batch_idx: 170 |  Loss: (0.0099) | Acc: (99.86%) (21857/21888)\n",
            "Epoch: 101 | Batch_idx: 180 |  Loss: (0.0099) | Acc: (99.86%) (23136/23168)\n",
            "Epoch: 101 | Batch_idx: 190 |  Loss: (0.0099) | Acc: (99.87%) (24416/24448)\n",
            "Epoch: 101 | Batch_idx: 200 |  Loss: (0.0101) | Acc: (99.85%) (25690/25728)\n",
            "Epoch: 101 | Batch_idx: 210 |  Loss: (0.0101) | Acc: (99.86%) (26970/27008)\n",
            "Epoch: 101 | Batch_idx: 220 |  Loss: (0.0102) | Acc: (99.86%) (28248/28288)\n",
            "Epoch: 101 | Batch_idx: 230 |  Loss: (0.0101) | Acc: (99.86%) (29526/29568)\n",
            "Epoch: 101 | Batch_idx: 240 |  Loss: (0.0101) | Acc: (99.86%) (30805/30848)\n",
            "Epoch: 101 | Batch_idx: 250 |  Loss: (0.0101) | Acc: (99.86%) (32084/32128)\n",
            "Epoch: 101 | Batch_idx: 260 |  Loss: (0.0101) | Acc: (99.87%) (33363/33408)\n",
            "Epoch: 101 | Batch_idx: 270 |  Loss: (0.0100) | Acc: (99.87%) (34642/34688)\n",
            "Epoch: 101 | Batch_idx: 280 |  Loss: (0.0100) | Acc: (99.87%) (35921/35968)\n",
            "Epoch: 101 | Batch_idx: 290 |  Loss: (0.0101) | Acc: (99.86%) (37197/37248)\n",
            "Epoch: 101 | Batch_idx: 300 |  Loss: (0.0101) | Acc: (99.87%) (38476/38528)\n",
            "Epoch: 101 | Batch_idx: 310 |  Loss: (0.0101) | Acc: (99.86%) (39754/39808)\n",
            "Epoch: 101 | Batch_idx: 320 |  Loss: (0.0102) | Acc: (99.87%) (41034/41088)\n",
            "Epoch: 101 | Batch_idx: 330 |  Loss: (0.0101) | Acc: (99.87%) (42314/42368)\n",
            "Epoch: 101 | Batch_idx: 340 |  Loss: (0.0101) | Acc: (99.87%) (43592/43648)\n",
            "Epoch: 101 | Batch_idx: 350 |  Loss: (0.0101) | Acc: (99.87%) (44869/44928)\n",
            "Epoch: 101 | Batch_idx: 360 |  Loss: (0.0101) | Acc: (99.87%) (46148/46208)\n",
            "Epoch: 101 | Batch_idx: 370 |  Loss: (0.0101) | Acc: (99.87%) (47428/47488)\n",
            "Epoch: 101 | Batch_idx: 380 |  Loss: (0.0101) | Acc: (99.87%) (48707/48768)\n",
            "Epoch: 101 | Batch_idx: 390 |  Loss: (0.0100) | Acc: (99.88%) (49939/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2608) | Acc: (92.76%) (9276/10000)\n",
            "Epoch: 102 | Batch_idx: 0 |  Loss: (0.0040) | Acc: (100.00%) (128/128)\n",
            "Epoch: 102 | Batch_idx: 10 |  Loss: (0.0091) | Acc: (99.79%) (1405/1408)\n",
            "Epoch: 102 | Batch_idx: 20 |  Loss: (0.0120) | Acc: (99.63%) (2678/2688)\n",
            "Epoch: 102 | Batch_idx: 30 |  Loss: (0.0112) | Acc: (99.75%) (3958/3968)\n",
            "Epoch: 102 | Batch_idx: 40 |  Loss: (0.0106) | Acc: (99.79%) (5237/5248)\n",
            "Epoch: 102 | Batch_idx: 50 |  Loss: (0.0106) | Acc: (99.82%) (6516/6528)\n",
            "Epoch: 102 | Batch_idx: 60 |  Loss: (0.0110) | Acc: (99.78%) (7791/7808)\n",
            "Epoch: 102 | Batch_idx: 70 |  Loss: (0.0106) | Acc: (99.81%) (9071/9088)\n",
            "Epoch: 102 | Batch_idx: 80 |  Loss: (0.0101) | Acc: (99.84%) (10351/10368)\n",
            "Epoch: 102 | Batch_idx: 90 |  Loss: (0.0099) | Acc: (99.85%) (11631/11648)\n",
            "Epoch: 102 | Batch_idx: 100 |  Loss: (0.0097) | Acc: (99.87%) (12911/12928)\n",
            "Epoch: 102 | Batch_idx: 110 |  Loss: (0.0098) | Acc: (99.87%) (14190/14208)\n",
            "Epoch: 102 | Batch_idx: 120 |  Loss: (0.0100) | Acc: (99.85%) (15465/15488)\n",
            "Epoch: 102 | Batch_idx: 130 |  Loss: (0.0101) | Acc: (99.84%) (16742/16768)\n",
            "Epoch: 102 | Batch_idx: 140 |  Loss: (0.0100) | Acc: (99.86%) (18022/18048)\n",
            "Epoch: 102 | Batch_idx: 150 |  Loss: (0.0100) | Acc: (99.86%) (19301/19328)\n",
            "Epoch: 102 | Batch_idx: 160 |  Loss: (0.0100) | Acc: (99.87%) (20581/20608)\n",
            "Epoch: 102 | Batch_idx: 170 |  Loss: (0.0099) | Acc: (99.87%) (21859/21888)\n",
            "Epoch: 102 | Batch_idx: 180 |  Loss: (0.0099) | Acc: (99.87%) (23138/23168)\n",
            "Epoch: 102 | Batch_idx: 190 |  Loss: (0.0101) | Acc: (99.87%) (24415/24448)\n",
            "Epoch: 102 | Batch_idx: 200 |  Loss: (0.0100) | Acc: (99.87%) (25695/25728)\n",
            "Epoch: 102 | Batch_idx: 210 |  Loss: (0.0101) | Acc: (99.86%) (26971/27008)\n",
            "Epoch: 102 | Batch_idx: 220 |  Loss: (0.0102) | Acc: (99.86%) (28249/28288)\n",
            "Epoch: 102 | Batch_idx: 230 |  Loss: (0.0102) | Acc: (99.86%) (29528/29568)\n",
            "Epoch: 102 | Batch_idx: 240 |  Loss: (0.0102) | Acc: (99.87%) (30807/30848)\n",
            "Epoch: 102 | Batch_idx: 250 |  Loss: (0.0101) | Acc: (99.87%) (32087/32128)\n",
            "Epoch: 102 | Batch_idx: 260 |  Loss: (0.0101) | Acc: (99.88%) (33367/33408)\n",
            "Epoch: 102 | Batch_idx: 270 |  Loss: (0.0102) | Acc: (99.87%) (34644/34688)\n",
            "Epoch: 102 | Batch_idx: 280 |  Loss: (0.0102) | Acc: (99.87%) (35923/35968)\n",
            "Epoch: 102 | Batch_idx: 290 |  Loss: (0.0102) | Acc: (99.87%) (37201/37248)\n",
            "Epoch: 102 | Batch_idx: 300 |  Loss: (0.0102) | Acc: (99.88%) (38480/38528)\n",
            "Epoch: 102 | Batch_idx: 310 |  Loss: (0.0102) | Acc: (99.88%) (39760/39808)\n",
            "Epoch: 102 | Batch_idx: 320 |  Loss: (0.0101) | Acc: (99.88%) (41039/41088)\n",
            "Epoch: 102 | Batch_idx: 330 |  Loss: (0.0101) | Acc: (99.88%) (42318/42368)\n",
            "Epoch: 102 | Batch_idx: 340 |  Loss: (0.0101) | Acc: (99.88%) (43596/43648)\n",
            "Epoch: 102 | Batch_idx: 350 |  Loss: (0.0101) | Acc: (99.88%) (44873/44928)\n",
            "Epoch: 102 | Batch_idx: 360 |  Loss: (0.0102) | Acc: (99.88%) (46151/46208)\n",
            "Epoch: 102 | Batch_idx: 370 |  Loss: (0.0102) | Acc: (99.88%) (47431/47488)\n",
            "Epoch: 102 | Batch_idx: 380 |  Loss: (0.0101) | Acc: (99.88%) (48711/48768)\n",
            "Epoch: 102 | Batch_idx: 390 |  Loss: (0.0101) | Acc: (99.88%) (49941/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2588) | Acc: (92.94%) (9294/10000)\n",
            "Epoch: 103 | Batch_idx: 0 |  Loss: (0.0070) | Acc: (100.00%) (128/128)\n",
            "Epoch: 103 | Batch_idx: 10 |  Loss: (0.0093) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 103 | Batch_idx: 20 |  Loss: (0.0101) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 103 | Batch_idx: 30 |  Loss: (0.0102) | Acc: (99.90%) (3964/3968)\n",
            "Epoch: 103 | Batch_idx: 40 |  Loss: (0.0108) | Acc: (99.89%) (5242/5248)\n",
            "Epoch: 103 | Batch_idx: 50 |  Loss: (0.0107) | Acc: (99.86%) (6519/6528)\n",
            "Epoch: 103 | Batch_idx: 60 |  Loss: (0.0108) | Acc: (99.86%) (7797/7808)\n",
            "Epoch: 103 | Batch_idx: 70 |  Loss: (0.0106) | Acc: (99.87%) (9076/9088)\n",
            "Epoch: 103 | Batch_idx: 80 |  Loss: (0.0105) | Acc: (99.88%) (10356/10368)\n",
            "Epoch: 103 | Batch_idx: 90 |  Loss: (0.0105) | Acc: (99.87%) (11633/11648)\n",
            "Epoch: 103 | Batch_idx: 100 |  Loss: (0.0103) | Acc: (99.88%) (12912/12928)\n",
            "Epoch: 103 | Batch_idx: 110 |  Loss: (0.0105) | Acc: (99.87%) (14190/14208)\n",
            "Epoch: 103 | Batch_idx: 120 |  Loss: (0.0103) | Acc: (99.88%) (15470/15488)\n",
            "Epoch: 103 | Batch_idx: 130 |  Loss: (0.0104) | Acc: (99.87%) (16747/16768)\n",
            "Epoch: 103 | Batch_idx: 140 |  Loss: (0.0104) | Acc: (99.88%) (18026/18048)\n",
            "Epoch: 103 | Batch_idx: 150 |  Loss: (0.0103) | Acc: (99.89%) (19306/19328)\n",
            "Epoch: 103 | Batch_idx: 160 |  Loss: (0.0103) | Acc: (99.89%) (20585/20608)\n",
            "Epoch: 103 | Batch_idx: 170 |  Loss: (0.0101) | Acc: (99.89%) (21865/21888)\n",
            "Epoch: 103 | Batch_idx: 180 |  Loss: (0.0102) | Acc: (99.90%) (23144/23168)\n",
            "Epoch: 103 | Batch_idx: 190 |  Loss: (0.0103) | Acc: (99.89%) (24421/24448)\n",
            "Epoch: 103 | Batch_idx: 200 |  Loss: (0.0102) | Acc: (99.89%) (25699/25728)\n",
            "Epoch: 103 | Batch_idx: 210 |  Loss: (0.0103) | Acc: (99.89%) (26977/27008)\n",
            "Epoch: 103 | Batch_idx: 220 |  Loss: (0.0103) | Acc: (99.89%) (28256/28288)\n",
            "Epoch: 103 | Batch_idx: 230 |  Loss: (0.0101) | Acc: (99.89%) (29536/29568)\n",
            "Epoch: 103 | Batch_idx: 240 |  Loss: (0.0101) | Acc: (99.89%) (30815/30848)\n",
            "Epoch: 103 | Batch_idx: 250 |  Loss: (0.0101) | Acc: (99.89%) (32093/32128)\n",
            "Epoch: 103 | Batch_idx: 260 |  Loss: (0.0102) | Acc: (99.89%) (33372/33408)\n",
            "Epoch: 103 | Batch_idx: 270 |  Loss: (0.0102) | Acc: (99.89%) (34651/34688)\n",
            "Epoch: 103 | Batch_idx: 280 |  Loss: (0.0101) | Acc: (99.89%) (35930/35968)\n",
            "Epoch: 103 | Batch_idx: 290 |  Loss: (0.0101) | Acc: (99.89%) (37207/37248)\n",
            "Epoch: 103 | Batch_idx: 300 |  Loss: (0.0102) | Acc: (99.89%) (38485/38528)\n",
            "Epoch: 103 | Batch_idx: 310 |  Loss: (0.0102) | Acc: (99.89%) (39763/39808)\n",
            "Epoch: 103 | Batch_idx: 320 |  Loss: (0.0101) | Acc: (99.89%) (41043/41088)\n",
            "Epoch: 103 | Batch_idx: 330 |  Loss: (0.0101) | Acc: (99.89%) (42322/42368)\n",
            "Epoch: 103 | Batch_idx: 340 |  Loss: (0.0100) | Acc: (99.89%) (43602/43648)\n",
            "Epoch: 103 | Batch_idx: 350 |  Loss: (0.0100) | Acc: (99.89%) (44880/44928)\n",
            "Epoch: 103 | Batch_idx: 360 |  Loss: (0.0100) | Acc: (99.90%) (46160/46208)\n",
            "Epoch: 103 | Batch_idx: 370 |  Loss: (0.0100) | Acc: (99.90%) (47439/47488)\n",
            "Epoch: 103 | Batch_idx: 380 |  Loss: (0.0100) | Acc: (99.90%) (48719/48768)\n",
            "Epoch: 103 | Batch_idx: 390 |  Loss: (0.0100) | Acc: (99.90%) (49949/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2617) | Acc: (92.74%) (9274/10000)\n",
            "Epoch: 104 | Batch_idx: 0 |  Loss: (0.0065) | Acc: (100.00%) (128/128)\n",
            "Epoch: 104 | Batch_idx: 10 |  Loss: (0.0104) | Acc: (99.86%) (1406/1408)\n",
            "Epoch: 104 | Batch_idx: 20 |  Loss: (0.0106) | Acc: (99.85%) (2684/2688)\n",
            "Epoch: 104 | Batch_idx: 30 |  Loss: (0.0100) | Acc: (99.90%) (3964/3968)\n",
            "Epoch: 104 | Batch_idx: 40 |  Loss: (0.0099) | Acc: (99.92%) (5244/5248)\n",
            "Epoch: 104 | Batch_idx: 50 |  Loss: (0.0096) | Acc: (99.92%) (6523/6528)\n",
            "Epoch: 104 | Batch_idx: 60 |  Loss: (0.0092) | Acc: (99.94%) (7803/7808)\n",
            "Epoch: 104 | Batch_idx: 70 |  Loss: (0.0090) | Acc: (99.94%) (9083/9088)\n",
            "Epoch: 104 | Batch_idx: 80 |  Loss: (0.0091) | Acc: (99.94%) (10362/10368)\n",
            "Epoch: 104 | Batch_idx: 90 |  Loss: (0.0092) | Acc: (99.94%) (11641/11648)\n",
            "Epoch: 104 | Batch_idx: 100 |  Loss: (0.0094) | Acc: (99.94%) (12920/12928)\n",
            "Epoch: 104 | Batch_idx: 110 |  Loss: (0.0093) | Acc: (99.94%) (14200/14208)\n",
            "Epoch: 104 | Batch_idx: 120 |  Loss: (0.0092) | Acc: (99.94%) (15479/15488)\n",
            "Epoch: 104 | Batch_idx: 130 |  Loss: (0.0092) | Acc: (99.94%) (16758/16768)\n",
            "Epoch: 104 | Batch_idx: 140 |  Loss: (0.0092) | Acc: (99.93%) (18036/18048)\n",
            "Epoch: 104 | Batch_idx: 150 |  Loss: (0.0092) | Acc: (99.93%) (19314/19328)\n",
            "Epoch: 104 | Batch_idx: 160 |  Loss: (0.0093) | Acc: (99.93%) (20593/20608)\n",
            "Epoch: 104 | Batch_idx: 170 |  Loss: (0.0095) | Acc: (99.92%) (21871/21888)\n",
            "Epoch: 104 | Batch_idx: 180 |  Loss: (0.0095) | Acc: (99.93%) (23151/23168)\n",
            "Epoch: 104 | Batch_idx: 190 |  Loss: (0.0095) | Acc: (99.93%) (24430/24448)\n",
            "Epoch: 104 | Batch_idx: 200 |  Loss: (0.0095) | Acc: (99.93%) (25709/25728)\n",
            "Epoch: 104 | Batch_idx: 210 |  Loss: (0.0094) | Acc: (99.93%) (26988/27008)\n",
            "Epoch: 104 | Batch_idx: 220 |  Loss: (0.0093) | Acc: (99.93%) (28268/28288)\n",
            "Epoch: 104 | Batch_idx: 230 |  Loss: (0.0093) | Acc: (99.93%) (29548/29568)\n",
            "Epoch: 104 | Batch_idx: 240 |  Loss: (0.0093) | Acc: (99.94%) (30828/30848)\n",
            "Epoch: 104 | Batch_idx: 250 |  Loss: (0.0094) | Acc: (99.93%) (32106/32128)\n",
            "Epoch: 104 | Batch_idx: 260 |  Loss: (0.0094) | Acc: (99.93%) (33384/33408)\n",
            "Epoch: 104 | Batch_idx: 270 |  Loss: (0.0094) | Acc: (99.93%) (34662/34688)\n",
            "Epoch: 104 | Batch_idx: 280 |  Loss: (0.0094) | Acc: (99.92%) (35941/35968)\n",
            "Epoch: 104 | Batch_idx: 290 |  Loss: (0.0093) | Acc: (99.92%) (37219/37248)\n",
            "Epoch: 104 | Batch_idx: 300 |  Loss: (0.0095) | Acc: (99.92%) (38497/38528)\n",
            "Epoch: 104 | Batch_idx: 310 |  Loss: (0.0094) | Acc: (99.92%) (39777/39808)\n",
            "Epoch: 104 | Batch_idx: 320 |  Loss: (0.0095) | Acc: (99.91%) (41051/41088)\n",
            "Epoch: 104 | Batch_idx: 330 |  Loss: (0.0094) | Acc: (99.91%) (42331/42368)\n",
            "Epoch: 104 | Batch_idx: 340 |  Loss: (0.0094) | Acc: (99.92%) (43611/43648)\n",
            "Epoch: 104 | Batch_idx: 350 |  Loss: (0.0095) | Acc: (99.91%) (44888/44928)\n",
            "Epoch: 104 | Batch_idx: 360 |  Loss: (0.0095) | Acc: (99.91%) (46166/46208)\n",
            "Epoch: 104 | Batch_idx: 370 |  Loss: (0.0095) | Acc: (99.91%) (47445/47488)\n",
            "Epoch: 104 | Batch_idx: 380 |  Loss: (0.0095) | Acc: (99.91%) (48725/48768)\n",
            "Epoch: 104 | Batch_idx: 390 |  Loss: (0.0095) | Acc: (99.91%) (49956/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2619) | Acc: (92.77%) (9277/10000)\n",
            "Epoch: 105 | Batch_idx: 0 |  Loss: (0.0052) | Acc: (100.00%) (128/128)\n",
            "Epoch: 105 | Batch_idx: 10 |  Loss: (0.0084) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 105 | Batch_idx: 20 |  Loss: (0.0091) | Acc: (99.89%) (2685/2688)\n",
            "Epoch: 105 | Batch_idx: 30 |  Loss: (0.0096) | Acc: (99.90%) (3964/3968)\n",
            "Epoch: 105 | Batch_idx: 40 |  Loss: (0.0091) | Acc: (99.90%) (5243/5248)\n",
            "Epoch: 105 | Batch_idx: 50 |  Loss: (0.0088) | Acc: (99.92%) (6523/6528)\n",
            "Epoch: 105 | Batch_idx: 60 |  Loss: (0.0091) | Acc: (99.91%) (7801/7808)\n",
            "Epoch: 105 | Batch_idx: 70 |  Loss: (0.0090) | Acc: (99.92%) (9081/9088)\n",
            "Epoch: 105 | Batch_idx: 80 |  Loss: (0.0090) | Acc: (99.93%) (10361/10368)\n",
            "Epoch: 105 | Batch_idx: 90 |  Loss: (0.0089) | Acc: (99.93%) (11640/11648)\n",
            "Epoch: 105 | Batch_idx: 100 |  Loss: (0.0089) | Acc: (99.94%) (12920/12928)\n",
            "Epoch: 105 | Batch_idx: 110 |  Loss: (0.0089) | Acc: (99.94%) (14199/14208)\n",
            "Epoch: 105 | Batch_idx: 120 |  Loss: (0.0091) | Acc: (99.94%) (15478/15488)\n",
            "Epoch: 105 | Batch_idx: 130 |  Loss: (0.0093) | Acc: (99.93%) (16756/16768)\n",
            "Epoch: 105 | Batch_idx: 140 |  Loss: (0.0095) | Acc: (99.92%) (18034/18048)\n",
            "Epoch: 105 | Batch_idx: 150 |  Loss: (0.0094) | Acc: (99.93%) (19314/19328)\n",
            "Epoch: 105 | Batch_idx: 160 |  Loss: (0.0091) | Acc: (99.93%) (20594/20608)\n",
            "Epoch: 105 | Batch_idx: 170 |  Loss: (0.0091) | Acc: (99.93%) (21872/21888)\n",
            "Epoch: 105 | Batch_idx: 180 |  Loss: (0.0091) | Acc: (99.93%) (23151/23168)\n",
            "Epoch: 105 | Batch_idx: 190 |  Loss: (0.0089) | Acc: (99.93%) (24431/24448)\n",
            "Epoch: 105 | Batch_idx: 200 |  Loss: (0.0089) | Acc: (99.93%) (25710/25728)\n",
            "Epoch: 105 | Batch_idx: 210 |  Loss: (0.0089) | Acc: (99.93%) (26988/27008)\n",
            "Epoch: 105 | Batch_idx: 220 |  Loss: (0.0089) | Acc: (99.93%) (28267/28288)\n",
            "Epoch: 105 | Batch_idx: 230 |  Loss: (0.0088) | Acc: (99.93%) (29546/29568)\n",
            "Epoch: 105 | Batch_idx: 240 |  Loss: (0.0088) | Acc: (99.93%) (30826/30848)\n",
            "Epoch: 105 | Batch_idx: 250 |  Loss: (0.0089) | Acc: (99.93%) (32105/32128)\n",
            "Epoch: 105 | Batch_idx: 260 |  Loss: (0.0088) | Acc: (99.93%) (33385/33408)\n",
            "Epoch: 105 | Batch_idx: 270 |  Loss: (0.0088) | Acc: (99.93%) (34665/34688)\n",
            "Epoch: 105 | Batch_idx: 280 |  Loss: (0.0088) | Acc: (99.93%) (35944/35968)\n",
            "Epoch: 105 | Batch_idx: 290 |  Loss: (0.0089) | Acc: (99.93%) (37221/37248)\n",
            "Epoch: 105 | Batch_idx: 300 |  Loss: (0.0089) | Acc: (99.93%) (38501/38528)\n",
            "Epoch: 105 | Batch_idx: 310 |  Loss: (0.0089) | Acc: (99.93%) (39780/39808)\n",
            "Epoch: 105 | Batch_idx: 320 |  Loss: (0.0090) | Acc: (99.93%) (41058/41088)\n",
            "Epoch: 105 | Batch_idx: 330 |  Loss: (0.0090) | Acc: (99.93%) (42338/42368)\n",
            "Epoch: 105 | Batch_idx: 340 |  Loss: (0.0090) | Acc: (99.93%) (43616/43648)\n",
            "Epoch: 105 | Batch_idx: 350 |  Loss: (0.0090) | Acc: (99.92%) (44894/44928)\n",
            "Epoch: 105 | Batch_idx: 360 |  Loss: (0.0091) | Acc: (99.92%) (46171/46208)\n",
            "Epoch: 105 | Batch_idx: 370 |  Loss: (0.0092) | Acc: (99.92%) (47450/47488)\n",
            "Epoch: 105 | Batch_idx: 380 |  Loss: (0.0091) | Acc: (99.92%) (48730/48768)\n",
            "Epoch: 105 | Batch_idx: 390 |  Loss: (0.0091) | Acc: (99.92%) (49961/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2597) | Acc: (92.78%) (9278/10000)\n",
            "Epoch: 106 | Batch_idx: 0 |  Loss: (0.0053) | Acc: (100.00%) (128/128)\n",
            "Epoch: 106 | Batch_idx: 10 |  Loss: (0.0080) | Acc: (99.86%) (1406/1408)\n",
            "Epoch: 106 | Batch_idx: 20 |  Loss: (0.0081) | Acc: (99.89%) (2685/2688)\n",
            "Epoch: 106 | Batch_idx: 30 |  Loss: (0.0082) | Acc: (99.90%) (3964/3968)\n",
            "Epoch: 106 | Batch_idx: 40 |  Loss: (0.0090) | Acc: (99.85%) (5240/5248)\n",
            "Epoch: 106 | Batch_idx: 50 |  Loss: (0.0092) | Acc: (99.83%) (6517/6528)\n",
            "Epoch: 106 | Batch_idx: 60 |  Loss: (0.0093) | Acc: (99.82%) (7794/7808)\n",
            "Epoch: 106 | Batch_idx: 70 |  Loss: (0.0091) | Acc: (99.83%) (9073/9088)\n",
            "Epoch: 106 | Batch_idx: 80 |  Loss: (0.0090) | Acc: (99.85%) (10352/10368)\n",
            "Epoch: 106 | Batch_idx: 90 |  Loss: (0.0088) | Acc: (99.86%) (11632/11648)\n",
            "Epoch: 106 | Batch_idx: 100 |  Loss: (0.0087) | Acc: (99.88%) (12912/12928)\n",
            "Epoch: 106 | Batch_idx: 110 |  Loss: (0.0087) | Acc: (99.89%) (14192/14208)\n",
            "Epoch: 106 | Batch_idx: 120 |  Loss: (0.0086) | Acc: (99.89%) (15471/15488)\n",
            "Epoch: 106 | Batch_idx: 130 |  Loss: (0.0089) | Acc: (99.88%) (16748/16768)\n",
            "Epoch: 106 | Batch_idx: 140 |  Loss: (0.0090) | Acc: (99.88%) (18026/18048)\n",
            "Epoch: 106 | Batch_idx: 150 |  Loss: (0.0092) | Acc: (99.88%) (19305/19328)\n",
            "Epoch: 106 | Batch_idx: 160 |  Loss: (0.0091) | Acc: (99.89%) (20585/20608)\n",
            "Epoch: 106 | Batch_idx: 170 |  Loss: (0.0091) | Acc: (99.89%) (21863/21888)\n",
            "Epoch: 106 | Batch_idx: 180 |  Loss: (0.0091) | Acc: (99.88%) (23141/23168)\n",
            "Epoch: 106 | Batch_idx: 190 |  Loss: (0.0092) | Acc: (99.89%) (24420/24448)\n",
            "Epoch: 106 | Batch_idx: 200 |  Loss: (0.0091) | Acc: (99.88%) (25698/25728)\n",
            "Epoch: 106 | Batch_idx: 210 |  Loss: (0.0090) | Acc: (99.89%) (26977/27008)\n",
            "Epoch: 106 | Batch_idx: 220 |  Loss: (0.0090) | Acc: (99.89%) (28256/28288)\n",
            "Epoch: 106 | Batch_idx: 230 |  Loss: (0.0089) | Acc: (99.89%) (29535/29568)\n",
            "Epoch: 106 | Batch_idx: 240 |  Loss: (0.0090) | Acc: (99.89%) (30813/30848)\n",
            "Epoch: 106 | Batch_idx: 250 |  Loss: (0.0090) | Acc: (99.89%) (32092/32128)\n",
            "Epoch: 106 | Batch_idx: 260 |  Loss: (0.0091) | Acc: (99.89%) (33370/33408)\n",
            "Epoch: 106 | Batch_idx: 270 |  Loss: (0.0091) | Acc: (99.89%) (34649/34688)\n",
            "Epoch: 106 | Batch_idx: 280 |  Loss: (0.0091) | Acc: (99.89%) (35927/35968)\n",
            "Epoch: 106 | Batch_idx: 290 |  Loss: (0.0090) | Acc: (99.89%) (37206/37248)\n",
            "Epoch: 106 | Batch_idx: 300 |  Loss: (0.0090) | Acc: (99.89%) (38485/38528)\n",
            "Epoch: 106 | Batch_idx: 310 |  Loss: (0.0091) | Acc: (99.88%) (39761/39808)\n",
            "Epoch: 106 | Batch_idx: 320 |  Loss: (0.0091) | Acc: (99.89%) (41041/41088)\n",
            "Epoch: 106 | Batch_idx: 330 |  Loss: (0.0092) | Acc: (99.88%) (42317/42368)\n",
            "Epoch: 106 | Batch_idx: 340 |  Loss: (0.0091) | Acc: (99.88%) (43595/43648)\n",
            "Epoch: 106 | Batch_idx: 350 |  Loss: (0.0091) | Acc: (99.88%) (44874/44928)\n",
            "Epoch: 106 | Batch_idx: 360 |  Loss: (0.0091) | Acc: (99.88%) (46154/46208)\n",
            "Epoch: 106 | Batch_idx: 370 |  Loss: (0.0090) | Acc: (99.89%) (47434/47488)\n",
            "Epoch: 106 | Batch_idx: 380 |  Loss: (0.0090) | Acc: (99.89%) (48714/48768)\n",
            "Epoch: 106 | Batch_idx: 390 |  Loss: (0.0090) | Acc: (99.89%) (49945/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2577) | Acc: (92.96%) (9296/10000)\n",
            "Epoch: 107 | Batch_idx: 0 |  Loss: (0.0046) | Acc: (100.00%) (128/128)\n",
            "Epoch: 107 | Batch_idx: 10 |  Loss: (0.0078) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 107 | Batch_idx: 20 |  Loss: (0.0074) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 107 | Batch_idx: 30 |  Loss: (0.0080) | Acc: (99.90%) (3964/3968)\n",
            "Epoch: 107 | Batch_idx: 40 |  Loss: (0.0077) | Acc: (99.92%) (5244/5248)\n",
            "Epoch: 107 | Batch_idx: 50 |  Loss: (0.0079) | Acc: (99.92%) (6523/6528)\n",
            "Epoch: 107 | Batch_idx: 60 |  Loss: (0.0081) | Acc: (99.90%) (7800/7808)\n",
            "Epoch: 107 | Batch_idx: 70 |  Loss: (0.0082) | Acc: (99.91%) (9080/9088)\n",
            "Epoch: 107 | Batch_idx: 80 |  Loss: (0.0082) | Acc: (99.91%) (10359/10368)\n",
            "Epoch: 107 | Batch_idx: 90 |  Loss: (0.0082) | Acc: (99.91%) (11637/11648)\n",
            "Epoch: 107 | Batch_idx: 100 |  Loss: (0.0082) | Acc: (99.90%) (12915/12928)\n",
            "Epoch: 107 | Batch_idx: 110 |  Loss: (0.0081) | Acc: (99.91%) (14195/14208)\n",
            "Epoch: 107 | Batch_idx: 120 |  Loss: (0.0081) | Acc: (99.92%) (15475/15488)\n",
            "Epoch: 107 | Batch_idx: 130 |  Loss: (0.0083) | Acc: (99.91%) (16753/16768)\n",
            "Epoch: 107 | Batch_idx: 140 |  Loss: (0.0083) | Acc: (99.91%) (18032/18048)\n",
            "Epoch: 107 | Batch_idx: 150 |  Loss: (0.0085) | Acc: (99.91%) (19310/19328)\n",
            "Epoch: 107 | Batch_idx: 160 |  Loss: (0.0084) | Acc: (99.91%) (20589/20608)\n",
            "Epoch: 107 | Batch_idx: 170 |  Loss: (0.0087) | Acc: (99.91%) (21868/21888)\n",
            "Epoch: 107 | Batch_idx: 180 |  Loss: (0.0086) | Acc: (99.91%) (23148/23168)\n",
            "Epoch: 107 | Batch_idx: 190 |  Loss: (0.0085) | Acc: (99.91%) (24427/24448)\n",
            "Epoch: 107 | Batch_idx: 200 |  Loss: (0.0085) | Acc: (99.91%) (25704/25728)\n",
            "Epoch: 107 | Batch_idx: 210 |  Loss: (0.0085) | Acc: (99.91%) (26983/27008)\n",
            "Epoch: 107 | Batch_idx: 220 |  Loss: (0.0086) | Acc: (99.90%) (28261/28288)\n",
            "Epoch: 107 | Batch_idx: 230 |  Loss: (0.0087) | Acc: (99.91%) (29540/29568)\n",
            "Epoch: 107 | Batch_idx: 240 |  Loss: (0.0087) | Acc: (99.91%) (30819/30848)\n",
            "Epoch: 107 | Batch_idx: 250 |  Loss: (0.0086) | Acc: (99.91%) (32098/32128)\n",
            "Epoch: 107 | Batch_idx: 260 |  Loss: (0.0086) | Acc: (99.91%) (33378/33408)\n",
            "Epoch: 107 | Batch_idx: 270 |  Loss: (0.0087) | Acc: (99.91%) (34657/34688)\n",
            "Epoch: 107 | Batch_idx: 280 |  Loss: (0.0086) | Acc: (99.91%) (35937/35968)\n",
            "Epoch: 107 | Batch_idx: 290 |  Loss: (0.0086) | Acc: (99.92%) (37217/37248)\n",
            "Epoch: 107 | Batch_idx: 300 |  Loss: (0.0085) | Acc: (99.92%) (38496/38528)\n",
            "Epoch: 107 | Batch_idx: 310 |  Loss: (0.0086) | Acc: (99.92%) (39775/39808)\n",
            "Epoch: 107 | Batch_idx: 320 |  Loss: (0.0085) | Acc: (99.92%) (41055/41088)\n",
            "Epoch: 107 | Batch_idx: 330 |  Loss: (0.0085) | Acc: (99.92%) (42335/42368)\n",
            "Epoch: 107 | Batch_idx: 340 |  Loss: (0.0085) | Acc: (99.92%) (43613/43648)\n",
            "Epoch: 107 | Batch_idx: 350 |  Loss: (0.0085) | Acc: (99.92%) (44892/44928)\n",
            "Epoch: 107 | Batch_idx: 360 |  Loss: (0.0085) | Acc: (99.92%) (46171/46208)\n",
            "Epoch: 107 | Batch_idx: 370 |  Loss: (0.0085) | Acc: (99.92%) (47451/47488)\n",
            "Epoch: 107 | Batch_idx: 380 |  Loss: (0.0086) | Acc: (99.92%) (48729/48768)\n",
            "Epoch: 107 | Batch_idx: 390 |  Loss: (0.0087) | Acc: (99.92%) (49960/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2597) | Acc: (92.82%) (9282/10000)\n",
            "Epoch: 108 | Batch_idx: 0 |  Loss: (0.0078) | Acc: (100.00%) (128/128)\n",
            "Epoch: 108 | Batch_idx: 10 |  Loss: (0.0074) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 108 | Batch_idx: 20 |  Loss: (0.0083) | Acc: (99.89%) (2685/2688)\n",
            "Epoch: 108 | Batch_idx: 30 |  Loss: (0.0082) | Acc: (99.92%) (3965/3968)\n",
            "Epoch: 108 | Batch_idx: 40 |  Loss: (0.0086) | Acc: (99.89%) (5242/5248)\n",
            "Epoch: 108 | Batch_idx: 50 |  Loss: (0.0089) | Acc: (99.89%) (6521/6528)\n",
            "Epoch: 108 | Batch_idx: 60 |  Loss: (0.0087) | Acc: (99.90%) (7800/7808)\n",
            "Epoch: 108 | Batch_idx: 70 |  Loss: (0.0088) | Acc: (99.91%) (9080/9088)\n",
            "Epoch: 108 | Batch_idx: 80 |  Loss: (0.0090) | Acc: (99.91%) (10359/10368)\n",
            "Epoch: 108 | Batch_idx: 90 |  Loss: (0.0089) | Acc: (99.92%) (11639/11648)\n",
            "Epoch: 108 | Batch_idx: 100 |  Loss: (0.0088) | Acc: (99.92%) (12918/12928)\n",
            "Epoch: 108 | Batch_idx: 110 |  Loss: (0.0088) | Acc: (99.93%) (14198/14208)\n",
            "Epoch: 108 | Batch_idx: 120 |  Loss: (0.0089) | Acc: (99.93%) (15477/15488)\n",
            "Epoch: 108 | Batch_idx: 130 |  Loss: (0.0088) | Acc: (99.92%) (16755/16768)\n",
            "Epoch: 108 | Batch_idx: 140 |  Loss: (0.0089) | Acc: (99.92%) (18033/18048)\n",
            "Epoch: 108 | Batch_idx: 150 |  Loss: (0.0088) | Acc: (99.91%) (19311/19328)\n",
            "Epoch: 108 | Batch_idx: 160 |  Loss: (0.0088) | Acc: (99.92%) (20591/20608)\n",
            "Epoch: 108 | Batch_idx: 170 |  Loss: (0.0087) | Acc: (99.92%) (21870/21888)\n",
            "Epoch: 108 | Batch_idx: 180 |  Loss: (0.0088) | Acc: (99.91%) (23147/23168)\n",
            "Epoch: 108 | Batch_idx: 190 |  Loss: (0.0087) | Acc: (99.91%) (24427/24448)\n",
            "Epoch: 108 | Batch_idx: 200 |  Loss: (0.0088) | Acc: (99.92%) (25707/25728)\n",
            "Epoch: 108 | Batch_idx: 210 |  Loss: (0.0087) | Acc: (99.92%) (26987/27008)\n",
            "Epoch: 108 | Batch_idx: 220 |  Loss: (0.0087) | Acc: (99.93%) (28267/28288)\n",
            "Epoch: 108 | Batch_idx: 230 |  Loss: (0.0087) | Acc: (99.93%) (29546/29568)\n",
            "Epoch: 108 | Batch_idx: 240 |  Loss: (0.0087) | Acc: (99.93%) (30826/30848)\n",
            "Epoch: 108 | Batch_idx: 250 |  Loss: (0.0087) | Acc: (99.93%) (32105/32128)\n",
            "Epoch: 108 | Batch_idx: 260 |  Loss: (0.0087) | Acc: (99.93%) (33385/33408)\n",
            "Epoch: 108 | Batch_idx: 270 |  Loss: (0.0086) | Acc: (99.93%) (34665/34688)\n",
            "Epoch: 108 | Batch_idx: 280 |  Loss: (0.0086) | Acc: (99.93%) (35943/35968)\n",
            "Epoch: 108 | Batch_idx: 290 |  Loss: (0.0086) | Acc: (99.93%) (37223/37248)\n",
            "Epoch: 108 | Batch_idx: 300 |  Loss: (0.0086) | Acc: (99.94%) (38503/38528)\n",
            "Epoch: 108 | Batch_idx: 310 |  Loss: (0.0086) | Acc: (99.93%) (39782/39808)\n",
            "Epoch: 108 | Batch_idx: 320 |  Loss: (0.0086) | Acc: (99.94%) (41062/41088)\n",
            "Epoch: 108 | Batch_idx: 330 |  Loss: (0.0085) | Acc: (99.94%) (42342/42368)\n",
            "Epoch: 108 | Batch_idx: 340 |  Loss: (0.0085) | Acc: (99.94%) (43621/43648)\n",
            "Epoch: 108 | Batch_idx: 350 |  Loss: (0.0085) | Acc: (99.94%) (44901/44928)\n",
            "Epoch: 108 | Batch_idx: 360 |  Loss: (0.0085) | Acc: (99.94%) (46181/46208)\n",
            "Epoch: 108 | Batch_idx: 370 |  Loss: (0.0085) | Acc: (99.94%) (47459/47488)\n",
            "Epoch: 108 | Batch_idx: 380 |  Loss: (0.0084) | Acc: (99.94%) (48739/48768)\n",
            "Epoch: 108 | Batch_idx: 390 |  Loss: (0.0084) | Acc: (99.94%) (49971/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2588) | Acc: (93.07%) (9307/10000)\n",
            "Epoch: 109 | Batch_idx: 0 |  Loss: (0.0035) | Acc: (100.00%) (128/128)\n",
            "Epoch: 109 | Batch_idx: 10 |  Loss: (0.0078) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 109 | Batch_idx: 20 |  Loss: (0.0084) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 109 | Batch_idx: 30 |  Loss: (0.0081) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 109 | Batch_idx: 40 |  Loss: (0.0083) | Acc: (99.92%) (5244/5248)\n",
            "Epoch: 109 | Batch_idx: 50 |  Loss: (0.0087) | Acc: (99.91%) (6522/6528)\n",
            "Epoch: 109 | Batch_idx: 60 |  Loss: (0.0085) | Acc: (99.92%) (7802/7808)\n",
            "Epoch: 109 | Batch_idx: 70 |  Loss: (0.0084) | Acc: (99.92%) (9081/9088)\n",
            "Epoch: 109 | Batch_idx: 80 |  Loss: (0.0082) | Acc: (99.93%) (10361/10368)\n",
            "Epoch: 109 | Batch_idx: 90 |  Loss: (0.0080) | Acc: (99.94%) (11641/11648)\n",
            "Epoch: 109 | Batch_idx: 100 |  Loss: (0.0082) | Acc: (99.94%) (12920/12928)\n",
            "Epoch: 109 | Batch_idx: 110 |  Loss: (0.0082) | Acc: (99.94%) (14199/14208)\n",
            "Epoch: 109 | Batch_idx: 120 |  Loss: (0.0081) | Acc: (99.94%) (15479/15488)\n",
            "Epoch: 109 | Batch_idx: 130 |  Loss: (0.0081) | Acc: (99.95%) (16759/16768)\n",
            "Epoch: 109 | Batch_idx: 140 |  Loss: (0.0081) | Acc: (99.94%) (18038/18048)\n",
            "Epoch: 109 | Batch_idx: 150 |  Loss: (0.0080) | Acc: (99.95%) (19318/19328)\n",
            "Epoch: 109 | Batch_idx: 160 |  Loss: (0.0080) | Acc: (99.95%) (20598/20608)\n",
            "Epoch: 109 | Batch_idx: 170 |  Loss: (0.0082) | Acc: (99.95%) (21877/21888)\n",
            "Epoch: 109 | Batch_idx: 180 |  Loss: (0.0083) | Acc: (99.94%) (23153/23168)\n",
            "Epoch: 109 | Batch_idx: 190 |  Loss: (0.0083) | Acc: (99.93%) (24432/24448)\n",
            "Epoch: 109 | Batch_idx: 200 |  Loss: (0.0083) | Acc: (99.93%) (25711/25728)\n",
            "Epoch: 109 | Batch_idx: 210 |  Loss: (0.0084) | Acc: (99.93%) (26988/27008)\n",
            "Epoch: 109 | Batch_idx: 220 |  Loss: (0.0084) | Acc: (99.92%) (28266/28288)\n",
            "Epoch: 109 | Batch_idx: 230 |  Loss: (0.0084) | Acc: (99.92%) (29545/29568)\n",
            "Epoch: 109 | Batch_idx: 240 |  Loss: (0.0085) | Acc: (99.92%) (30824/30848)\n",
            "Epoch: 109 | Batch_idx: 250 |  Loss: (0.0084) | Acc: (99.93%) (32104/32128)\n",
            "Epoch: 109 | Batch_idx: 260 |  Loss: (0.0084) | Acc: (99.93%) (33384/33408)\n",
            "Epoch: 109 | Batch_idx: 270 |  Loss: (0.0084) | Acc: (99.93%) (34664/34688)\n",
            "Epoch: 109 | Batch_idx: 280 |  Loss: (0.0083) | Acc: (99.93%) (35944/35968)\n",
            "Epoch: 109 | Batch_idx: 290 |  Loss: (0.0083) | Acc: (99.94%) (37224/37248)\n",
            "Epoch: 109 | Batch_idx: 300 |  Loss: (0.0083) | Acc: (99.93%) (38502/38528)\n",
            "Epoch: 109 | Batch_idx: 310 |  Loss: (0.0084) | Acc: (99.93%) (39779/39808)\n",
            "Epoch: 109 | Batch_idx: 320 |  Loss: (0.0084) | Acc: (99.93%) (41059/41088)\n",
            "Epoch: 109 | Batch_idx: 330 |  Loss: (0.0085) | Acc: (99.93%) (42337/42368)\n",
            "Epoch: 109 | Batch_idx: 340 |  Loss: (0.0085) | Acc: (99.93%) (43617/43648)\n",
            "Epoch: 109 | Batch_idx: 350 |  Loss: (0.0085) | Acc: (99.93%) (44897/44928)\n",
            "Epoch: 109 | Batch_idx: 360 |  Loss: (0.0084) | Acc: (99.93%) (46175/46208)\n",
            "Epoch: 109 | Batch_idx: 370 |  Loss: (0.0084) | Acc: (99.93%) (47455/47488)\n",
            "Epoch: 109 | Batch_idx: 380 |  Loss: (0.0084) | Acc: (99.93%) (48733/48768)\n",
            "Epoch: 109 | Batch_idx: 390 |  Loss: (0.0084) | Acc: (99.92%) (49962/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2572) | Acc: (93.02%) (9302/10000)\n",
            "Epoch: 110 | Batch_idx: 0 |  Loss: (0.0167) | Acc: (99.22%) (127/128)\n",
            "Epoch: 110 | Batch_idx: 10 |  Loss: (0.0096) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 110 | Batch_idx: 20 |  Loss: (0.0091) | Acc: (99.89%) (2685/2688)\n",
            "Epoch: 110 | Batch_idx: 30 |  Loss: (0.0087) | Acc: (99.92%) (3965/3968)\n",
            "Epoch: 110 | Batch_idx: 40 |  Loss: (0.0089) | Acc: (99.94%) (5245/5248)\n",
            "Epoch: 110 | Batch_idx: 50 |  Loss: (0.0086) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 110 | Batch_idx: 60 |  Loss: (0.0085) | Acc: (99.94%) (7803/7808)\n",
            "Epoch: 110 | Batch_idx: 70 |  Loss: (0.0086) | Acc: (99.93%) (9082/9088)\n",
            "Epoch: 110 | Batch_idx: 80 |  Loss: (0.0088) | Acc: (99.92%) (10360/10368)\n",
            "Epoch: 110 | Batch_idx: 90 |  Loss: (0.0090) | Acc: (99.91%) (11638/11648)\n",
            "Epoch: 110 | Batch_idx: 100 |  Loss: (0.0090) | Acc: (99.90%) (12915/12928)\n",
            "Epoch: 110 | Batch_idx: 110 |  Loss: (0.0088) | Acc: (99.90%) (14194/14208)\n",
            "Epoch: 110 | Batch_idx: 120 |  Loss: (0.0090) | Acc: (99.90%) (15473/15488)\n",
            "Epoch: 110 | Batch_idx: 130 |  Loss: (0.0089) | Acc: (99.91%) (16753/16768)\n",
            "Epoch: 110 | Batch_idx: 140 |  Loss: (0.0088) | Acc: (99.92%) (18033/18048)\n",
            "Epoch: 110 | Batch_idx: 150 |  Loss: (0.0088) | Acc: (99.92%) (19313/19328)\n",
            "Epoch: 110 | Batch_idx: 160 |  Loss: (0.0087) | Acc: (99.93%) (20593/20608)\n",
            "Epoch: 110 | Batch_idx: 170 |  Loss: (0.0087) | Acc: (99.93%) (21872/21888)\n",
            "Epoch: 110 | Batch_idx: 180 |  Loss: (0.0087) | Acc: (99.93%) (23151/23168)\n",
            "Epoch: 110 | Batch_idx: 190 |  Loss: (0.0088) | Acc: (99.92%) (24429/24448)\n",
            "Epoch: 110 | Batch_idx: 200 |  Loss: (0.0088) | Acc: (99.93%) (25709/25728)\n",
            "Epoch: 110 | Batch_idx: 210 |  Loss: (0.0088) | Acc: (99.93%) (26989/27008)\n",
            "Epoch: 110 | Batch_idx: 220 |  Loss: (0.0088) | Acc: (99.93%) (28268/28288)\n",
            "Epoch: 110 | Batch_idx: 230 |  Loss: (0.0087) | Acc: (99.93%) (29548/29568)\n",
            "Epoch: 110 | Batch_idx: 240 |  Loss: (0.0088) | Acc: (99.92%) (30824/30848)\n",
            "Epoch: 110 | Batch_idx: 250 |  Loss: (0.0087) | Acc: (99.93%) (32104/32128)\n",
            "Epoch: 110 | Batch_idx: 260 |  Loss: (0.0087) | Acc: (99.93%) (33383/33408)\n",
            "Epoch: 110 | Batch_idx: 270 |  Loss: (0.0086) | Acc: (99.92%) (34661/34688)\n",
            "Epoch: 110 | Batch_idx: 280 |  Loss: (0.0086) | Acc: (99.92%) (35941/35968)\n",
            "Epoch: 110 | Batch_idx: 290 |  Loss: (0.0086) | Acc: (99.93%) (37221/37248)\n",
            "Epoch: 110 | Batch_idx: 300 |  Loss: (0.0086) | Acc: (99.93%) (38501/38528)\n",
            "Epoch: 110 | Batch_idx: 310 |  Loss: (0.0086) | Acc: (99.93%) (39781/39808)\n",
            "Epoch: 110 | Batch_idx: 320 |  Loss: (0.0086) | Acc: (99.93%) (41061/41088)\n",
            "Epoch: 110 | Batch_idx: 330 |  Loss: (0.0086) | Acc: (99.93%) (42340/42368)\n",
            "Epoch: 110 | Batch_idx: 340 |  Loss: (0.0086) | Acc: (99.93%) (43619/43648)\n",
            "Epoch: 110 | Batch_idx: 350 |  Loss: (0.0086) | Acc: (99.93%) (44897/44928)\n",
            "Epoch: 110 | Batch_idx: 360 |  Loss: (0.0086) | Acc: (99.93%) (46175/46208)\n",
            "Epoch: 110 | Batch_idx: 370 |  Loss: (0.0086) | Acc: (99.93%) (47454/47488)\n",
            "Epoch: 110 | Batch_idx: 380 |  Loss: (0.0086) | Acc: (99.93%) (48733/48768)\n",
            "Epoch: 110 | Batch_idx: 390 |  Loss: (0.0087) | Acc: (99.93%) (49965/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2612) | Acc: (92.91%) (9291/10000)\n",
            "Epoch: 111 | Batch_idx: 0 |  Loss: (0.0039) | Acc: (100.00%) (128/128)\n",
            "Epoch: 111 | Batch_idx: 10 |  Loss: (0.0057) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 111 | Batch_idx: 20 |  Loss: (0.0064) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 111 | Batch_idx: 30 |  Loss: (0.0071) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 111 | Batch_idx: 40 |  Loss: (0.0072) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 111 | Batch_idx: 50 |  Loss: (0.0071) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 111 | Batch_idx: 60 |  Loss: (0.0071) | Acc: (99.97%) (7806/7808)\n",
            "Epoch: 111 | Batch_idx: 70 |  Loss: (0.0071) | Acc: (99.98%) (9086/9088)\n",
            "Epoch: 111 | Batch_idx: 80 |  Loss: (0.0073) | Acc: (99.97%) (10365/10368)\n",
            "Epoch: 111 | Batch_idx: 90 |  Loss: (0.0076) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 111 | Batch_idx: 100 |  Loss: (0.0078) | Acc: (99.97%) (12924/12928)\n",
            "Epoch: 111 | Batch_idx: 110 |  Loss: (0.0077) | Acc: (99.97%) (14204/14208)\n",
            "Epoch: 111 | Batch_idx: 120 |  Loss: (0.0080) | Acc: (99.97%) (15483/15488)\n",
            "Epoch: 111 | Batch_idx: 130 |  Loss: (0.0079) | Acc: (99.96%) (16762/16768)\n",
            "Epoch: 111 | Batch_idx: 140 |  Loss: (0.0079) | Acc: (99.96%) (18041/18048)\n",
            "Epoch: 111 | Batch_idx: 150 |  Loss: (0.0078) | Acc: (99.96%) (19320/19328)\n",
            "Epoch: 111 | Batch_idx: 160 |  Loss: (0.0079) | Acc: (99.96%) (20599/20608)\n",
            "Epoch: 111 | Batch_idx: 170 |  Loss: (0.0079) | Acc: (99.96%) (21879/21888)\n",
            "Epoch: 111 | Batch_idx: 180 |  Loss: (0.0079) | Acc: (99.96%) (23158/23168)\n",
            "Epoch: 111 | Batch_idx: 190 |  Loss: (0.0079) | Acc: (99.96%) (24437/24448)\n",
            "Epoch: 111 | Batch_idx: 200 |  Loss: (0.0080) | Acc: (99.96%) (25717/25728)\n",
            "Epoch: 111 | Batch_idx: 210 |  Loss: (0.0079) | Acc: (99.96%) (26997/27008)\n",
            "Epoch: 111 | Batch_idx: 220 |  Loss: (0.0080) | Acc: (99.95%) (28275/28288)\n",
            "Epoch: 111 | Batch_idx: 230 |  Loss: (0.0080) | Acc: (99.96%) (29555/29568)\n",
            "Epoch: 111 | Batch_idx: 240 |  Loss: (0.0079) | Acc: (99.96%) (30835/30848)\n",
            "Epoch: 111 | Batch_idx: 250 |  Loss: (0.0079) | Acc: (99.96%) (32114/32128)\n",
            "Epoch: 111 | Batch_idx: 260 |  Loss: (0.0079) | Acc: (99.96%) (33394/33408)\n",
            "Epoch: 111 | Batch_idx: 270 |  Loss: (0.0079) | Acc: (99.96%) (34673/34688)\n",
            "Epoch: 111 | Batch_idx: 280 |  Loss: (0.0079) | Acc: (99.96%) (35952/35968)\n",
            "Epoch: 111 | Batch_idx: 290 |  Loss: (0.0080) | Acc: (99.95%) (37231/37248)\n",
            "Epoch: 111 | Batch_idx: 300 |  Loss: (0.0080) | Acc: (99.95%) (38510/38528)\n",
            "Epoch: 111 | Batch_idx: 310 |  Loss: (0.0080) | Acc: (99.95%) (39789/39808)\n",
            "Epoch: 111 | Batch_idx: 320 |  Loss: (0.0080) | Acc: (99.95%) (41068/41088)\n",
            "Epoch: 111 | Batch_idx: 330 |  Loss: (0.0079) | Acc: (99.95%) (42347/42368)\n",
            "Epoch: 111 | Batch_idx: 340 |  Loss: (0.0079) | Acc: (99.95%) (43624/43648)\n",
            "Epoch: 111 | Batch_idx: 350 |  Loss: (0.0079) | Acc: (99.95%) (44904/44928)\n",
            "Epoch: 111 | Batch_idx: 360 |  Loss: (0.0080) | Acc: (99.94%) (46182/46208)\n",
            "Epoch: 111 | Batch_idx: 370 |  Loss: (0.0080) | Acc: (99.94%) (47461/47488)\n",
            "Epoch: 111 | Batch_idx: 380 |  Loss: (0.0081) | Acc: (99.94%) (48740/48768)\n",
            "Epoch: 111 | Batch_idx: 390 |  Loss: (0.0081) | Acc: (99.94%) (49972/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2600) | Acc: (93.02%) (9302/10000)\n",
            "Epoch: 112 | Batch_idx: 0 |  Loss: (0.0041) | Acc: (100.00%) (128/128)\n",
            "Epoch: 112 | Batch_idx: 10 |  Loss: (0.0084) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 112 | Batch_idx: 20 |  Loss: (0.0086) | Acc: (99.85%) (2684/2688)\n",
            "Epoch: 112 | Batch_idx: 30 |  Loss: (0.0090) | Acc: (99.82%) (3961/3968)\n",
            "Epoch: 112 | Batch_idx: 40 |  Loss: (0.0089) | Acc: (99.83%) (5239/5248)\n",
            "Epoch: 112 | Batch_idx: 50 |  Loss: (0.0091) | Acc: (99.83%) (6517/6528)\n",
            "Epoch: 112 | Batch_idx: 60 |  Loss: (0.0087) | Acc: (99.86%) (7797/7808)\n",
            "Epoch: 112 | Batch_idx: 70 |  Loss: (0.0088) | Acc: (99.85%) (9074/9088)\n",
            "Epoch: 112 | Batch_idx: 80 |  Loss: (0.0090) | Acc: (99.85%) (10352/10368)\n",
            "Epoch: 112 | Batch_idx: 90 |  Loss: (0.0091) | Acc: (99.83%) (11628/11648)\n",
            "Epoch: 112 | Batch_idx: 100 |  Loss: (0.0089) | Acc: (99.85%) (12908/12928)\n",
            "Epoch: 112 | Batch_idx: 110 |  Loss: (0.0090) | Acc: (99.85%) (14186/14208)\n",
            "Epoch: 112 | Batch_idx: 120 |  Loss: (0.0088) | Acc: (99.86%) (15466/15488)\n",
            "Epoch: 112 | Batch_idx: 130 |  Loss: (0.0087) | Acc: (99.87%) (16746/16768)\n",
            "Epoch: 112 | Batch_idx: 140 |  Loss: (0.0086) | Acc: (99.87%) (18024/18048)\n",
            "Epoch: 112 | Batch_idx: 150 |  Loss: (0.0087) | Acc: (99.87%) (19302/19328)\n",
            "Epoch: 112 | Batch_idx: 160 |  Loss: (0.0086) | Acc: (99.87%) (20582/20608)\n",
            "Epoch: 112 | Batch_idx: 170 |  Loss: (0.0087) | Acc: (99.87%) (21860/21888)\n",
            "Epoch: 112 | Batch_idx: 180 |  Loss: (0.0086) | Acc: (99.88%) (23140/23168)\n",
            "Epoch: 112 | Batch_idx: 190 |  Loss: (0.0085) | Acc: (99.88%) (24418/24448)\n",
            "Epoch: 112 | Batch_idx: 200 |  Loss: (0.0085) | Acc: (99.88%) (25696/25728)\n",
            "Epoch: 112 | Batch_idx: 210 |  Loss: (0.0085) | Acc: (99.88%) (26975/27008)\n",
            "Epoch: 112 | Batch_idx: 220 |  Loss: (0.0085) | Acc: (99.88%) (28253/28288)\n",
            "Epoch: 112 | Batch_idx: 230 |  Loss: (0.0084) | Acc: (99.88%) (29533/29568)\n",
            "Epoch: 112 | Batch_idx: 240 |  Loss: (0.0084) | Acc: (99.89%) (30813/30848)\n",
            "Epoch: 112 | Batch_idx: 250 |  Loss: (0.0083) | Acc: (99.89%) (32093/32128)\n",
            "Epoch: 112 | Batch_idx: 260 |  Loss: (0.0083) | Acc: (99.89%) (33371/33408)\n",
            "Epoch: 112 | Batch_idx: 270 |  Loss: (0.0083) | Acc: (99.89%) (34650/34688)\n",
            "Epoch: 112 | Batch_idx: 280 |  Loss: (0.0085) | Acc: (99.88%) (35925/35968)\n",
            "Epoch: 112 | Batch_idx: 290 |  Loss: (0.0084) | Acc: (99.88%) (37205/37248)\n",
            "Epoch: 112 | Batch_idx: 300 |  Loss: (0.0084) | Acc: (99.89%) (38484/38528)\n",
            "Epoch: 112 | Batch_idx: 310 |  Loss: (0.0083) | Acc: (99.89%) (39763/39808)\n",
            "Epoch: 112 | Batch_idx: 320 |  Loss: (0.0084) | Acc: (99.89%) (41041/41088)\n",
            "Epoch: 112 | Batch_idx: 330 |  Loss: (0.0083) | Acc: (99.89%) (42320/42368)\n",
            "Epoch: 112 | Batch_idx: 340 |  Loss: (0.0083) | Acc: (99.89%) (43600/43648)\n",
            "Epoch: 112 | Batch_idx: 350 |  Loss: (0.0083) | Acc: (99.89%) (44879/44928)\n",
            "Epoch: 112 | Batch_idx: 360 |  Loss: (0.0083) | Acc: (99.89%) (46158/46208)\n",
            "Epoch: 112 | Batch_idx: 370 |  Loss: (0.0082) | Acc: (99.89%) (47438/47488)\n",
            "Epoch: 112 | Batch_idx: 380 |  Loss: (0.0084) | Acc: (99.89%) (48715/48768)\n",
            "Epoch: 112 | Batch_idx: 390 |  Loss: (0.0084) | Acc: (99.89%) (49946/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2588) | Acc: (92.94%) (9294/10000)\n",
            "Epoch: 113 | Batch_idx: 0 |  Loss: (0.0091) | Acc: (100.00%) (128/128)\n",
            "Epoch: 113 | Batch_idx: 10 |  Loss: (0.0094) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 113 | Batch_idx: 20 |  Loss: (0.0079) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 113 | Batch_idx: 30 |  Loss: (0.0082) | Acc: (99.92%) (3965/3968)\n",
            "Epoch: 113 | Batch_idx: 40 |  Loss: (0.0083) | Acc: (99.90%) (5243/5248)\n",
            "Epoch: 113 | Batch_idx: 50 |  Loss: (0.0082) | Acc: (99.89%) (6521/6528)\n",
            "Epoch: 113 | Batch_idx: 60 |  Loss: (0.0082) | Acc: (99.90%) (7800/7808)\n",
            "Epoch: 113 | Batch_idx: 70 |  Loss: (0.0086) | Acc: (99.89%) (9078/9088)\n",
            "Epoch: 113 | Batch_idx: 80 |  Loss: (0.0084) | Acc: (99.89%) (10357/10368)\n",
            "Epoch: 113 | Batch_idx: 90 |  Loss: (0.0082) | Acc: (99.91%) (11637/11648)\n",
            "Epoch: 113 | Batch_idx: 100 |  Loss: (0.0082) | Acc: (99.91%) (12916/12928)\n",
            "Epoch: 113 | Batch_idx: 110 |  Loss: (0.0084) | Acc: (99.89%) (14193/14208)\n",
            "Epoch: 113 | Batch_idx: 120 |  Loss: (0.0084) | Acc: (99.90%) (15472/15488)\n",
            "Epoch: 113 | Batch_idx: 130 |  Loss: (0.0083) | Acc: (99.90%) (16752/16768)\n",
            "Epoch: 113 | Batch_idx: 140 |  Loss: (0.0083) | Acc: (99.90%) (18030/18048)\n",
            "Epoch: 113 | Batch_idx: 150 |  Loss: (0.0083) | Acc: (99.91%) (19310/19328)\n",
            "Epoch: 113 | Batch_idx: 160 |  Loss: (0.0083) | Acc: (99.91%) (20589/20608)\n",
            "Epoch: 113 | Batch_idx: 170 |  Loss: (0.0082) | Acc: (99.91%) (21868/21888)\n",
            "Epoch: 113 | Batch_idx: 180 |  Loss: (0.0082) | Acc: (99.91%) (23148/23168)\n",
            "Epoch: 113 | Batch_idx: 190 |  Loss: (0.0082) | Acc: (99.92%) (24428/24448)\n",
            "Epoch: 113 | Batch_idx: 200 |  Loss: (0.0082) | Acc: (99.92%) (25708/25728)\n",
            "Epoch: 113 | Batch_idx: 210 |  Loss: (0.0082) | Acc: (99.92%) (26987/27008)\n",
            "Epoch: 113 | Batch_idx: 220 |  Loss: (0.0081) | Acc: (99.93%) (28267/28288)\n",
            "Epoch: 113 | Batch_idx: 230 |  Loss: (0.0082) | Acc: (99.93%) (29546/29568)\n",
            "Epoch: 113 | Batch_idx: 240 |  Loss: (0.0082) | Acc: (99.93%) (30826/30848)\n",
            "Epoch: 113 | Batch_idx: 250 |  Loss: (0.0081) | Acc: (99.93%) (32105/32128)\n",
            "Epoch: 113 | Batch_idx: 260 |  Loss: (0.0081) | Acc: (99.93%) (33385/33408)\n",
            "Epoch: 113 | Batch_idx: 270 |  Loss: (0.0081) | Acc: (99.93%) (34664/34688)\n",
            "Epoch: 113 | Batch_idx: 280 |  Loss: (0.0081) | Acc: (99.93%) (35943/35968)\n",
            "Epoch: 113 | Batch_idx: 290 |  Loss: (0.0081) | Acc: (99.93%) (37222/37248)\n",
            "Epoch: 113 | Batch_idx: 300 |  Loss: (0.0081) | Acc: (99.93%) (38502/38528)\n",
            "Epoch: 113 | Batch_idx: 310 |  Loss: (0.0080) | Acc: (99.93%) (39781/39808)\n",
            "Epoch: 113 | Batch_idx: 320 |  Loss: (0.0080) | Acc: (99.93%) (41060/41088)\n",
            "Epoch: 113 | Batch_idx: 330 |  Loss: (0.0080) | Acc: (99.93%) (42338/42368)\n",
            "Epoch: 113 | Batch_idx: 340 |  Loss: (0.0081) | Acc: (99.93%) (43617/43648)\n",
            "Epoch: 113 | Batch_idx: 350 |  Loss: (0.0081) | Acc: (99.93%) (44896/44928)\n",
            "Epoch: 113 | Batch_idx: 360 |  Loss: (0.0081) | Acc: (99.93%) (46175/46208)\n",
            "Epoch: 113 | Batch_idx: 370 |  Loss: (0.0081) | Acc: (99.93%) (47454/47488)\n",
            "Epoch: 113 | Batch_idx: 380 |  Loss: (0.0082) | Acc: (99.92%) (48730/48768)\n",
            "Epoch: 113 | Batch_idx: 390 |  Loss: (0.0082) | Acc: (99.92%) (49962/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2623) | Acc: (93.07%) (9307/10000)\n",
            "Epoch: 114 | Batch_idx: 0 |  Loss: (0.0040) | Acc: (100.00%) (128/128)\n",
            "Epoch: 114 | Batch_idx: 10 |  Loss: (0.0077) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 114 | Batch_idx: 20 |  Loss: (0.0074) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 114 | Batch_idx: 30 |  Loss: (0.0076) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 114 | Batch_idx: 40 |  Loss: (0.0072) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 114 | Batch_idx: 50 |  Loss: (0.0074) | Acc: (99.97%) (6526/6528)\n",
            "Epoch: 114 | Batch_idx: 60 |  Loss: (0.0077) | Acc: (99.96%) (7805/7808)\n",
            "Epoch: 114 | Batch_idx: 70 |  Loss: (0.0077) | Acc: (99.94%) (9083/9088)\n",
            "Epoch: 114 | Batch_idx: 80 |  Loss: (0.0075) | Acc: (99.95%) (10363/10368)\n",
            "Epoch: 114 | Batch_idx: 90 |  Loss: (0.0076) | Acc: (99.95%) (11642/11648)\n",
            "Epoch: 114 | Batch_idx: 100 |  Loss: (0.0079) | Acc: (99.94%) (12920/12928)\n",
            "Epoch: 114 | Batch_idx: 110 |  Loss: (0.0078) | Acc: (99.94%) (14200/14208)\n",
            "Epoch: 114 | Batch_idx: 120 |  Loss: (0.0077) | Acc: (99.95%) (15480/15488)\n",
            "Epoch: 114 | Batch_idx: 130 |  Loss: (0.0076) | Acc: (99.95%) (16760/16768)\n",
            "Epoch: 114 | Batch_idx: 140 |  Loss: (0.0076) | Acc: (99.96%) (18040/18048)\n",
            "Epoch: 114 | Batch_idx: 150 |  Loss: (0.0076) | Acc: (99.96%) (19320/19328)\n",
            "Epoch: 114 | Batch_idx: 160 |  Loss: (0.0077) | Acc: (99.95%) (20598/20608)\n",
            "Epoch: 114 | Batch_idx: 170 |  Loss: (0.0078) | Acc: (99.95%) (21877/21888)\n",
            "Epoch: 114 | Batch_idx: 180 |  Loss: (0.0078) | Acc: (99.95%) (23157/23168)\n",
            "Epoch: 114 | Batch_idx: 190 |  Loss: (0.0079) | Acc: (99.95%) (24435/24448)\n",
            "Epoch: 114 | Batch_idx: 200 |  Loss: (0.0078) | Acc: (99.95%) (25714/25728)\n",
            "Epoch: 114 | Batch_idx: 210 |  Loss: (0.0079) | Acc: (99.94%) (26993/27008)\n",
            "Epoch: 114 | Batch_idx: 220 |  Loss: (0.0078) | Acc: (99.95%) (28273/28288)\n",
            "Epoch: 114 | Batch_idx: 230 |  Loss: (0.0079) | Acc: (99.95%) (29553/29568)\n",
            "Epoch: 114 | Batch_idx: 240 |  Loss: (0.0078) | Acc: (99.95%) (30832/30848)\n",
            "Epoch: 114 | Batch_idx: 250 |  Loss: (0.0078) | Acc: (99.95%) (32111/32128)\n",
            "Epoch: 114 | Batch_idx: 260 |  Loss: (0.0079) | Acc: (99.95%) (33390/33408)\n",
            "Epoch: 114 | Batch_idx: 270 |  Loss: (0.0079) | Acc: (99.95%) (34669/34688)\n",
            "Epoch: 114 | Batch_idx: 280 |  Loss: (0.0079) | Acc: (99.95%) (35949/35968)\n",
            "Epoch: 114 | Batch_idx: 290 |  Loss: (0.0079) | Acc: (99.95%) (37228/37248)\n",
            "Epoch: 114 | Batch_idx: 300 |  Loss: (0.0079) | Acc: (99.95%) (38508/38528)\n",
            "Epoch: 114 | Batch_idx: 310 |  Loss: (0.0079) | Acc: (99.95%) (39787/39808)\n",
            "Epoch: 114 | Batch_idx: 320 |  Loss: (0.0079) | Acc: (99.95%) (41067/41088)\n",
            "Epoch: 114 | Batch_idx: 330 |  Loss: (0.0080) | Acc: (99.95%) (42345/42368)\n",
            "Epoch: 114 | Batch_idx: 340 |  Loss: (0.0080) | Acc: (99.95%) (43625/43648)\n",
            "Epoch: 114 | Batch_idx: 350 |  Loss: (0.0080) | Acc: (99.95%) (44904/44928)\n",
            "Epoch: 114 | Batch_idx: 360 |  Loss: (0.0079) | Acc: (99.95%) (46184/46208)\n",
            "Epoch: 114 | Batch_idx: 370 |  Loss: (0.0079) | Acc: (99.95%) (47464/47488)\n",
            "Epoch: 114 | Batch_idx: 380 |  Loss: (0.0079) | Acc: (99.95%) (48744/48768)\n",
            "Epoch: 114 | Batch_idx: 390 |  Loss: (0.0079) | Acc: (99.95%) (49976/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2600) | Acc: (92.93%) (9293/10000)\n",
            "Epoch: 115 | Batch_idx: 0 |  Loss: (0.0087) | Acc: (100.00%) (128/128)\n",
            "Epoch: 115 | Batch_idx: 10 |  Loss: (0.0073) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 115 | Batch_idx: 20 |  Loss: (0.0071) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 115 | Batch_idx: 30 |  Loss: (0.0072) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 115 | Batch_idx: 40 |  Loss: (0.0072) | Acc: (99.94%) (5245/5248)\n",
            "Epoch: 115 | Batch_idx: 50 |  Loss: (0.0071) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 115 | Batch_idx: 60 |  Loss: (0.0068) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 115 | Batch_idx: 70 |  Loss: (0.0067) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 115 | Batch_idx: 80 |  Loss: (0.0067) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 115 | Batch_idx: 90 |  Loss: (0.0067) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 115 | Batch_idx: 100 |  Loss: (0.0068) | Acc: (99.96%) (12923/12928)\n",
            "Epoch: 115 | Batch_idx: 110 |  Loss: (0.0070) | Acc: (99.96%) (14202/14208)\n",
            "Epoch: 115 | Batch_idx: 120 |  Loss: (0.0070) | Acc: (99.96%) (15482/15488)\n",
            "Epoch: 115 | Batch_idx: 130 |  Loss: (0.0072) | Acc: (99.96%) (16761/16768)\n",
            "Epoch: 115 | Batch_idx: 140 |  Loss: (0.0074) | Acc: (99.95%) (18039/18048)\n",
            "Epoch: 115 | Batch_idx: 150 |  Loss: (0.0074) | Acc: (99.95%) (19319/19328)\n",
            "Epoch: 115 | Batch_idx: 160 |  Loss: (0.0073) | Acc: (99.96%) (20599/20608)\n",
            "Epoch: 115 | Batch_idx: 170 |  Loss: (0.0073) | Acc: (99.95%) (21878/21888)\n",
            "Epoch: 115 | Batch_idx: 180 |  Loss: (0.0073) | Acc: (99.95%) (23157/23168)\n",
            "Epoch: 115 | Batch_idx: 190 |  Loss: (0.0074) | Acc: (99.94%) (24434/24448)\n",
            "Epoch: 115 | Batch_idx: 200 |  Loss: (0.0074) | Acc: (99.95%) (25714/25728)\n",
            "Epoch: 115 | Batch_idx: 210 |  Loss: (0.0074) | Acc: (99.95%) (26994/27008)\n",
            "Epoch: 115 | Batch_idx: 220 |  Loss: (0.0075) | Acc: (99.95%) (28273/28288)\n",
            "Epoch: 115 | Batch_idx: 230 |  Loss: (0.0075) | Acc: (99.95%) (29552/29568)\n",
            "Epoch: 115 | Batch_idx: 240 |  Loss: (0.0075) | Acc: (99.95%) (30832/30848)\n",
            "Epoch: 115 | Batch_idx: 250 |  Loss: (0.0076) | Acc: (99.95%) (32112/32128)\n",
            "Epoch: 115 | Batch_idx: 260 |  Loss: (0.0075) | Acc: (99.95%) (33392/33408)\n",
            "Epoch: 115 | Batch_idx: 270 |  Loss: (0.0075) | Acc: (99.95%) (34671/34688)\n",
            "Epoch: 115 | Batch_idx: 280 |  Loss: (0.0075) | Acc: (99.95%) (35951/35968)\n",
            "Epoch: 115 | Batch_idx: 290 |  Loss: (0.0076) | Acc: (99.95%) (37231/37248)\n",
            "Epoch: 115 | Batch_idx: 300 |  Loss: (0.0075) | Acc: (99.96%) (38511/38528)\n",
            "Epoch: 115 | Batch_idx: 310 |  Loss: (0.0076) | Acc: (99.95%) (39790/39808)\n",
            "Epoch: 115 | Batch_idx: 320 |  Loss: (0.0076) | Acc: (99.96%) (41070/41088)\n",
            "Epoch: 115 | Batch_idx: 330 |  Loss: (0.0076) | Acc: (99.95%) (42348/42368)\n",
            "Epoch: 115 | Batch_idx: 340 |  Loss: (0.0077) | Acc: (99.95%) (43627/43648)\n",
            "Epoch: 115 | Batch_idx: 350 |  Loss: (0.0076) | Acc: (99.95%) (44907/44928)\n",
            "Epoch: 115 | Batch_idx: 360 |  Loss: (0.0076) | Acc: (99.95%) (46186/46208)\n",
            "Epoch: 115 | Batch_idx: 370 |  Loss: (0.0076) | Acc: (99.95%) (47466/47488)\n",
            "Epoch: 115 | Batch_idx: 380 |  Loss: (0.0076) | Acc: (99.95%) (48745/48768)\n",
            "Epoch: 115 | Batch_idx: 390 |  Loss: (0.0076) | Acc: (99.95%) (49974/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2592) | Acc: (92.83%) (9283/10000)\n",
            "Epoch: 116 | Batch_idx: 0 |  Loss: (0.0070) | Acc: (100.00%) (128/128)\n",
            "Epoch: 116 | Batch_idx: 10 |  Loss: (0.0084) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 116 | Batch_idx: 20 |  Loss: (0.0075) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 116 | Batch_idx: 30 |  Loss: (0.0074) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 116 | Batch_idx: 40 |  Loss: (0.0073) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 116 | Batch_idx: 50 |  Loss: (0.0072) | Acc: (99.97%) (6526/6528)\n",
            "Epoch: 116 | Batch_idx: 60 |  Loss: (0.0071) | Acc: (99.97%) (7806/7808)\n",
            "Epoch: 116 | Batch_idx: 70 |  Loss: (0.0070) | Acc: (99.98%) (9086/9088)\n",
            "Epoch: 116 | Batch_idx: 80 |  Loss: (0.0070) | Acc: (99.98%) (10366/10368)\n",
            "Epoch: 116 | Batch_idx: 90 |  Loss: (0.0068) | Acc: (99.98%) (11646/11648)\n",
            "Epoch: 116 | Batch_idx: 100 |  Loss: (0.0072) | Acc: (99.97%) (12924/12928)\n",
            "Epoch: 116 | Batch_idx: 110 |  Loss: (0.0072) | Acc: (99.97%) (14204/14208)\n",
            "Epoch: 116 | Batch_idx: 120 |  Loss: (0.0073) | Acc: (99.97%) (15484/15488)\n",
            "Epoch: 116 | Batch_idx: 130 |  Loss: (0.0071) | Acc: (99.98%) (16764/16768)\n",
            "Epoch: 116 | Batch_idx: 140 |  Loss: (0.0071) | Acc: (99.97%) (18043/18048)\n",
            "Epoch: 116 | Batch_idx: 150 |  Loss: (0.0070) | Acc: (99.97%) (19323/19328)\n",
            "Epoch: 116 | Batch_idx: 160 |  Loss: (0.0071) | Acc: (99.97%) (20602/20608)\n",
            "Epoch: 116 | Batch_idx: 170 |  Loss: (0.0072) | Acc: (99.96%) (21880/21888)\n",
            "Epoch: 116 | Batch_idx: 180 |  Loss: (0.0073) | Acc: (99.97%) (23160/23168)\n",
            "Epoch: 116 | Batch_idx: 190 |  Loss: (0.0073) | Acc: (99.96%) (24439/24448)\n",
            "Epoch: 116 | Batch_idx: 200 |  Loss: (0.0074) | Acc: (99.96%) (25718/25728)\n",
            "Epoch: 116 | Batch_idx: 210 |  Loss: (0.0074) | Acc: (99.96%) (26998/27008)\n",
            "Epoch: 116 | Batch_idx: 220 |  Loss: (0.0075) | Acc: (99.96%) (28277/28288)\n",
            "Epoch: 116 | Batch_idx: 230 |  Loss: (0.0075) | Acc: (99.96%) (29556/29568)\n",
            "Epoch: 116 | Batch_idx: 240 |  Loss: (0.0075) | Acc: (99.96%) (30835/30848)\n",
            "Epoch: 116 | Batch_idx: 250 |  Loss: (0.0075) | Acc: (99.96%) (32115/32128)\n",
            "Epoch: 116 | Batch_idx: 260 |  Loss: (0.0075) | Acc: (99.96%) (33395/33408)\n",
            "Epoch: 116 | Batch_idx: 270 |  Loss: (0.0075) | Acc: (99.96%) (34674/34688)\n",
            "Epoch: 116 | Batch_idx: 280 |  Loss: (0.0075) | Acc: (99.96%) (35954/35968)\n",
            "Epoch: 116 | Batch_idx: 290 |  Loss: (0.0076) | Acc: (99.96%) (37232/37248)\n",
            "Epoch: 116 | Batch_idx: 300 |  Loss: (0.0076) | Acc: (99.96%) (38512/38528)\n",
            "Epoch: 116 | Batch_idx: 310 |  Loss: (0.0075) | Acc: (99.96%) (39791/39808)\n",
            "Epoch: 116 | Batch_idx: 320 |  Loss: (0.0075) | Acc: (99.96%) (41071/41088)\n",
            "Epoch: 116 | Batch_idx: 330 |  Loss: (0.0075) | Acc: (99.96%) (42351/42368)\n",
            "Epoch: 116 | Batch_idx: 340 |  Loss: (0.0076) | Acc: (99.96%) (43630/43648)\n",
            "Epoch: 116 | Batch_idx: 350 |  Loss: (0.0076) | Acc: (99.96%) (44910/44928)\n",
            "Epoch: 116 | Batch_idx: 360 |  Loss: (0.0076) | Acc: (99.96%) (46189/46208)\n",
            "Epoch: 116 | Batch_idx: 370 |  Loss: (0.0076) | Acc: (99.96%) (47469/47488)\n",
            "Epoch: 116 | Batch_idx: 380 |  Loss: (0.0076) | Acc: (99.96%) (48749/48768)\n",
            "Epoch: 116 | Batch_idx: 390 |  Loss: (0.0076) | Acc: (99.96%) (49979/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2599) | Acc: (92.96%) (9296/10000)\n",
            "Epoch: 117 | Batch_idx: 0 |  Loss: (0.0054) | Acc: (100.00%) (128/128)\n",
            "Epoch: 117 | Batch_idx: 10 |  Loss: (0.0074) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 117 | Batch_idx: 20 |  Loss: (0.0079) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 117 | Batch_idx: 30 |  Loss: (0.0080) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 117 | Batch_idx: 40 |  Loss: (0.0073) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 117 | Batch_idx: 50 |  Loss: (0.0075) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 117 | Batch_idx: 60 |  Loss: (0.0075) | Acc: (99.94%) (7803/7808)\n",
            "Epoch: 117 | Batch_idx: 70 |  Loss: (0.0078) | Acc: (99.93%) (9082/9088)\n",
            "Epoch: 117 | Batch_idx: 80 |  Loss: (0.0077) | Acc: (99.93%) (10361/10368)\n",
            "Epoch: 117 | Batch_idx: 90 |  Loss: (0.0077) | Acc: (99.93%) (11640/11648)\n",
            "Epoch: 117 | Batch_idx: 100 |  Loss: (0.0075) | Acc: (99.94%) (12920/12928)\n",
            "Epoch: 117 | Batch_idx: 110 |  Loss: (0.0076) | Acc: (99.93%) (14198/14208)\n",
            "Epoch: 117 | Batch_idx: 120 |  Loss: (0.0076) | Acc: (99.93%) (15477/15488)\n",
            "Epoch: 117 | Batch_idx: 130 |  Loss: (0.0077) | Acc: (99.92%) (16755/16768)\n",
            "Epoch: 117 | Batch_idx: 140 |  Loss: (0.0078) | Acc: (99.92%) (18033/18048)\n",
            "Epoch: 117 | Batch_idx: 150 |  Loss: (0.0078) | Acc: (99.92%) (19313/19328)\n",
            "Epoch: 117 | Batch_idx: 160 |  Loss: (0.0078) | Acc: (99.92%) (20592/20608)\n",
            "Epoch: 117 | Batch_idx: 170 |  Loss: (0.0079) | Acc: (99.92%) (21870/21888)\n",
            "Epoch: 117 | Batch_idx: 180 |  Loss: (0.0079) | Acc: (99.92%) (23149/23168)\n",
            "Epoch: 117 | Batch_idx: 190 |  Loss: (0.0078) | Acc: (99.92%) (24429/24448)\n",
            "Epoch: 117 | Batch_idx: 200 |  Loss: (0.0078) | Acc: (99.93%) (25709/25728)\n",
            "Epoch: 117 | Batch_idx: 210 |  Loss: (0.0078) | Acc: (99.93%) (26988/27008)\n",
            "Epoch: 117 | Batch_idx: 220 |  Loss: (0.0077) | Acc: (99.93%) (28268/28288)\n",
            "Epoch: 117 | Batch_idx: 230 |  Loss: (0.0078) | Acc: (99.93%) (29546/29568)\n",
            "Epoch: 117 | Batch_idx: 240 |  Loss: (0.0078) | Acc: (99.93%) (30825/30848)\n",
            "Epoch: 117 | Batch_idx: 250 |  Loss: (0.0078) | Acc: (99.93%) (32105/32128)\n",
            "Epoch: 117 | Batch_idx: 260 |  Loss: (0.0078) | Acc: (99.93%) (33385/33408)\n",
            "Epoch: 117 | Batch_idx: 270 |  Loss: (0.0077) | Acc: (99.93%) (34665/34688)\n",
            "Epoch: 117 | Batch_idx: 280 |  Loss: (0.0077) | Acc: (99.94%) (35945/35968)\n",
            "Epoch: 117 | Batch_idx: 290 |  Loss: (0.0077) | Acc: (99.94%) (37224/37248)\n",
            "Epoch: 117 | Batch_idx: 300 |  Loss: (0.0077) | Acc: (99.94%) (38504/38528)\n",
            "Epoch: 117 | Batch_idx: 310 |  Loss: (0.0077) | Acc: (99.94%) (39784/39808)\n",
            "Epoch: 117 | Batch_idx: 320 |  Loss: (0.0077) | Acc: (99.94%) (41063/41088)\n",
            "Epoch: 117 | Batch_idx: 330 |  Loss: (0.0077) | Acc: (99.94%) (42343/42368)\n",
            "Epoch: 117 | Batch_idx: 340 |  Loss: (0.0077) | Acc: (99.94%) (43621/43648)\n",
            "Epoch: 117 | Batch_idx: 350 |  Loss: (0.0078) | Acc: (99.94%) (44900/44928)\n",
            "Epoch: 117 | Batch_idx: 360 |  Loss: (0.0078) | Acc: (99.94%) (46179/46208)\n",
            "Epoch: 117 | Batch_idx: 370 |  Loss: (0.0078) | Acc: (99.94%) (47458/47488)\n",
            "Epoch: 117 | Batch_idx: 380 |  Loss: (0.0078) | Acc: (99.93%) (48736/48768)\n",
            "Epoch: 117 | Batch_idx: 390 |  Loss: (0.0078) | Acc: (99.93%) (49967/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2616) | Acc: (92.83%) (9283/10000)\n",
            "Epoch: 118 | Batch_idx: 0 |  Loss: (0.0069) | Acc: (100.00%) (128/128)\n",
            "Epoch: 118 | Batch_idx: 10 |  Loss: (0.0086) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 118 | Batch_idx: 20 |  Loss: (0.0091) | Acc: (99.89%) (2685/2688)\n",
            "Epoch: 118 | Batch_idx: 30 |  Loss: (0.0084) | Acc: (99.92%) (3965/3968)\n",
            "Epoch: 118 | Batch_idx: 40 |  Loss: (0.0087) | Acc: (99.92%) (5244/5248)\n",
            "Epoch: 118 | Batch_idx: 50 |  Loss: (0.0085) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 118 | Batch_idx: 60 |  Loss: (0.0083) | Acc: (99.94%) (7803/7808)\n",
            "Epoch: 118 | Batch_idx: 70 |  Loss: (0.0083) | Acc: (99.93%) (9082/9088)\n",
            "Epoch: 118 | Batch_idx: 80 |  Loss: (0.0080) | Acc: (99.94%) (10362/10368)\n",
            "Epoch: 118 | Batch_idx: 90 |  Loss: (0.0079) | Acc: (99.94%) (11641/11648)\n",
            "Epoch: 118 | Batch_idx: 100 |  Loss: (0.0078) | Acc: (99.95%) (12921/12928)\n",
            "Epoch: 118 | Batch_idx: 110 |  Loss: (0.0077) | Acc: (99.95%) (14201/14208)\n",
            "Epoch: 118 | Batch_idx: 120 |  Loss: (0.0077) | Acc: (99.95%) (15480/15488)\n",
            "Epoch: 118 | Batch_idx: 130 |  Loss: (0.0079) | Acc: (99.93%) (16757/16768)\n",
            "Epoch: 118 | Batch_idx: 140 |  Loss: (0.0078) | Acc: (99.93%) (18036/18048)\n",
            "Epoch: 118 | Batch_idx: 150 |  Loss: (0.0077) | Acc: (99.94%) (19316/19328)\n",
            "Epoch: 118 | Batch_idx: 160 |  Loss: (0.0077) | Acc: (99.94%) (20596/20608)\n",
            "Epoch: 118 | Batch_idx: 170 |  Loss: (0.0077) | Acc: (99.94%) (21875/21888)\n",
            "Epoch: 118 | Batch_idx: 180 |  Loss: (0.0077) | Acc: (99.94%) (23154/23168)\n",
            "Epoch: 118 | Batch_idx: 190 |  Loss: (0.0076) | Acc: (99.94%) (24434/24448)\n",
            "Epoch: 118 | Batch_idx: 200 |  Loss: (0.0077) | Acc: (99.94%) (25712/25728)\n",
            "Epoch: 118 | Batch_idx: 210 |  Loss: (0.0077) | Acc: (99.94%) (26991/27008)\n",
            "Epoch: 118 | Batch_idx: 220 |  Loss: (0.0077) | Acc: (99.94%) (28270/28288)\n",
            "Epoch: 118 | Batch_idx: 230 |  Loss: (0.0078) | Acc: (99.93%) (29547/29568)\n",
            "Epoch: 118 | Batch_idx: 240 |  Loss: (0.0077) | Acc: (99.93%) (30827/30848)\n",
            "Epoch: 118 | Batch_idx: 250 |  Loss: (0.0077) | Acc: (99.93%) (32107/32128)\n",
            "Epoch: 118 | Batch_idx: 260 |  Loss: (0.0076) | Acc: (99.93%) (33385/33408)\n",
            "Epoch: 118 | Batch_idx: 270 |  Loss: (0.0076) | Acc: (99.93%) (34663/34688)\n",
            "Epoch: 118 | Batch_idx: 280 |  Loss: (0.0076) | Acc: (99.92%) (35941/35968)\n",
            "Epoch: 118 | Batch_idx: 290 |  Loss: (0.0077) | Acc: (99.92%) (37220/37248)\n",
            "Epoch: 118 | Batch_idx: 300 |  Loss: (0.0077) | Acc: (99.92%) (38498/38528)\n",
            "Epoch: 118 | Batch_idx: 310 |  Loss: (0.0077) | Acc: (99.92%) (39778/39808)\n",
            "Epoch: 118 | Batch_idx: 320 |  Loss: (0.0077) | Acc: (99.93%) (41058/41088)\n",
            "Epoch: 118 | Batch_idx: 330 |  Loss: (0.0077) | Acc: (99.93%) (42338/42368)\n",
            "Epoch: 118 | Batch_idx: 340 |  Loss: (0.0077) | Acc: (99.93%) (43618/43648)\n",
            "Epoch: 118 | Batch_idx: 350 |  Loss: (0.0077) | Acc: (99.93%) (44897/44928)\n",
            "Epoch: 118 | Batch_idx: 360 |  Loss: (0.0077) | Acc: (99.93%) (46177/46208)\n",
            "Epoch: 118 | Batch_idx: 370 |  Loss: (0.0078) | Acc: (99.93%) (47456/47488)\n",
            "Epoch: 118 | Batch_idx: 380 |  Loss: (0.0077) | Acc: (99.93%) (48736/48768)\n",
            "Epoch: 118 | Batch_idx: 390 |  Loss: (0.0078) | Acc: (99.94%) (49968/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2598) | Acc: (92.84%) (9284/10000)\n",
            "Epoch: 119 | Batch_idx: 0 |  Loss: (0.0084) | Acc: (100.00%) (128/128)\n",
            "Epoch: 119 | Batch_idx: 10 |  Loss: (0.0077) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 119 | Batch_idx: 20 |  Loss: (0.0081) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 119 | Batch_idx: 30 |  Loss: (0.0077) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 119 | Batch_idx: 40 |  Loss: (0.0080) | Acc: (99.94%) (5245/5248)\n",
            "Epoch: 119 | Batch_idx: 50 |  Loss: (0.0078) | Acc: (99.95%) (6525/6528)\n",
            "Epoch: 119 | Batch_idx: 60 |  Loss: (0.0077) | Acc: (99.96%) (7805/7808)\n",
            "Epoch: 119 | Batch_idx: 70 |  Loss: (0.0076) | Acc: (99.97%) (9085/9088)\n",
            "Epoch: 119 | Batch_idx: 80 |  Loss: (0.0076) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 119 | Batch_idx: 90 |  Loss: (0.0076) | Acc: (99.96%) (11643/11648)\n",
            "Epoch: 119 | Batch_idx: 100 |  Loss: (0.0075) | Acc: (99.95%) (12922/12928)\n",
            "Epoch: 119 | Batch_idx: 110 |  Loss: (0.0076) | Acc: (99.94%) (14200/14208)\n",
            "Epoch: 119 | Batch_idx: 120 |  Loss: (0.0076) | Acc: (99.95%) (15480/15488)\n",
            "Epoch: 119 | Batch_idx: 130 |  Loss: (0.0075) | Acc: (99.95%) (16760/16768)\n",
            "Epoch: 119 | Batch_idx: 140 |  Loss: (0.0074) | Acc: (99.96%) (18040/18048)\n",
            "Epoch: 119 | Batch_idx: 150 |  Loss: (0.0074) | Acc: (99.96%) (19320/19328)\n",
            "Epoch: 119 | Batch_idx: 160 |  Loss: (0.0074) | Acc: (99.96%) (20599/20608)\n",
            "Epoch: 119 | Batch_idx: 170 |  Loss: (0.0074) | Acc: (99.95%) (21878/21888)\n",
            "Epoch: 119 | Batch_idx: 180 |  Loss: (0.0074) | Acc: (99.95%) (23157/23168)\n",
            "Epoch: 119 | Batch_idx: 190 |  Loss: (0.0074) | Acc: (99.95%) (24436/24448)\n",
            "Epoch: 119 | Batch_idx: 200 |  Loss: (0.0075) | Acc: (99.95%) (25714/25728)\n",
            "Epoch: 119 | Batch_idx: 210 |  Loss: (0.0074) | Acc: (99.94%) (26993/27008)\n",
            "Epoch: 119 | Batch_idx: 220 |  Loss: (0.0075) | Acc: (99.94%) (28272/28288)\n",
            "Epoch: 119 | Batch_idx: 230 |  Loss: (0.0075) | Acc: (99.94%) (29551/29568)\n",
            "Epoch: 119 | Batch_idx: 240 |  Loss: (0.0076) | Acc: (99.94%) (30830/30848)\n",
            "Epoch: 119 | Batch_idx: 250 |  Loss: (0.0075) | Acc: (99.94%) (32110/32128)\n",
            "Epoch: 119 | Batch_idx: 260 |  Loss: (0.0076) | Acc: (99.94%) (33389/33408)\n",
            "Epoch: 119 | Batch_idx: 270 |  Loss: (0.0076) | Acc: (99.95%) (34669/34688)\n",
            "Epoch: 119 | Batch_idx: 280 |  Loss: (0.0075) | Acc: (99.95%) (35949/35968)\n",
            "Epoch: 119 | Batch_idx: 290 |  Loss: (0.0075) | Acc: (99.95%) (37229/37248)\n",
            "Epoch: 119 | Batch_idx: 300 |  Loss: (0.0076) | Acc: (99.95%) (38507/38528)\n",
            "Epoch: 119 | Batch_idx: 310 |  Loss: (0.0075) | Acc: (99.95%) (39787/39808)\n",
            "Epoch: 119 | Batch_idx: 320 |  Loss: (0.0076) | Acc: (99.94%) (41065/41088)\n",
            "Epoch: 119 | Batch_idx: 330 |  Loss: (0.0076) | Acc: (99.94%) (42344/42368)\n",
            "Epoch: 119 | Batch_idx: 340 |  Loss: (0.0076) | Acc: (99.95%) (43624/43648)\n",
            "Epoch: 119 | Batch_idx: 350 |  Loss: (0.0076) | Acc: (99.95%) (44904/44928)\n",
            "Epoch: 119 | Batch_idx: 360 |  Loss: (0.0075) | Acc: (99.95%) (46184/46208)\n",
            "Epoch: 119 | Batch_idx: 370 |  Loss: (0.0075) | Acc: (99.95%) (47464/47488)\n",
            "Epoch: 119 | Batch_idx: 380 |  Loss: (0.0075) | Acc: (99.95%) (48744/48768)\n",
            "Epoch: 119 | Batch_idx: 390 |  Loss: (0.0075) | Acc: (99.95%) (49975/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2603) | Acc: (92.97%) (9297/10000)\n",
            "Epoch: 120 | Batch_idx: 0 |  Loss: (0.0070) | Acc: (100.00%) (128/128)\n",
            "Epoch: 120 | Batch_idx: 10 |  Loss: (0.0068) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 120 | Batch_idx: 20 |  Loss: (0.0065) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 120 | Batch_idx: 30 |  Loss: (0.0063) | Acc: (100.00%) (3968/3968)\n",
            "Epoch: 120 | Batch_idx: 40 |  Loss: (0.0064) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 120 | Batch_idx: 50 |  Loss: (0.0062) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 120 | Batch_idx: 60 |  Loss: (0.0066) | Acc: (99.99%) (7807/7808)\n",
            "Epoch: 120 | Batch_idx: 70 |  Loss: (0.0068) | Acc: (99.99%) (9087/9088)\n",
            "Epoch: 120 | Batch_idx: 80 |  Loss: (0.0067) | Acc: (99.98%) (10366/10368)\n",
            "Epoch: 120 | Batch_idx: 90 |  Loss: (0.0067) | Acc: (99.98%) (11646/11648)\n",
            "Epoch: 120 | Batch_idx: 100 |  Loss: (0.0069) | Acc: (99.98%) (12925/12928)\n",
            "Epoch: 120 | Batch_idx: 110 |  Loss: (0.0068) | Acc: (99.98%) (14205/14208)\n",
            "Epoch: 120 | Batch_idx: 120 |  Loss: (0.0068) | Acc: (99.98%) (15485/15488)\n",
            "Epoch: 120 | Batch_idx: 130 |  Loss: (0.0068) | Acc: (99.98%) (16765/16768)\n",
            "Epoch: 120 | Batch_idx: 140 |  Loss: (0.0068) | Acc: (99.98%) (18045/18048)\n",
            "Epoch: 120 | Batch_idx: 150 |  Loss: (0.0067) | Acc: (99.98%) (19324/19328)\n",
            "Epoch: 120 | Batch_idx: 160 |  Loss: (0.0067) | Acc: (99.98%) (20604/20608)\n",
            "Epoch: 120 | Batch_idx: 170 |  Loss: (0.0068) | Acc: (99.98%) (21883/21888)\n",
            "Epoch: 120 | Batch_idx: 180 |  Loss: (0.0068) | Acc: (99.98%) (23163/23168)\n",
            "Epoch: 120 | Batch_idx: 190 |  Loss: (0.0069) | Acc: (99.98%) (24442/24448)\n",
            "Epoch: 120 | Batch_idx: 200 |  Loss: (0.0069) | Acc: (99.97%) (25721/25728)\n",
            "Epoch: 120 | Batch_idx: 210 |  Loss: (0.0070) | Acc: (99.97%) (27001/27008)\n",
            "Epoch: 120 | Batch_idx: 220 |  Loss: (0.0070) | Acc: (99.98%) (28281/28288)\n",
            "Epoch: 120 | Batch_idx: 230 |  Loss: (0.0070) | Acc: (99.98%) (29561/29568)\n",
            "Epoch: 120 | Batch_idx: 240 |  Loss: (0.0071) | Acc: (99.97%) (30839/30848)\n",
            "Epoch: 120 | Batch_idx: 250 |  Loss: (0.0070) | Acc: (99.97%) (32119/32128)\n",
            "Epoch: 120 | Batch_idx: 260 |  Loss: (0.0070) | Acc: (99.97%) (33398/33408)\n",
            "Epoch: 120 | Batch_idx: 270 |  Loss: (0.0070) | Acc: (99.97%) (34677/34688)\n",
            "Epoch: 120 | Batch_idx: 280 |  Loss: (0.0071) | Acc: (99.97%) (35956/35968)\n",
            "Epoch: 120 | Batch_idx: 290 |  Loss: (0.0072) | Acc: (99.97%) (37235/37248)\n",
            "Epoch: 120 | Batch_idx: 300 |  Loss: (0.0071) | Acc: (99.96%) (38514/38528)\n",
            "Epoch: 120 | Batch_idx: 310 |  Loss: (0.0072) | Acc: (99.96%) (39792/39808)\n",
            "Epoch: 120 | Batch_idx: 320 |  Loss: (0.0072) | Acc: (99.96%) (41072/41088)\n",
            "Epoch: 120 | Batch_idx: 330 |  Loss: (0.0072) | Acc: (99.96%) (42350/42368)\n",
            "Epoch: 120 | Batch_idx: 340 |  Loss: (0.0072) | Acc: (99.96%) (43630/43648)\n",
            "Epoch: 120 | Batch_idx: 350 |  Loss: (0.0072) | Acc: (99.96%) (44910/44928)\n",
            "Epoch: 120 | Batch_idx: 360 |  Loss: (0.0072) | Acc: (99.96%) (46188/46208)\n",
            "Epoch: 120 | Batch_idx: 370 |  Loss: (0.0072) | Acc: (99.96%) (47468/47488)\n",
            "Epoch: 120 | Batch_idx: 380 |  Loss: (0.0072) | Acc: (99.96%) (48748/48768)\n",
            "Epoch: 120 | Batch_idx: 390 |  Loss: (0.0072) | Acc: (99.96%) (49980/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2598) | Acc: (92.87%) (9287/10000)\n",
            "Epoch: 121 | Batch_idx: 0 |  Loss: (0.0053) | Acc: (100.00%) (128/128)\n",
            "Epoch: 121 | Batch_idx: 10 |  Loss: (0.0065) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 121 | Batch_idx: 20 |  Loss: (0.0065) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 121 | Batch_idx: 30 |  Loss: (0.0071) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 121 | Batch_idx: 40 |  Loss: (0.0070) | Acc: (99.94%) (5245/5248)\n",
            "Epoch: 121 | Batch_idx: 50 |  Loss: (0.0070) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 121 | Batch_idx: 60 |  Loss: (0.0069) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 121 | Batch_idx: 70 |  Loss: (0.0069) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 121 | Batch_idx: 80 |  Loss: (0.0069) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 121 | Batch_idx: 90 |  Loss: (0.0069) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 121 | Batch_idx: 100 |  Loss: (0.0070) | Acc: (99.96%) (12923/12928)\n",
            "Epoch: 121 | Batch_idx: 110 |  Loss: (0.0070) | Acc: (99.96%) (14203/14208)\n",
            "Epoch: 121 | Batch_idx: 120 |  Loss: (0.0072) | Acc: (99.97%) (15483/15488)\n",
            "Epoch: 121 | Batch_idx: 130 |  Loss: (0.0072) | Acc: (99.96%) (16762/16768)\n",
            "Epoch: 121 | Batch_idx: 140 |  Loss: (0.0075) | Acc: (99.96%) (18040/18048)\n",
            "Epoch: 121 | Batch_idx: 150 |  Loss: (0.0074) | Acc: (99.96%) (19320/19328)\n",
            "Epoch: 121 | Batch_idx: 160 |  Loss: (0.0073) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 121 | Batch_idx: 170 |  Loss: (0.0073) | Acc: (99.96%) (21880/21888)\n",
            "Epoch: 121 | Batch_idx: 180 |  Loss: (0.0074) | Acc: (99.96%) (23159/23168)\n",
            "Epoch: 121 | Batch_idx: 190 |  Loss: (0.0073) | Acc: (99.96%) (24439/24448)\n",
            "Epoch: 121 | Batch_idx: 200 |  Loss: (0.0074) | Acc: (99.96%) (25718/25728)\n",
            "Epoch: 121 | Batch_idx: 210 |  Loss: (0.0074) | Acc: (99.96%) (26998/27008)\n",
            "Epoch: 121 | Batch_idx: 220 |  Loss: (0.0073) | Acc: (99.96%) (28278/28288)\n",
            "Epoch: 121 | Batch_idx: 230 |  Loss: (0.0074) | Acc: (99.96%) (29557/29568)\n",
            "Epoch: 121 | Batch_idx: 240 |  Loss: (0.0074) | Acc: (99.96%) (30836/30848)\n",
            "Epoch: 121 | Batch_idx: 250 |  Loss: (0.0075) | Acc: (99.96%) (32115/32128)\n",
            "Epoch: 121 | Batch_idx: 260 |  Loss: (0.0075) | Acc: (99.96%) (33395/33408)\n",
            "Epoch: 121 | Batch_idx: 270 |  Loss: (0.0075) | Acc: (99.96%) (34674/34688)\n",
            "Epoch: 121 | Batch_idx: 280 |  Loss: (0.0075) | Acc: (99.96%) (35953/35968)\n",
            "Epoch: 121 | Batch_idx: 290 |  Loss: (0.0076) | Acc: (99.95%) (37231/37248)\n",
            "Epoch: 121 | Batch_idx: 300 |  Loss: (0.0075) | Acc: (99.96%) (38511/38528)\n",
            "Epoch: 121 | Batch_idx: 310 |  Loss: (0.0075) | Acc: (99.96%) (39791/39808)\n",
            "Epoch: 121 | Batch_idx: 320 |  Loss: (0.0075) | Acc: (99.95%) (41069/41088)\n",
            "Epoch: 121 | Batch_idx: 330 |  Loss: (0.0075) | Acc: (99.96%) (42349/42368)\n",
            "Epoch: 121 | Batch_idx: 340 |  Loss: (0.0075) | Acc: (99.95%) (43628/43648)\n",
            "Epoch: 121 | Batch_idx: 350 |  Loss: (0.0075) | Acc: (99.96%) (44908/44928)\n",
            "Epoch: 121 | Batch_idx: 360 |  Loss: (0.0074) | Acc: (99.96%) (46188/46208)\n",
            "Epoch: 121 | Batch_idx: 370 |  Loss: (0.0074) | Acc: (99.96%) (47468/47488)\n",
            "Epoch: 121 | Batch_idx: 380 |  Loss: (0.0074) | Acc: (99.96%) (48748/48768)\n",
            "Epoch: 121 | Batch_idx: 390 |  Loss: (0.0074) | Acc: (99.96%) (49980/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2586) | Acc: (92.92%) (9292/10000)\n",
            "Epoch: 122 | Batch_idx: 0 |  Loss: (0.0085) | Acc: (100.00%) (128/128)\n",
            "Epoch: 122 | Batch_idx: 10 |  Loss: (0.0074) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 122 | Batch_idx: 20 |  Loss: (0.0074) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 122 | Batch_idx: 30 |  Loss: (0.0072) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 122 | Batch_idx: 40 |  Loss: (0.0068) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 122 | Batch_idx: 50 |  Loss: (0.0068) | Acc: (99.97%) (6526/6528)\n",
            "Epoch: 122 | Batch_idx: 60 |  Loss: (0.0068) | Acc: (99.96%) (7805/7808)\n",
            "Epoch: 122 | Batch_idx: 70 |  Loss: (0.0072) | Acc: (99.94%) (9083/9088)\n",
            "Epoch: 122 | Batch_idx: 80 |  Loss: (0.0072) | Acc: (99.94%) (10362/10368)\n",
            "Epoch: 122 | Batch_idx: 90 |  Loss: (0.0072) | Acc: (99.94%) (11641/11648)\n",
            "Epoch: 122 | Batch_idx: 100 |  Loss: (0.0071) | Acc: (99.95%) (12921/12928)\n",
            "Epoch: 122 | Batch_idx: 110 |  Loss: (0.0072) | Acc: (99.94%) (14200/14208)\n",
            "Epoch: 122 | Batch_idx: 120 |  Loss: (0.0070) | Acc: (99.95%) (15480/15488)\n",
            "Epoch: 122 | Batch_idx: 130 |  Loss: (0.0071) | Acc: (99.95%) (16759/16768)\n",
            "Epoch: 122 | Batch_idx: 140 |  Loss: (0.0071) | Acc: (99.94%) (18037/18048)\n",
            "Epoch: 122 | Batch_idx: 150 |  Loss: (0.0071) | Acc: (99.94%) (19316/19328)\n",
            "Epoch: 122 | Batch_idx: 160 |  Loss: (0.0072) | Acc: (99.94%) (20595/20608)\n",
            "Epoch: 122 | Batch_idx: 170 |  Loss: (0.0072) | Acc: (99.94%) (21874/21888)\n",
            "Epoch: 122 | Batch_idx: 180 |  Loss: (0.0072) | Acc: (99.94%) (23154/23168)\n",
            "Epoch: 122 | Batch_idx: 190 |  Loss: (0.0072) | Acc: (99.94%) (24434/24448)\n",
            "Epoch: 122 | Batch_idx: 200 |  Loss: (0.0073) | Acc: (99.93%) (25711/25728)\n",
            "Epoch: 122 | Batch_idx: 210 |  Loss: (0.0073) | Acc: (99.93%) (26990/27008)\n",
            "Epoch: 122 | Batch_idx: 220 |  Loss: (0.0072) | Acc: (99.94%) (28270/28288)\n",
            "Epoch: 122 | Batch_idx: 230 |  Loss: (0.0072) | Acc: (99.94%) (29549/29568)\n",
            "Epoch: 122 | Batch_idx: 240 |  Loss: (0.0072) | Acc: (99.94%) (30829/30848)\n",
            "Epoch: 122 | Batch_idx: 250 |  Loss: (0.0072) | Acc: (99.94%) (32109/32128)\n",
            "Epoch: 122 | Batch_idx: 260 |  Loss: (0.0072) | Acc: (99.94%) (33389/33408)\n",
            "Epoch: 122 | Batch_idx: 270 |  Loss: (0.0072) | Acc: (99.95%) (34669/34688)\n",
            "Epoch: 122 | Batch_idx: 280 |  Loss: (0.0072) | Acc: (99.95%) (35949/35968)\n",
            "Epoch: 122 | Batch_idx: 290 |  Loss: (0.0072) | Acc: (99.95%) (37228/37248)\n",
            "Epoch: 122 | Batch_idx: 300 |  Loss: (0.0073) | Acc: (99.95%) (38507/38528)\n",
            "Epoch: 122 | Batch_idx: 310 |  Loss: (0.0072) | Acc: (99.95%) (39787/39808)\n",
            "Epoch: 122 | Batch_idx: 320 |  Loss: (0.0072) | Acc: (99.95%) (41067/41088)\n",
            "Epoch: 122 | Batch_idx: 330 |  Loss: (0.0073) | Acc: (99.95%) (42345/42368)\n",
            "Epoch: 122 | Batch_idx: 340 |  Loss: (0.0074) | Acc: (99.94%) (43623/43648)\n",
            "Epoch: 122 | Batch_idx: 350 |  Loss: (0.0074) | Acc: (99.94%) (44903/44928)\n",
            "Epoch: 122 | Batch_idx: 360 |  Loss: (0.0074) | Acc: (99.94%) (46180/46208)\n",
            "Epoch: 122 | Batch_idx: 370 |  Loss: (0.0074) | Acc: (99.94%) (47460/47488)\n",
            "Epoch: 122 | Batch_idx: 380 |  Loss: (0.0074) | Acc: (99.94%) (48740/48768)\n",
            "Epoch: 122 | Batch_idx: 390 |  Loss: (0.0074) | Acc: (99.94%) (49971/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2612) | Acc: (92.90%) (9290/10000)\n",
            "Epoch: 123 | Batch_idx: 0 |  Loss: (0.0092) | Acc: (100.00%) (128/128)\n",
            "Epoch: 123 | Batch_idx: 10 |  Loss: (0.0063) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 123 | Batch_idx: 20 |  Loss: (0.0073) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 123 | Batch_idx: 30 |  Loss: (0.0075) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 123 | Batch_idx: 40 |  Loss: (0.0074) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 123 | Batch_idx: 50 |  Loss: (0.0077) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 123 | Batch_idx: 60 |  Loss: (0.0075) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 123 | Batch_idx: 70 |  Loss: (0.0077) | Acc: (99.94%) (9083/9088)\n",
            "Epoch: 123 | Batch_idx: 80 |  Loss: (0.0077) | Acc: (99.93%) (10361/10368)\n",
            "Epoch: 123 | Batch_idx: 90 |  Loss: (0.0076) | Acc: (99.94%) (11641/11648)\n",
            "Epoch: 123 | Batch_idx: 100 |  Loss: (0.0076) | Acc: (99.94%) (12920/12928)\n",
            "Epoch: 123 | Batch_idx: 110 |  Loss: (0.0077) | Acc: (99.92%) (14197/14208)\n",
            "Epoch: 123 | Batch_idx: 120 |  Loss: (0.0075) | Acc: (99.93%) (15477/15488)\n",
            "Epoch: 123 | Batch_idx: 130 |  Loss: (0.0075) | Acc: (99.93%) (16757/16768)\n",
            "Epoch: 123 | Batch_idx: 140 |  Loss: (0.0074) | Acc: (99.93%) (18036/18048)\n",
            "Epoch: 123 | Batch_idx: 150 |  Loss: (0.0073) | Acc: (99.94%) (19316/19328)\n",
            "Epoch: 123 | Batch_idx: 160 |  Loss: (0.0074) | Acc: (99.94%) (20596/20608)\n",
            "Epoch: 123 | Batch_idx: 170 |  Loss: (0.0073) | Acc: (99.95%) (21876/21888)\n",
            "Epoch: 123 | Batch_idx: 180 |  Loss: (0.0072) | Acc: (99.95%) (23156/23168)\n",
            "Epoch: 123 | Batch_idx: 190 |  Loss: (0.0072) | Acc: (99.95%) (24436/24448)\n",
            "Epoch: 123 | Batch_idx: 200 |  Loss: (0.0072) | Acc: (99.95%) (25715/25728)\n",
            "Epoch: 123 | Batch_idx: 210 |  Loss: (0.0071) | Acc: (99.95%) (26995/27008)\n",
            "Epoch: 123 | Batch_idx: 220 |  Loss: (0.0071) | Acc: (99.95%) (28275/28288)\n",
            "Epoch: 123 | Batch_idx: 230 |  Loss: (0.0072) | Acc: (99.95%) (29552/29568)\n",
            "Epoch: 123 | Batch_idx: 240 |  Loss: (0.0072) | Acc: (99.94%) (30830/30848)\n",
            "Epoch: 123 | Batch_idx: 250 |  Loss: (0.0072) | Acc: (99.94%) (32110/32128)\n",
            "Epoch: 123 | Batch_idx: 260 |  Loss: (0.0071) | Acc: (99.95%) (33390/33408)\n",
            "Epoch: 123 | Batch_idx: 270 |  Loss: (0.0071) | Acc: (99.95%) (34670/34688)\n",
            "Epoch: 123 | Batch_idx: 280 |  Loss: (0.0071) | Acc: (99.95%) (35950/35968)\n",
            "Epoch: 123 | Batch_idx: 290 |  Loss: (0.0071) | Acc: (99.95%) (37230/37248)\n",
            "Epoch: 123 | Batch_idx: 300 |  Loss: (0.0071) | Acc: (99.95%) (38508/38528)\n",
            "Epoch: 123 | Batch_idx: 310 |  Loss: (0.0071) | Acc: (99.94%) (39786/39808)\n",
            "Epoch: 123 | Batch_idx: 320 |  Loss: (0.0071) | Acc: (99.95%) (41066/41088)\n",
            "Epoch: 123 | Batch_idx: 330 |  Loss: (0.0071) | Acc: (99.95%) (42346/42368)\n",
            "Epoch: 123 | Batch_idx: 340 |  Loss: (0.0071) | Acc: (99.95%) (43625/43648)\n",
            "Epoch: 123 | Batch_idx: 350 |  Loss: (0.0072) | Acc: (99.95%) (44905/44928)\n",
            "Epoch: 123 | Batch_idx: 360 |  Loss: (0.0071) | Acc: (99.95%) (46185/46208)\n",
            "Epoch: 123 | Batch_idx: 370 |  Loss: (0.0071) | Acc: (99.95%) (47465/47488)\n",
            "Epoch: 123 | Batch_idx: 380 |  Loss: (0.0071) | Acc: (99.95%) (48745/48768)\n",
            "Epoch: 123 | Batch_idx: 390 |  Loss: (0.0071) | Acc: (99.95%) (49977/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2581) | Acc: (93.03%) (9303/10000)\n",
            "Epoch: 124 | Batch_idx: 0 |  Loss: (0.0068) | Acc: (100.00%) (128/128)\n",
            "Epoch: 124 | Batch_idx: 10 |  Loss: (0.0067) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 124 | Batch_idx: 20 |  Loss: (0.0069) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 124 | Batch_idx: 30 |  Loss: (0.0069) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 124 | Batch_idx: 40 |  Loss: (0.0070) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 124 | Batch_idx: 50 |  Loss: (0.0070) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 124 | Batch_idx: 60 |  Loss: (0.0069) | Acc: (99.99%) (7807/7808)\n",
            "Epoch: 124 | Batch_idx: 70 |  Loss: (0.0069) | Acc: (99.99%) (9087/9088)\n",
            "Epoch: 124 | Batch_idx: 80 |  Loss: (0.0069) | Acc: (99.99%) (10367/10368)\n",
            "Epoch: 124 | Batch_idx: 90 |  Loss: (0.0067) | Acc: (99.99%) (11647/11648)\n",
            "Epoch: 124 | Batch_idx: 100 |  Loss: (0.0067) | Acc: (99.99%) (12927/12928)\n",
            "Epoch: 124 | Batch_idx: 110 |  Loss: (0.0067) | Acc: (99.99%) (14206/14208)\n",
            "Epoch: 124 | Batch_idx: 120 |  Loss: (0.0067) | Acc: (99.98%) (15485/15488)\n",
            "Epoch: 124 | Batch_idx: 130 |  Loss: (0.0068) | Acc: (99.98%) (16764/16768)\n",
            "Epoch: 124 | Batch_idx: 140 |  Loss: (0.0067) | Acc: (99.97%) (18043/18048)\n",
            "Epoch: 124 | Batch_idx: 150 |  Loss: (0.0068) | Acc: (99.97%) (19323/19328)\n",
            "Epoch: 124 | Batch_idx: 160 |  Loss: (0.0068) | Acc: (99.97%) (20602/20608)\n",
            "Epoch: 124 | Batch_idx: 170 |  Loss: (0.0068) | Acc: (99.97%) (21882/21888)\n",
            "Epoch: 124 | Batch_idx: 180 |  Loss: (0.0068) | Acc: (99.97%) (23162/23168)\n",
            "Epoch: 124 | Batch_idx: 190 |  Loss: (0.0067) | Acc: (99.98%) (24442/24448)\n",
            "Epoch: 124 | Batch_idx: 200 |  Loss: (0.0067) | Acc: (99.98%) (25722/25728)\n",
            "Epoch: 124 | Batch_idx: 210 |  Loss: (0.0066) | Acc: (99.98%) (27002/27008)\n",
            "Epoch: 124 | Batch_idx: 220 |  Loss: (0.0066) | Acc: (99.98%) (28281/28288)\n",
            "Epoch: 124 | Batch_idx: 230 |  Loss: (0.0066) | Acc: (99.98%) (29561/29568)\n",
            "Epoch: 124 | Batch_idx: 240 |  Loss: (0.0066) | Acc: (99.97%) (30840/30848)\n",
            "Epoch: 124 | Batch_idx: 250 |  Loss: (0.0067) | Acc: (99.97%) (32119/32128)\n",
            "Epoch: 124 | Batch_idx: 260 |  Loss: (0.0067) | Acc: (99.97%) (33397/33408)\n",
            "Epoch: 124 | Batch_idx: 270 |  Loss: (0.0068) | Acc: (99.97%) (34676/34688)\n",
            "Epoch: 124 | Batch_idx: 280 |  Loss: (0.0068) | Acc: (99.97%) (35956/35968)\n",
            "Epoch: 124 | Batch_idx: 290 |  Loss: (0.0067) | Acc: (99.97%) (37236/37248)\n",
            "Epoch: 124 | Batch_idx: 300 |  Loss: (0.0067) | Acc: (99.97%) (38515/38528)\n",
            "Epoch: 124 | Batch_idx: 310 |  Loss: (0.0067) | Acc: (99.96%) (39794/39808)\n",
            "Epoch: 124 | Batch_idx: 320 |  Loss: (0.0068) | Acc: (99.97%) (41074/41088)\n",
            "Epoch: 124 | Batch_idx: 330 |  Loss: (0.0069) | Acc: (99.97%) (42354/42368)\n",
            "Epoch: 124 | Batch_idx: 340 |  Loss: (0.0069) | Acc: (99.97%) (43633/43648)\n",
            "Epoch: 124 | Batch_idx: 350 |  Loss: (0.0068) | Acc: (99.96%) (44912/44928)\n",
            "Epoch: 124 | Batch_idx: 360 |  Loss: (0.0068) | Acc: (99.97%) (46192/46208)\n",
            "Epoch: 124 | Batch_idx: 370 |  Loss: (0.0068) | Acc: (99.97%) (47472/47488)\n",
            "Epoch: 124 | Batch_idx: 380 |  Loss: (0.0069) | Acc: (99.96%) (48750/48768)\n",
            "Epoch: 124 | Batch_idx: 390 |  Loss: (0.0069) | Acc: (99.96%) (49982/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2604) | Acc: (92.97%) (9297/10000)\n",
            "Epoch: 125 | Batch_idx: 0 |  Loss: (0.0052) | Acc: (100.00%) (128/128)\n",
            "Epoch: 125 | Batch_idx: 10 |  Loss: (0.0064) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 125 | Batch_idx: 20 |  Loss: (0.0064) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 125 | Batch_idx: 30 |  Loss: (0.0069) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 125 | Batch_idx: 40 |  Loss: (0.0071) | Acc: (99.94%) (5245/5248)\n",
            "Epoch: 125 | Batch_idx: 50 |  Loss: (0.0074) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 125 | Batch_idx: 60 |  Loss: (0.0072) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 125 | Batch_idx: 70 |  Loss: (0.0071) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 125 | Batch_idx: 80 |  Loss: (0.0072) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 125 | Batch_idx: 90 |  Loss: (0.0070) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 125 | Batch_idx: 100 |  Loss: (0.0071) | Acc: (99.97%) (12924/12928)\n",
            "Epoch: 125 | Batch_idx: 110 |  Loss: (0.0069) | Acc: (99.96%) (14203/14208)\n",
            "Epoch: 125 | Batch_idx: 120 |  Loss: (0.0069) | Acc: (99.96%) (15482/15488)\n",
            "Epoch: 125 | Batch_idx: 130 |  Loss: (0.0068) | Acc: (99.96%) (16762/16768)\n",
            "Epoch: 125 | Batch_idx: 140 |  Loss: (0.0068) | Acc: (99.97%) (18042/18048)\n",
            "Epoch: 125 | Batch_idx: 150 |  Loss: (0.0068) | Acc: (99.96%) (19321/19328)\n",
            "Epoch: 125 | Batch_idx: 160 |  Loss: (0.0069) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 125 | Batch_idx: 170 |  Loss: (0.0071) | Acc: (99.96%) (21880/21888)\n",
            "Epoch: 125 | Batch_idx: 180 |  Loss: (0.0070) | Acc: (99.97%) (23160/23168)\n",
            "Epoch: 125 | Batch_idx: 190 |  Loss: (0.0070) | Acc: (99.97%) (24440/24448)\n",
            "Epoch: 125 | Batch_idx: 200 |  Loss: (0.0070) | Acc: (99.97%) (25719/25728)\n",
            "Epoch: 125 | Batch_idx: 210 |  Loss: (0.0070) | Acc: (99.97%) (26999/27008)\n",
            "Epoch: 125 | Batch_idx: 220 |  Loss: (0.0069) | Acc: (99.97%) (28279/28288)\n",
            "Epoch: 125 | Batch_idx: 230 |  Loss: (0.0069) | Acc: (99.97%) (29559/29568)\n",
            "Epoch: 125 | Batch_idx: 240 |  Loss: (0.0069) | Acc: (99.97%) (30839/30848)\n",
            "Epoch: 125 | Batch_idx: 250 |  Loss: (0.0069) | Acc: (99.97%) (32118/32128)\n",
            "Epoch: 125 | Batch_idx: 260 |  Loss: (0.0070) | Acc: (99.96%) (33396/33408)\n",
            "Epoch: 125 | Batch_idx: 270 |  Loss: (0.0070) | Acc: (99.97%) (34676/34688)\n",
            "Epoch: 125 | Batch_idx: 280 |  Loss: (0.0071) | Acc: (99.96%) (35954/35968)\n",
            "Epoch: 125 | Batch_idx: 290 |  Loss: (0.0071) | Acc: (99.96%) (37233/37248)\n",
            "Epoch: 125 | Batch_idx: 300 |  Loss: (0.0070) | Acc: (99.96%) (38513/38528)\n",
            "Epoch: 125 | Batch_idx: 310 |  Loss: (0.0070) | Acc: (99.96%) (39793/39808)\n",
            "Epoch: 125 | Batch_idx: 320 |  Loss: (0.0070) | Acc: (99.96%) (41073/41088)\n",
            "Epoch: 125 | Batch_idx: 330 |  Loss: (0.0070) | Acc: (99.96%) (42353/42368)\n",
            "Epoch: 125 | Batch_idx: 340 |  Loss: (0.0069) | Acc: (99.97%) (43633/43648)\n",
            "Epoch: 125 | Batch_idx: 350 |  Loss: (0.0069) | Acc: (99.97%) (44913/44928)\n",
            "Epoch: 125 | Batch_idx: 360 |  Loss: (0.0069) | Acc: (99.97%) (46192/46208)\n",
            "Epoch: 125 | Batch_idx: 370 |  Loss: (0.0069) | Acc: (99.97%) (47472/47488)\n",
            "Epoch: 125 | Batch_idx: 380 |  Loss: (0.0068) | Acc: (99.97%) (48752/48768)\n",
            "Epoch: 125 | Batch_idx: 390 |  Loss: (0.0069) | Acc: (99.97%) (49984/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2573) | Acc: (92.96%) (9296/10000)\n",
            "Epoch: 126 | Batch_idx: 0 |  Loss: (0.0037) | Acc: (100.00%) (128/128)\n",
            "Epoch: 126 | Batch_idx: 10 |  Loss: (0.0067) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 126 | Batch_idx: 20 |  Loss: (0.0063) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 126 | Batch_idx: 30 |  Loss: (0.0072) | Acc: (99.92%) (3965/3968)\n",
            "Epoch: 126 | Batch_idx: 40 |  Loss: (0.0072) | Acc: (99.92%) (5244/5248)\n",
            "Epoch: 126 | Batch_idx: 50 |  Loss: (0.0073) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 126 | Batch_idx: 60 |  Loss: (0.0070) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 126 | Batch_idx: 70 |  Loss: (0.0069) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 126 | Batch_idx: 80 |  Loss: (0.0069) | Acc: (99.95%) (10363/10368)\n",
            "Epoch: 126 | Batch_idx: 90 |  Loss: (0.0068) | Acc: (99.96%) (11643/11648)\n",
            "Epoch: 126 | Batch_idx: 100 |  Loss: (0.0069) | Acc: (99.96%) (12923/12928)\n",
            "Epoch: 126 | Batch_idx: 110 |  Loss: (0.0070) | Acc: (99.96%) (14203/14208)\n",
            "Epoch: 126 | Batch_idx: 120 |  Loss: (0.0069) | Acc: (99.97%) (15483/15488)\n",
            "Epoch: 126 | Batch_idx: 130 |  Loss: (0.0069) | Acc: (99.97%) (16763/16768)\n",
            "Epoch: 126 | Batch_idx: 140 |  Loss: (0.0068) | Acc: (99.96%) (18041/18048)\n",
            "Epoch: 126 | Batch_idx: 150 |  Loss: (0.0068) | Acc: (99.96%) (19321/19328)\n",
            "Epoch: 126 | Batch_idx: 160 |  Loss: (0.0070) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 126 | Batch_idx: 170 |  Loss: (0.0069) | Acc: (99.96%) (21880/21888)\n",
            "Epoch: 126 | Batch_idx: 180 |  Loss: (0.0069) | Acc: (99.97%) (23160/23168)\n",
            "Epoch: 126 | Batch_idx: 190 |  Loss: (0.0069) | Acc: (99.97%) (24440/24448)\n",
            "Epoch: 126 | Batch_idx: 200 |  Loss: (0.0068) | Acc: (99.97%) (25720/25728)\n",
            "Epoch: 126 | Batch_idx: 210 |  Loss: (0.0067) | Acc: (99.97%) (27000/27008)\n",
            "Epoch: 126 | Batch_idx: 220 |  Loss: (0.0067) | Acc: (99.97%) (28279/28288)\n",
            "Epoch: 126 | Batch_idx: 230 |  Loss: (0.0066) | Acc: (99.97%) (29558/29568)\n",
            "Epoch: 126 | Batch_idx: 240 |  Loss: (0.0066) | Acc: (99.97%) (30838/30848)\n",
            "Epoch: 126 | Batch_idx: 250 |  Loss: (0.0066) | Acc: (99.97%) (32118/32128)\n",
            "Epoch: 126 | Batch_idx: 260 |  Loss: (0.0066) | Acc: (99.97%) (33397/33408)\n",
            "Epoch: 126 | Batch_idx: 270 |  Loss: (0.0067) | Acc: (99.97%) (34677/34688)\n",
            "Epoch: 126 | Batch_idx: 280 |  Loss: (0.0066) | Acc: (99.97%) (35957/35968)\n",
            "Epoch: 126 | Batch_idx: 290 |  Loss: (0.0066) | Acc: (99.97%) (37236/37248)\n",
            "Epoch: 126 | Batch_idx: 300 |  Loss: (0.0066) | Acc: (99.97%) (38516/38528)\n",
            "Epoch: 126 | Batch_idx: 310 |  Loss: (0.0066) | Acc: (99.97%) (39796/39808)\n",
            "Epoch: 126 | Batch_idx: 320 |  Loss: (0.0066) | Acc: (99.97%) (41076/41088)\n",
            "Epoch: 126 | Batch_idx: 330 |  Loss: (0.0066) | Acc: (99.97%) (42356/42368)\n",
            "Epoch: 126 | Batch_idx: 340 |  Loss: (0.0066) | Acc: (99.97%) (43636/43648)\n",
            "Epoch: 126 | Batch_idx: 350 |  Loss: (0.0066) | Acc: (99.97%) (44916/44928)\n",
            "Epoch: 126 | Batch_idx: 360 |  Loss: (0.0066) | Acc: (99.97%) (46196/46208)\n",
            "Epoch: 126 | Batch_idx: 370 |  Loss: (0.0067) | Acc: (99.97%) (47474/47488)\n",
            "Epoch: 126 | Batch_idx: 380 |  Loss: (0.0068) | Acc: (99.97%) (48751/48768)\n",
            "Epoch: 126 | Batch_idx: 390 |  Loss: (0.0068) | Acc: (99.97%) (49983/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2586) | Acc: (92.89%) (9289/10000)\n",
            "Epoch: 127 | Batch_idx: 0 |  Loss: (0.0070) | Acc: (100.00%) (128/128)\n",
            "Epoch: 127 | Batch_idx: 10 |  Loss: (0.0063) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 127 | Batch_idx: 20 |  Loss: (0.0076) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 127 | Batch_idx: 30 |  Loss: (0.0073) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 127 | Batch_idx: 40 |  Loss: (0.0069) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 127 | Batch_idx: 50 |  Loss: (0.0071) | Acc: (99.95%) (6525/6528)\n",
            "Epoch: 127 | Batch_idx: 60 |  Loss: (0.0071) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 127 | Batch_idx: 70 |  Loss: (0.0068) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 127 | Batch_idx: 80 |  Loss: (0.0069) | Acc: (99.94%) (10362/10368)\n",
            "Epoch: 127 | Batch_idx: 90 |  Loss: (0.0068) | Acc: (99.95%) (11642/11648)\n",
            "Epoch: 127 | Batch_idx: 100 |  Loss: (0.0068) | Acc: (99.95%) (12922/12928)\n",
            "Epoch: 127 | Batch_idx: 110 |  Loss: (0.0068) | Acc: (99.96%) (14202/14208)\n",
            "Epoch: 127 | Batch_idx: 120 |  Loss: (0.0067) | Acc: (99.96%) (15482/15488)\n",
            "Epoch: 127 | Batch_idx: 130 |  Loss: (0.0069) | Acc: (99.95%) (16760/16768)\n",
            "Epoch: 127 | Batch_idx: 140 |  Loss: (0.0068) | Acc: (99.96%) (18040/18048)\n",
            "Epoch: 127 | Batch_idx: 150 |  Loss: (0.0069) | Acc: (99.95%) (19319/19328)\n",
            "Epoch: 127 | Batch_idx: 160 |  Loss: (0.0069) | Acc: (99.96%) (20599/20608)\n",
            "Epoch: 127 | Batch_idx: 170 |  Loss: (0.0069) | Acc: (99.95%) (21878/21888)\n",
            "Epoch: 127 | Batch_idx: 180 |  Loss: (0.0070) | Acc: (99.96%) (23158/23168)\n",
            "Epoch: 127 | Batch_idx: 190 |  Loss: (0.0070) | Acc: (99.96%) (24437/24448)\n",
            "Epoch: 127 | Batch_idx: 200 |  Loss: (0.0069) | Acc: (99.95%) (25716/25728)\n",
            "Epoch: 127 | Batch_idx: 210 |  Loss: (0.0070) | Acc: (99.95%) (26995/27008)\n",
            "Epoch: 127 | Batch_idx: 220 |  Loss: (0.0070) | Acc: (99.95%) (28274/28288)\n",
            "Epoch: 127 | Batch_idx: 230 |  Loss: (0.0071) | Acc: (99.95%) (29552/29568)\n",
            "Epoch: 127 | Batch_idx: 240 |  Loss: (0.0073) | Acc: (99.94%) (30828/30848)\n",
            "Epoch: 127 | Batch_idx: 250 |  Loss: (0.0072) | Acc: (99.94%) (32108/32128)\n",
            "Epoch: 127 | Batch_idx: 260 |  Loss: (0.0072) | Acc: (99.94%) (33387/33408)\n",
            "Epoch: 127 | Batch_idx: 270 |  Loss: (0.0071) | Acc: (99.94%) (34667/34688)\n",
            "Epoch: 127 | Batch_idx: 280 |  Loss: (0.0071) | Acc: (99.94%) (35947/35968)\n",
            "Epoch: 127 | Batch_idx: 290 |  Loss: (0.0071) | Acc: (99.94%) (37227/37248)\n",
            "Epoch: 127 | Batch_idx: 300 |  Loss: (0.0071) | Acc: (99.95%) (38507/38528)\n",
            "Epoch: 127 | Batch_idx: 310 |  Loss: (0.0070) | Acc: (99.95%) (39787/39808)\n",
            "Epoch: 127 | Batch_idx: 320 |  Loss: (0.0070) | Acc: (99.95%) (41067/41088)\n",
            "Epoch: 127 | Batch_idx: 330 |  Loss: (0.0070) | Acc: (99.95%) (42347/42368)\n",
            "Epoch: 127 | Batch_idx: 340 |  Loss: (0.0070) | Acc: (99.95%) (43626/43648)\n",
            "Epoch: 127 | Batch_idx: 350 |  Loss: (0.0069) | Acc: (99.95%) (44906/44928)\n",
            "Epoch: 127 | Batch_idx: 360 |  Loss: (0.0069) | Acc: (99.95%) (46186/46208)\n",
            "Epoch: 127 | Batch_idx: 370 |  Loss: (0.0069) | Acc: (99.95%) (47466/47488)\n",
            "Epoch: 127 | Batch_idx: 380 |  Loss: (0.0069) | Acc: (99.95%) (48745/48768)\n",
            "Epoch: 127 | Batch_idx: 390 |  Loss: (0.0069) | Acc: (99.95%) (49977/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2589) | Acc: (92.91%) (9291/10000)\n",
            "Epoch: 128 | Batch_idx: 0 |  Loss: (0.0066) | Acc: (100.00%) (128/128)\n",
            "Epoch: 128 | Batch_idx: 10 |  Loss: (0.0063) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 128 | Batch_idx: 20 |  Loss: (0.0066) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 128 | Batch_idx: 30 |  Loss: (0.0073) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 128 | Batch_idx: 40 |  Loss: (0.0070) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 128 | Batch_idx: 50 |  Loss: (0.0067) | Acc: (99.97%) (6526/6528)\n",
            "Epoch: 128 | Batch_idx: 60 |  Loss: (0.0069) | Acc: (99.97%) (7806/7808)\n",
            "Epoch: 128 | Batch_idx: 70 |  Loss: (0.0067) | Acc: (99.98%) (9086/9088)\n",
            "Epoch: 128 | Batch_idx: 80 |  Loss: (0.0067) | Acc: (99.98%) (10366/10368)\n",
            "Epoch: 128 | Batch_idx: 90 |  Loss: (0.0065) | Acc: (99.98%) (11646/11648)\n",
            "Epoch: 128 | Batch_idx: 100 |  Loss: (0.0064) | Acc: (99.98%) (12926/12928)\n",
            "Epoch: 128 | Batch_idx: 110 |  Loss: (0.0063) | Acc: (99.99%) (14206/14208)\n",
            "Epoch: 128 | Batch_idx: 120 |  Loss: (0.0065) | Acc: (99.99%) (15486/15488)\n",
            "Epoch: 128 | Batch_idx: 130 |  Loss: (0.0065) | Acc: (99.98%) (16765/16768)\n",
            "Epoch: 128 | Batch_idx: 140 |  Loss: (0.0065) | Acc: (99.98%) (18045/18048)\n",
            "Epoch: 128 | Batch_idx: 150 |  Loss: (0.0065) | Acc: (99.98%) (19325/19328)\n",
            "Epoch: 128 | Batch_idx: 160 |  Loss: (0.0066) | Acc: (99.98%) (20604/20608)\n",
            "Epoch: 128 | Batch_idx: 170 |  Loss: (0.0066) | Acc: (99.98%) (21883/21888)\n",
            "Epoch: 128 | Batch_idx: 180 |  Loss: (0.0067) | Acc: (99.97%) (23162/23168)\n",
            "Epoch: 128 | Batch_idx: 190 |  Loss: (0.0067) | Acc: (99.98%) (24442/24448)\n",
            "Epoch: 128 | Batch_idx: 200 |  Loss: (0.0067) | Acc: (99.98%) (25722/25728)\n",
            "Epoch: 128 | Batch_idx: 210 |  Loss: (0.0067) | Acc: (99.98%) (27002/27008)\n",
            "Epoch: 128 | Batch_idx: 220 |  Loss: (0.0067) | Acc: (99.98%) (28282/28288)\n",
            "Epoch: 128 | Batch_idx: 230 |  Loss: (0.0067) | Acc: (99.98%) (29561/29568)\n",
            "Epoch: 128 | Batch_idx: 240 |  Loss: (0.0067) | Acc: (99.98%) (30841/30848)\n",
            "Epoch: 128 | Batch_idx: 250 |  Loss: (0.0067) | Acc: (99.98%) (32121/32128)\n",
            "Epoch: 128 | Batch_idx: 260 |  Loss: (0.0066) | Acc: (99.98%) (33401/33408)\n",
            "Epoch: 128 | Batch_idx: 270 |  Loss: (0.0066) | Acc: (99.98%) (34681/34688)\n",
            "Epoch: 128 | Batch_idx: 280 |  Loss: (0.0066) | Acc: (99.98%) (35961/35968)\n",
            "Epoch: 128 | Batch_idx: 290 |  Loss: (0.0066) | Acc: (99.98%) (37240/37248)\n",
            "Epoch: 128 | Batch_idx: 300 |  Loss: (0.0066) | Acc: (99.97%) (38518/38528)\n",
            "Epoch: 128 | Batch_idx: 310 |  Loss: (0.0066) | Acc: (99.97%) (39798/39808)\n",
            "Epoch: 128 | Batch_idx: 320 |  Loss: (0.0066) | Acc: (99.98%) (41078/41088)\n",
            "Epoch: 128 | Batch_idx: 330 |  Loss: (0.0066) | Acc: (99.98%) (42358/42368)\n",
            "Epoch: 128 | Batch_idx: 340 |  Loss: (0.0066) | Acc: (99.98%) (43638/43648)\n",
            "Epoch: 128 | Batch_idx: 350 |  Loss: (0.0066) | Acc: (99.98%) (44918/44928)\n",
            "Epoch: 128 | Batch_idx: 360 |  Loss: (0.0066) | Acc: (99.98%) (46198/46208)\n",
            "Epoch: 128 | Batch_idx: 370 |  Loss: (0.0066) | Acc: (99.98%) (47478/47488)\n",
            "Epoch: 128 | Batch_idx: 380 |  Loss: (0.0066) | Acc: (99.98%) (48757/48768)\n",
            "Epoch: 128 | Batch_idx: 390 |  Loss: (0.0066) | Acc: (99.97%) (49987/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2604) | Acc: (93.04%) (9304/10000)\n",
            "Epoch: 129 | Batch_idx: 0 |  Loss: (0.0043) | Acc: (100.00%) (128/128)\n",
            "Epoch: 129 | Batch_idx: 10 |  Loss: (0.0050) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 129 | Batch_idx: 20 |  Loss: (0.0061) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 129 | Batch_idx: 30 |  Loss: (0.0066) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 129 | Batch_idx: 40 |  Loss: (0.0064) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 129 | Batch_idx: 50 |  Loss: (0.0062) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 129 | Batch_idx: 60 |  Loss: (0.0064) | Acc: (99.99%) (7807/7808)\n",
            "Epoch: 129 | Batch_idx: 70 |  Loss: (0.0066) | Acc: (99.98%) (9086/9088)\n",
            "Epoch: 129 | Batch_idx: 80 |  Loss: (0.0067) | Acc: (99.98%) (10366/10368)\n",
            "Epoch: 129 | Batch_idx: 90 |  Loss: (0.0067) | Acc: (99.97%) (11645/11648)\n",
            "Epoch: 129 | Batch_idx: 100 |  Loss: (0.0066) | Acc: (99.98%) (12925/12928)\n",
            "Epoch: 129 | Batch_idx: 110 |  Loss: (0.0067) | Acc: (99.98%) (14205/14208)\n",
            "Epoch: 129 | Batch_idx: 120 |  Loss: (0.0066) | Acc: (99.98%) (15485/15488)\n",
            "Epoch: 129 | Batch_idx: 130 |  Loss: (0.0067) | Acc: (99.96%) (16762/16768)\n",
            "Epoch: 129 | Batch_idx: 140 |  Loss: (0.0068) | Acc: (99.96%) (18041/18048)\n",
            "Epoch: 129 | Batch_idx: 150 |  Loss: (0.0069) | Acc: (99.96%) (19321/19328)\n",
            "Epoch: 129 | Batch_idx: 160 |  Loss: (0.0068) | Acc: (99.97%) (20601/20608)\n",
            "Epoch: 129 | Batch_idx: 170 |  Loss: (0.0068) | Acc: (99.95%) (21878/21888)\n",
            "Epoch: 129 | Batch_idx: 180 |  Loss: (0.0068) | Acc: (99.96%) (23158/23168)\n",
            "Epoch: 129 | Batch_idx: 190 |  Loss: (0.0068) | Acc: (99.96%) (24438/24448)\n",
            "Epoch: 129 | Batch_idx: 200 |  Loss: (0.0068) | Acc: (99.96%) (25717/25728)\n",
            "Epoch: 129 | Batch_idx: 210 |  Loss: (0.0069) | Acc: (99.96%) (26996/27008)\n",
            "Epoch: 129 | Batch_idx: 220 |  Loss: (0.0069) | Acc: (99.96%) (28276/28288)\n",
            "Epoch: 129 | Batch_idx: 230 |  Loss: (0.0069) | Acc: (99.96%) (29556/29568)\n",
            "Epoch: 129 | Batch_idx: 240 |  Loss: (0.0069) | Acc: (99.95%) (30834/30848)\n",
            "Epoch: 129 | Batch_idx: 250 |  Loss: (0.0069) | Acc: (99.95%) (32113/32128)\n",
            "Epoch: 129 | Batch_idx: 260 |  Loss: (0.0069) | Acc: (99.95%) (33392/33408)\n",
            "Epoch: 129 | Batch_idx: 270 |  Loss: (0.0069) | Acc: (99.95%) (34672/34688)\n",
            "Epoch: 129 | Batch_idx: 280 |  Loss: (0.0069) | Acc: (99.96%) (35952/35968)\n",
            "Epoch: 129 | Batch_idx: 290 |  Loss: (0.0069) | Acc: (99.96%) (37232/37248)\n",
            "Epoch: 129 | Batch_idx: 300 |  Loss: (0.0068) | Acc: (99.96%) (38512/38528)\n",
            "Epoch: 129 | Batch_idx: 310 |  Loss: (0.0069) | Acc: (99.96%) (39791/39808)\n",
            "Epoch: 129 | Batch_idx: 320 |  Loss: (0.0068) | Acc: (99.96%) (41071/41088)\n",
            "Epoch: 129 | Batch_idx: 330 |  Loss: (0.0069) | Acc: (99.96%) (42350/42368)\n",
            "Epoch: 129 | Batch_idx: 340 |  Loss: (0.0068) | Acc: (99.96%) (43630/43648)\n",
            "Epoch: 129 | Batch_idx: 350 |  Loss: (0.0068) | Acc: (99.96%) (44908/44928)\n",
            "Epoch: 129 | Batch_idx: 360 |  Loss: (0.0068) | Acc: (99.96%) (46188/46208)\n",
            "Epoch: 129 | Batch_idx: 370 |  Loss: (0.0069) | Acc: (99.96%) (47467/47488)\n",
            "Epoch: 129 | Batch_idx: 380 |  Loss: (0.0069) | Acc: (99.96%) (48747/48768)\n",
            "Epoch: 129 | Batch_idx: 390 |  Loss: (0.0069) | Acc: (99.96%) (49979/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2599) | Acc: (92.98%) (9298/10000)\n",
            "Epoch: 130 | Batch_idx: 0 |  Loss: (0.0028) | Acc: (100.00%) (128/128)\n",
            "Epoch: 130 | Batch_idx: 10 |  Loss: (0.0086) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 130 | Batch_idx: 20 |  Loss: (0.0081) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 130 | Batch_idx: 30 |  Loss: (0.0077) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 130 | Batch_idx: 40 |  Loss: (0.0076) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 130 | Batch_idx: 50 |  Loss: (0.0074) | Acc: (99.97%) (6526/6528)\n",
            "Epoch: 130 | Batch_idx: 60 |  Loss: (0.0071) | Acc: (99.96%) (7805/7808)\n",
            "Epoch: 130 | Batch_idx: 70 |  Loss: (0.0070) | Acc: (99.97%) (9085/9088)\n",
            "Epoch: 130 | Batch_idx: 80 |  Loss: (0.0069) | Acc: (99.97%) (10365/10368)\n",
            "Epoch: 130 | Batch_idx: 90 |  Loss: (0.0068) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 130 | Batch_idx: 100 |  Loss: (0.0069) | Acc: (99.97%) (12924/12928)\n",
            "Epoch: 130 | Batch_idx: 110 |  Loss: (0.0069) | Acc: (99.97%) (14204/14208)\n",
            "Epoch: 130 | Batch_idx: 120 |  Loss: (0.0067) | Acc: (99.97%) (15484/15488)\n",
            "Epoch: 130 | Batch_idx: 130 |  Loss: (0.0068) | Acc: (99.96%) (16762/16768)\n",
            "Epoch: 130 | Batch_idx: 140 |  Loss: (0.0066) | Acc: (99.97%) (18042/18048)\n",
            "Epoch: 130 | Batch_idx: 150 |  Loss: (0.0066) | Acc: (99.96%) (19321/19328)\n",
            "Epoch: 130 | Batch_idx: 160 |  Loss: (0.0066) | Acc: (99.97%) (20601/20608)\n",
            "Epoch: 130 | Batch_idx: 170 |  Loss: (0.0066) | Acc: (99.97%) (21881/21888)\n",
            "Epoch: 130 | Batch_idx: 180 |  Loss: (0.0066) | Acc: (99.97%) (23161/23168)\n",
            "Epoch: 130 | Batch_idx: 190 |  Loss: (0.0066) | Acc: (99.97%) (24441/24448)\n",
            "Epoch: 130 | Batch_idx: 200 |  Loss: (0.0065) | Acc: (99.97%) (25721/25728)\n",
            "Epoch: 130 | Batch_idx: 210 |  Loss: (0.0066) | Acc: (99.97%) (27001/27008)\n",
            "Epoch: 130 | Batch_idx: 220 |  Loss: (0.0066) | Acc: (99.97%) (28280/28288)\n",
            "Epoch: 130 | Batch_idx: 230 |  Loss: (0.0066) | Acc: (99.97%) (29560/29568)\n",
            "Epoch: 130 | Batch_idx: 240 |  Loss: (0.0065) | Acc: (99.97%) (30840/30848)\n",
            "Epoch: 130 | Batch_idx: 250 |  Loss: (0.0065) | Acc: (99.98%) (32120/32128)\n",
            "Epoch: 130 | Batch_idx: 260 |  Loss: (0.0066) | Acc: (99.97%) (33399/33408)\n",
            "Epoch: 130 | Batch_idx: 270 |  Loss: (0.0066) | Acc: (99.97%) (34679/34688)\n",
            "Epoch: 130 | Batch_idx: 280 |  Loss: (0.0066) | Acc: (99.97%) (35958/35968)\n",
            "Epoch: 130 | Batch_idx: 290 |  Loss: (0.0066) | Acc: (99.97%) (37237/37248)\n",
            "Epoch: 130 | Batch_idx: 300 |  Loss: (0.0066) | Acc: (99.97%) (38516/38528)\n",
            "Epoch: 130 | Batch_idx: 310 |  Loss: (0.0066) | Acc: (99.97%) (39796/39808)\n",
            "Epoch: 130 | Batch_idx: 320 |  Loss: (0.0065) | Acc: (99.97%) (41076/41088)\n",
            "Epoch: 130 | Batch_idx: 330 |  Loss: (0.0065) | Acc: (99.97%) (42356/42368)\n",
            "Epoch: 130 | Batch_idx: 340 |  Loss: (0.0065) | Acc: (99.97%) (43635/43648)\n",
            "Epoch: 130 | Batch_idx: 350 |  Loss: (0.0066) | Acc: (99.97%) (44914/44928)\n",
            "Epoch: 130 | Batch_idx: 360 |  Loss: (0.0066) | Acc: (99.97%) (46192/46208)\n",
            "Epoch: 130 | Batch_idx: 370 |  Loss: (0.0066) | Acc: (99.97%) (47472/47488)\n",
            "Epoch: 130 | Batch_idx: 380 |  Loss: (0.0065) | Acc: (99.97%) (48752/48768)\n",
            "Epoch: 130 | Batch_idx: 390 |  Loss: (0.0065) | Acc: (99.97%) (49983/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2583) | Acc: (92.94%) (9294/10000)\n",
            "Epoch: 131 | Batch_idx: 0 |  Loss: (0.0047) | Acc: (100.00%) (128/128)\n",
            "Epoch: 131 | Batch_idx: 10 |  Loss: (0.0081) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 131 | Batch_idx: 20 |  Loss: (0.0077) | Acc: (99.89%) (2685/2688)\n",
            "Epoch: 131 | Batch_idx: 30 |  Loss: (0.0074) | Acc: (99.90%) (3964/3968)\n",
            "Epoch: 131 | Batch_idx: 40 |  Loss: (0.0074) | Acc: (99.92%) (5244/5248)\n",
            "Epoch: 131 | Batch_idx: 50 |  Loss: (0.0071) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 131 | Batch_idx: 60 |  Loss: (0.0074) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 131 | Batch_idx: 70 |  Loss: (0.0072) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 131 | Batch_idx: 80 |  Loss: (0.0071) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 131 | Batch_idx: 90 |  Loss: (0.0074) | Acc: (99.95%) (11642/11648)\n",
            "Epoch: 131 | Batch_idx: 100 |  Loss: (0.0073) | Acc: (99.95%) (12922/12928)\n",
            "Epoch: 131 | Batch_idx: 110 |  Loss: (0.0073) | Acc: (99.95%) (14201/14208)\n",
            "Epoch: 131 | Batch_idx: 120 |  Loss: (0.0075) | Acc: (99.94%) (15479/15488)\n",
            "Epoch: 131 | Batch_idx: 130 |  Loss: (0.0075) | Acc: (99.94%) (16758/16768)\n",
            "Epoch: 131 | Batch_idx: 140 |  Loss: (0.0074) | Acc: (99.93%) (18036/18048)\n",
            "Epoch: 131 | Batch_idx: 150 |  Loss: (0.0075) | Acc: (99.93%) (19314/19328)\n",
            "Epoch: 131 | Batch_idx: 160 |  Loss: (0.0074) | Acc: (99.93%) (20594/20608)\n",
            "Epoch: 131 | Batch_idx: 170 |  Loss: (0.0074) | Acc: (99.94%) (21874/21888)\n",
            "Epoch: 131 | Batch_idx: 180 |  Loss: (0.0073) | Acc: (99.94%) (23153/23168)\n",
            "Epoch: 131 | Batch_idx: 190 |  Loss: (0.0074) | Acc: (99.93%) (24431/24448)\n",
            "Epoch: 131 | Batch_idx: 200 |  Loss: (0.0073) | Acc: (99.93%) (25711/25728)\n",
            "Epoch: 131 | Batch_idx: 210 |  Loss: (0.0073) | Acc: (99.93%) (26990/27008)\n",
            "Epoch: 131 | Batch_idx: 220 |  Loss: (0.0072) | Acc: (99.94%) (28270/28288)\n",
            "Epoch: 131 | Batch_idx: 230 |  Loss: (0.0072) | Acc: (99.94%) (29549/29568)\n",
            "Epoch: 131 | Batch_idx: 240 |  Loss: (0.0072) | Acc: (99.94%) (30828/30848)\n",
            "Epoch: 131 | Batch_idx: 250 |  Loss: (0.0072) | Acc: (99.93%) (32107/32128)\n",
            "Epoch: 131 | Batch_idx: 260 |  Loss: (0.0072) | Acc: (99.94%) (33387/33408)\n",
            "Epoch: 131 | Batch_idx: 270 |  Loss: (0.0072) | Acc: (99.94%) (34666/34688)\n",
            "Epoch: 131 | Batch_idx: 280 |  Loss: (0.0072) | Acc: (99.94%) (35946/35968)\n",
            "Epoch: 131 | Batch_idx: 290 |  Loss: (0.0072) | Acc: (99.94%) (37226/37248)\n",
            "Epoch: 131 | Batch_idx: 300 |  Loss: (0.0072) | Acc: (99.94%) (38506/38528)\n",
            "Epoch: 131 | Batch_idx: 310 |  Loss: (0.0072) | Acc: (99.94%) (39786/39808)\n",
            "Epoch: 131 | Batch_idx: 320 |  Loss: (0.0072) | Acc: (99.94%) (41065/41088)\n",
            "Epoch: 131 | Batch_idx: 330 |  Loss: (0.0072) | Acc: (99.95%) (42345/42368)\n",
            "Epoch: 131 | Batch_idx: 340 |  Loss: (0.0072) | Acc: (99.94%) (43623/43648)\n",
            "Epoch: 131 | Batch_idx: 350 |  Loss: (0.0072) | Acc: (99.94%) (44903/44928)\n",
            "Epoch: 131 | Batch_idx: 360 |  Loss: (0.0072) | Acc: (99.95%) (46183/46208)\n",
            "Epoch: 131 | Batch_idx: 370 |  Loss: (0.0071) | Acc: (99.95%) (47463/47488)\n",
            "Epoch: 131 | Batch_idx: 380 |  Loss: (0.0071) | Acc: (99.95%) (48742/48768)\n",
            "Epoch: 131 | Batch_idx: 390 |  Loss: (0.0072) | Acc: (99.95%) (49973/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2585) | Acc: (92.98%) (9298/10000)\n",
            "Epoch: 132 | Batch_idx: 0 |  Loss: (0.0028) | Acc: (100.00%) (128/128)\n",
            "Epoch: 132 | Batch_idx: 10 |  Loss: (0.0059) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 132 | Batch_idx: 20 |  Loss: (0.0063) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 132 | Batch_idx: 30 |  Loss: (0.0064) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 132 | Batch_idx: 40 |  Loss: (0.0067) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 132 | Batch_idx: 50 |  Loss: (0.0070) | Acc: (99.97%) (6526/6528)\n",
            "Epoch: 132 | Batch_idx: 60 |  Loss: (0.0069) | Acc: (99.97%) (7806/7808)\n",
            "Epoch: 132 | Batch_idx: 70 |  Loss: (0.0073) | Acc: (99.97%) (9085/9088)\n",
            "Epoch: 132 | Batch_idx: 80 |  Loss: (0.0072) | Acc: (99.97%) (10365/10368)\n",
            "Epoch: 132 | Batch_idx: 90 |  Loss: (0.0072) | Acc: (99.97%) (11645/11648)\n",
            "Epoch: 132 | Batch_idx: 100 |  Loss: (0.0071) | Acc: (99.97%) (12924/12928)\n",
            "Epoch: 132 | Batch_idx: 110 |  Loss: (0.0071) | Acc: (99.97%) (14204/14208)\n",
            "Epoch: 132 | Batch_idx: 120 |  Loss: (0.0069) | Acc: (99.97%) (15484/15488)\n",
            "Epoch: 132 | Batch_idx: 130 |  Loss: (0.0069) | Acc: (99.98%) (16764/16768)\n",
            "Epoch: 132 | Batch_idx: 140 |  Loss: (0.0068) | Acc: (99.98%) (18044/18048)\n",
            "Epoch: 132 | Batch_idx: 150 |  Loss: (0.0067) | Acc: (99.98%) (19324/19328)\n",
            "Epoch: 132 | Batch_idx: 160 |  Loss: (0.0068) | Acc: (99.97%) (20602/20608)\n",
            "Epoch: 132 | Batch_idx: 170 |  Loss: (0.0068) | Acc: (99.97%) (21881/21888)\n",
            "Epoch: 132 | Batch_idx: 180 |  Loss: (0.0067) | Acc: (99.97%) (23161/23168)\n",
            "Epoch: 132 | Batch_idx: 190 |  Loss: (0.0068) | Acc: (99.97%) (24440/24448)\n",
            "Epoch: 132 | Batch_idx: 200 |  Loss: (0.0067) | Acc: (99.97%) (25720/25728)\n",
            "Epoch: 132 | Batch_idx: 210 |  Loss: (0.0066) | Acc: (99.97%) (27000/27008)\n",
            "Epoch: 132 | Batch_idx: 220 |  Loss: (0.0066) | Acc: (99.97%) (28280/28288)\n",
            "Epoch: 132 | Batch_idx: 230 |  Loss: (0.0067) | Acc: (99.97%) (29559/29568)\n",
            "Epoch: 132 | Batch_idx: 240 |  Loss: (0.0067) | Acc: (99.97%) (30838/30848)\n",
            "Epoch: 132 | Batch_idx: 250 |  Loss: (0.0067) | Acc: (99.97%) (32117/32128)\n",
            "Epoch: 132 | Batch_idx: 260 |  Loss: (0.0067) | Acc: (99.97%) (33397/33408)\n",
            "Epoch: 132 | Batch_idx: 270 |  Loss: (0.0067) | Acc: (99.97%) (34677/34688)\n",
            "Epoch: 132 | Batch_idx: 280 |  Loss: (0.0068) | Acc: (99.97%) (35956/35968)\n",
            "Epoch: 132 | Batch_idx: 290 |  Loss: (0.0067) | Acc: (99.97%) (37236/37248)\n",
            "Epoch: 132 | Batch_idx: 300 |  Loss: (0.0068) | Acc: (99.96%) (38514/38528)\n",
            "Epoch: 132 | Batch_idx: 310 |  Loss: (0.0068) | Acc: (99.96%) (39793/39808)\n",
            "Epoch: 132 | Batch_idx: 320 |  Loss: (0.0068) | Acc: (99.96%) (41073/41088)\n",
            "Epoch: 132 | Batch_idx: 330 |  Loss: (0.0068) | Acc: (99.96%) (42352/42368)\n",
            "Epoch: 132 | Batch_idx: 340 |  Loss: (0.0068) | Acc: (99.96%) (43632/43648)\n",
            "Epoch: 132 | Batch_idx: 350 |  Loss: (0.0068) | Acc: (99.96%) (44912/44928)\n",
            "Epoch: 132 | Batch_idx: 360 |  Loss: (0.0068) | Acc: (99.97%) (46192/46208)\n",
            "Epoch: 132 | Batch_idx: 370 |  Loss: (0.0068) | Acc: (99.97%) (47472/47488)\n",
            "Epoch: 132 | Batch_idx: 380 |  Loss: (0.0068) | Acc: (99.96%) (48749/48768)\n",
            "Epoch: 132 | Batch_idx: 390 |  Loss: (0.0069) | Acc: (99.96%) (49981/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2577) | Acc: (92.96%) (9296/10000)\n",
            "Epoch: 133 | Batch_idx: 0 |  Loss: (0.0062) | Acc: (100.00%) (128/128)\n",
            "Epoch: 133 | Batch_idx: 10 |  Loss: (0.0074) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 133 | Batch_idx: 20 |  Loss: (0.0072) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 133 | Batch_idx: 30 |  Loss: (0.0069) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 133 | Batch_idx: 40 |  Loss: (0.0066) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 133 | Batch_idx: 50 |  Loss: (0.0067) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 133 | Batch_idx: 60 |  Loss: (0.0065) | Acc: (99.97%) (7806/7808)\n",
            "Epoch: 133 | Batch_idx: 70 |  Loss: (0.0063) | Acc: (99.98%) (9086/9088)\n",
            "Epoch: 133 | Batch_idx: 80 |  Loss: (0.0063) | Acc: (99.98%) (10366/10368)\n",
            "Epoch: 133 | Batch_idx: 90 |  Loss: (0.0063) | Acc: (99.97%) (11645/11648)\n",
            "Epoch: 133 | Batch_idx: 100 |  Loss: (0.0065) | Acc: (99.96%) (12923/12928)\n",
            "Epoch: 133 | Batch_idx: 110 |  Loss: (0.0064) | Acc: (99.96%) (14202/14208)\n",
            "Epoch: 133 | Batch_idx: 120 |  Loss: (0.0064) | Acc: (99.96%) (15482/15488)\n",
            "Epoch: 133 | Batch_idx: 130 |  Loss: (0.0064) | Acc: (99.96%) (16761/16768)\n",
            "Epoch: 133 | Batch_idx: 140 |  Loss: (0.0064) | Acc: (99.96%) (18041/18048)\n",
            "Epoch: 133 | Batch_idx: 150 |  Loss: (0.0065) | Acc: (99.96%) (19321/19328)\n",
            "Epoch: 133 | Batch_idx: 160 |  Loss: (0.0064) | Acc: (99.97%) (20601/20608)\n",
            "Epoch: 133 | Batch_idx: 170 |  Loss: (0.0064) | Acc: (99.97%) (21881/21888)\n",
            "Epoch: 133 | Batch_idx: 180 |  Loss: (0.0064) | Acc: (99.97%) (23161/23168)\n",
            "Epoch: 133 | Batch_idx: 190 |  Loss: (0.0064) | Acc: (99.97%) (24441/24448)\n",
            "Epoch: 133 | Batch_idx: 200 |  Loss: (0.0064) | Acc: (99.97%) (25721/25728)\n",
            "Epoch: 133 | Batch_idx: 210 |  Loss: (0.0065) | Acc: (99.97%) (27001/27008)\n",
            "Epoch: 133 | Batch_idx: 220 |  Loss: (0.0065) | Acc: (99.97%) (28280/28288)\n",
            "Epoch: 133 | Batch_idx: 230 |  Loss: (0.0065) | Acc: (99.97%) (29560/29568)\n",
            "Epoch: 133 | Batch_idx: 240 |  Loss: (0.0065) | Acc: (99.97%) (30839/30848)\n",
            "Epoch: 133 | Batch_idx: 250 |  Loss: (0.0065) | Acc: (99.97%) (32118/32128)\n",
            "Epoch: 133 | Batch_idx: 260 |  Loss: (0.0065) | Acc: (99.97%) (33398/33408)\n",
            "Epoch: 133 | Batch_idx: 270 |  Loss: (0.0066) | Acc: (99.97%) (34676/34688)\n",
            "Epoch: 133 | Batch_idx: 280 |  Loss: (0.0066) | Acc: (99.97%) (35956/35968)\n",
            "Epoch: 133 | Batch_idx: 290 |  Loss: (0.0066) | Acc: (99.97%) (37235/37248)\n",
            "Epoch: 133 | Batch_idx: 300 |  Loss: (0.0066) | Acc: (99.97%) (38515/38528)\n",
            "Epoch: 133 | Batch_idx: 310 |  Loss: (0.0066) | Acc: (99.96%) (39794/39808)\n",
            "Epoch: 133 | Batch_idx: 320 |  Loss: (0.0066) | Acc: (99.97%) (41074/41088)\n",
            "Epoch: 133 | Batch_idx: 330 |  Loss: (0.0067) | Acc: (99.96%) (42352/42368)\n",
            "Epoch: 133 | Batch_idx: 340 |  Loss: (0.0067) | Acc: (99.96%) (43632/43648)\n",
            "Epoch: 133 | Batch_idx: 350 |  Loss: (0.0066) | Acc: (99.96%) (44912/44928)\n",
            "Epoch: 133 | Batch_idx: 360 |  Loss: (0.0066) | Acc: (99.97%) (46192/46208)\n",
            "Epoch: 133 | Batch_idx: 370 |  Loss: (0.0066) | Acc: (99.97%) (47472/47488)\n",
            "Epoch: 133 | Batch_idx: 380 |  Loss: (0.0066) | Acc: (99.97%) (48752/48768)\n",
            "Epoch: 133 | Batch_idx: 390 |  Loss: (0.0066) | Acc: (99.97%) (49984/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2584) | Acc: (93.01%) (9301/10000)\n",
            "Epoch: 134 | Batch_idx: 0 |  Loss: (0.0125) | Acc: (100.00%) (128/128)\n",
            "Epoch: 134 | Batch_idx: 10 |  Loss: (0.0057) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 134 | Batch_idx: 20 |  Loss: (0.0057) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 134 | Batch_idx: 30 |  Loss: (0.0059) | Acc: (100.00%) (3968/3968)\n",
            "Epoch: 134 | Batch_idx: 40 |  Loss: (0.0059) | Acc: (100.00%) (5248/5248)\n",
            "Epoch: 134 | Batch_idx: 50 |  Loss: (0.0062) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 134 | Batch_idx: 60 |  Loss: (0.0062) | Acc: (99.99%) (7807/7808)\n",
            "Epoch: 134 | Batch_idx: 70 |  Loss: (0.0061) | Acc: (99.99%) (9087/9088)\n",
            "Epoch: 134 | Batch_idx: 80 |  Loss: (0.0061) | Acc: (99.99%) (10367/10368)\n",
            "Epoch: 134 | Batch_idx: 90 |  Loss: (0.0060) | Acc: (99.99%) (11647/11648)\n",
            "Epoch: 134 | Batch_idx: 100 |  Loss: (0.0060) | Acc: (99.99%) (12927/12928)\n",
            "Epoch: 134 | Batch_idx: 110 |  Loss: (0.0061) | Acc: (99.99%) (14207/14208)\n",
            "Epoch: 134 | Batch_idx: 120 |  Loss: (0.0062) | Acc: (99.99%) (15486/15488)\n",
            "Epoch: 134 | Batch_idx: 130 |  Loss: (0.0063) | Acc: (99.98%) (16765/16768)\n",
            "Epoch: 134 | Batch_idx: 140 |  Loss: (0.0065) | Acc: (99.96%) (18041/18048)\n",
            "Epoch: 134 | Batch_idx: 150 |  Loss: (0.0065) | Acc: (99.96%) (19321/19328)\n",
            "Epoch: 134 | Batch_idx: 160 |  Loss: (0.0066) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 134 | Batch_idx: 170 |  Loss: (0.0068) | Acc: (99.95%) (21878/21888)\n",
            "Epoch: 134 | Batch_idx: 180 |  Loss: (0.0068) | Acc: (99.96%) (23158/23168)\n",
            "Epoch: 134 | Batch_idx: 190 |  Loss: (0.0068) | Acc: (99.96%) (24438/24448)\n",
            "Epoch: 134 | Batch_idx: 200 |  Loss: (0.0068) | Acc: (99.96%) (25717/25728)\n",
            "Epoch: 134 | Batch_idx: 210 |  Loss: (0.0068) | Acc: (99.96%) (26997/27008)\n",
            "Epoch: 134 | Batch_idx: 220 |  Loss: (0.0068) | Acc: (99.96%) (28276/28288)\n",
            "Epoch: 134 | Batch_idx: 230 |  Loss: (0.0068) | Acc: (99.96%) (29555/29568)\n",
            "Epoch: 134 | Batch_idx: 240 |  Loss: (0.0068) | Acc: (99.96%) (30835/30848)\n",
            "Epoch: 134 | Batch_idx: 250 |  Loss: (0.0068) | Acc: (99.96%) (32114/32128)\n",
            "Epoch: 134 | Batch_idx: 260 |  Loss: (0.0068) | Acc: (99.96%) (33394/33408)\n",
            "Epoch: 134 | Batch_idx: 270 |  Loss: (0.0068) | Acc: (99.96%) (34674/34688)\n",
            "Epoch: 134 | Batch_idx: 280 |  Loss: (0.0068) | Acc: (99.96%) (35953/35968)\n",
            "Epoch: 134 | Batch_idx: 290 |  Loss: (0.0068) | Acc: (99.96%) (37233/37248)\n",
            "Epoch: 134 | Batch_idx: 300 |  Loss: (0.0068) | Acc: (99.96%) (38513/38528)\n",
            "Epoch: 134 | Batch_idx: 310 |  Loss: (0.0068) | Acc: (99.96%) (39793/39808)\n",
            "Epoch: 134 | Batch_idx: 320 |  Loss: (0.0068) | Acc: (99.96%) (41073/41088)\n",
            "Epoch: 134 | Batch_idx: 330 |  Loss: (0.0068) | Acc: (99.96%) (42353/42368)\n",
            "Epoch: 134 | Batch_idx: 340 |  Loss: (0.0068) | Acc: (99.97%) (43633/43648)\n",
            "Epoch: 134 | Batch_idx: 350 |  Loss: (0.0068) | Acc: (99.96%) (44912/44928)\n",
            "Epoch: 134 | Batch_idx: 360 |  Loss: (0.0068) | Acc: (99.97%) (46192/46208)\n",
            "Epoch: 134 | Batch_idx: 370 |  Loss: (0.0067) | Acc: (99.97%) (47472/47488)\n",
            "Epoch: 134 | Batch_idx: 380 |  Loss: (0.0067) | Acc: (99.97%) (48752/48768)\n",
            "Epoch: 134 | Batch_idx: 390 |  Loss: (0.0067) | Acc: (99.97%) (49984/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2577) | Acc: (92.98%) (9298/10000)\n",
            "Epoch: 135 | Batch_idx: 0 |  Loss: (0.0042) | Acc: (100.00%) (128/128)\n",
            "Epoch: 135 | Batch_idx: 10 |  Loss: (0.0061) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 135 | Batch_idx: 20 |  Loss: (0.0055) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 135 | Batch_idx: 30 |  Loss: (0.0063) | Acc: (100.00%) (3968/3968)\n",
            "Epoch: 135 | Batch_idx: 40 |  Loss: (0.0066) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 135 | Batch_idx: 50 |  Loss: (0.0067) | Acc: (99.95%) (6525/6528)\n",
            "Epoch: 135 | Batch_idx: 60 |  Loss: (0.0066) | Acc: (99.96%) (7805/7808)\n",
            "Epoch: 135 | Batch_idx: 70 |  Loss: (0.0066) | Acc: (99.97%) (9085/9088)\n",
            "Epoch: 135 | Batch_idx: 80 |  Loss: (0.0065) | Acc: (99.97%) (10365/10368)\n",
            "Epoch: 135 | Batch_idx: 90 |  Loss: (0.0065) | Acc: (99.97%) (11645/11648)\n",
            "Epoch: 135 | Batch_idx: 100 |  Loss: (0.0065) | Acc: (99.98%) (12925/12928)\n",
            "Epoch: 135 | Batch_idx: 110 |  Loss: (0.0065) | Acc: (99.98%) (14205/14208)\n",
            "Epoch: 135 | Batch_idx: 120 |  Loss: (0.0066) | Acc: (99.97%) (15483/15488)\n",
            "Epoch: 135 | Batch_idx: 130 |  Loss: (0.0065) | Acc: (99.97%) (16763/16768)\n",
            "Epoch: 135 | Batch_idx: 140 |  Loss: (0.0067) | Acc: (99.96%) (18041/18048)\n",
            "Epoch: 135 | Batch_idx: 150 |  Loss: (0.0066) | Acc: (99.96%) (19321/19328)\n",
            "Epoch: 135 | Batch_idx: 160 |  Loss: (0.0066) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 135 | Batch_idx: 170 |  Loss: (0.0066) | Acc: (99.96%) (21880/21888)\n",
            "Epoch: 135 | Batch_idx: 180 |  Loss: (0.0068) | Acc: (99.96%) (23159/23168)\n",
            "Epoch: 135 | Batch_idx: 190 |  Loss: (0.0067) | Acc: (99.96%) (24439/24448)\n",
            "Epoch: 135 | Batch_idx: 200 |  Loss: (0.0068) | Acc: (99.96%) (25717/25728)\n",
            "Epoch: 135 | Batch_idx: 210 |  Loss: (0.0068) | Acc: (99.96%) (26997/27008)\n",
            "Epoch: 135 | Batch_idx: 220 |  Loss: (0.0068) | Acc: (99.96%) (28277/28288)\n",
            "Epoch: 135 | Batch_idx: 230 |  Loss: (0.0068) | Acc: (99.96%) (29555/29568)\n",
            "Epoch: 135 | Batch_idx: 240 |  Loss: (0.0068) | Acc: (99.96%) (30835/30848)\n",
            "Epoch: 135 | Batch_idx: 250 |  Loss: (0.0068) | Acc: (99.96%) (32115/32128)\n",
            "Epoch: 135 | Batch_idx: 260 |  Loss: (0.0068) | Acc: (99.96%) (33394/33408)\n",
            "Epoch: 135 | Batch_idx: 270 |  Loss: (0.0068) | Acc: (99.96%) (34674/34688)\n",
            "Epoch: 135 | Batch_idx: 280 |  Loss: (0.0069) | Acc: (99.96%) (35953/35968)\n",
            "Epoch: 135 | Batch_idx: 290 |  Loss: (0.0069) | Acc: (99.96%) (37232/37248)\n",
            "Epoch: 135 | Batch_idx: 300 |  Loss: (0.0068) | Acc: (99.96%) (38512/38528)\n",
            "Epoch: 135 | Batch_idx: 310 |  Loss: (0.0069) | Acc: (99.96%) (39791/39808)\n",
            "Epoch: 135 | Batch_idx: 320 |  Loss: (0.0068) | Acc: (99.96%) (41071/41088)\n",
            "Epoch: 135 | Batch_idx: 330 |  Loss: (0.0068) | Acc: (99.96%) (42350/42368)\n",
            "Epoch: 135 | Batch_idx: 340 |  Loss: (0.0069) | Acc: (99.95%) (43627/43648)\n",
            "Epoch: 135 | Batch_idx: 350 |  Loss: (0.0069) | Acc: (99.95%) (44907/44928)\n",
            "Epoch: 135 | Batch_idx: 360 |  Loss: (0.0069) | Acc: (99.95%) (46186/46208)\n",
            "Epoch: 135 | Batch_idx: 370 |  Loss: (0.0069) | Acc: (99.95%) (47466/47488)\n",
            "Epoch: 135 | Batch_idx: 380 |  Loss: (0.0069) | Acc: (99.95%) (48745/48768)\n",
            "Epoch: 135 | Batch_idx: 390 |  Loss: (0.0069) | Acc: (99.95%) (49977/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2566) | Acc: (93.04%) (9304/10000)\n",
            "Epoch: 136 | Batch_idx: 0 |  Loss: (0.0038) | Acc: (100.00%) (128/128)\n",
            "Epoch: 136 | Batch_idx: 10 |  Loss: (0.0069) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 136 | Batch_idx: 20 |  Loss: (0.0069) | Acc: (99.89%) (2685/2688)\n",
            "Epoch: 136 | Batch_idx: 30 |  Loss: (0.0068) | Acc: (99.92%) (3965/3968)\n",
            "Epoch: 136 | Batch_idx: 40 |  Loss: (0.0070) | Acc: (99.92%) (5244/5248)\n",
            "Epoch: 136 | Batch_idx: 50 |  Loss: (0.0069) | Acc: (99.92%) (6523/6528)\n",
            "Epoch: 136 | Batch_idx: 60 |  Loss: (0.0069) | Acc: (99.92%) (7802/7808)\n",
            "Epoch: 136 | Batch_idx: 70 |  Loss: (0.0067) | Acc: (99.93%) (9082/9088)\n",
            "Epoch: 136 | Batch_idx: 80 |  Loss: (0.0066) | Acc: (99.94%) (10362/10368)\n",
            "Epoch: 136 | Batch_idx: 90 |  Loss: (0.0066) | Acc: (99.95%) (11642/11648)\n",
            "Epoch: 136 | Batch_idx: 100 |  Loss: (0.0066) | Acc: (99.95%) (12921/12928)\n",
            "Epoch: 136 | Batch_idx: 110 |  Loss: (0.0066) | Acc: (99.95%) (14201/14208)\n",
            "Epoch: 136 | Batch_idx: 120 |  Loss: (0.0066) | Acc: (99.95%) (15480/15488)\n",
            "Epoch: 136 | Batch_idx: 130 |  Loss: (0.0066) | Acc: (99.95%) (16760/16768)\n",
            "Epoch: 136 | Batch_idx: 140 |  Loss: (0.0066) | Acc: (99.96%) (18040/18048)\n",
            "Epoch: 136 | Batch_idx: 150 |  Loss: (0.0066) | Acc: (99.96%) (19320/19328)\n",
            "Epoch: 136 | Batch_idx: 160 |  Loss: (0.0066) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 136 | Batch_idx: 170 |  Loss: (0.0067) | Acc: (99.95%) (21878/21888)\n",
            "Epoch: 136 | Batch_idx: 180 |  Loss: (0.0067) | Acc: (99.96%) (23158/23168)\n",
            "Epoch: 136 | Batch_idx: 190 |  Loss: (0.0067) | Acc: (99.96%) (24438/24448)\n",
            "Epoch: 136 | Batch_idx: 200 |  Loss: (0.0068) | Acc: (99.96%) (25717/25728)\n",
            "Epoch: 136 | Batch_idx: 210 |  Loss: (0.0067) | Acc: (99.96%) (26997/27008)\n",
            "Epoch: 136 | Batch_idx: 220 |  Loss: (0.0067) | Acc: (99.96%) (28277/28288)\n",
            "Epoch: 136 | Batch_idx: 230 |  Loss: (0.0068) | Acc: (99.96%) (29556/29568)\n",
            "Epoch: 136 | Batch_idx: 240 |  Loss: (0.0068) | Acc: (99.96%) (30836/30848)\n",
            "Epoch: 136 | Batch_idx: 250 |  Loss: (0.0067) | Acc: (99.96%) (32116/32128)\n",
            "Epoch: 136 | Batch_idx: 260 |  Loss: (0.0067) | Acc: (99.96%) (33396/33408)\n",
            "Epoch: 136 | Batch_idx: 270 |  Loss: (0.0066) | Acc: (99.97%) (34676/34688)\n",
            "Epoch: 136 | Batch_idx: 280 |  Loss: (0.0066) | Acc: (99.97%) (35956/35968)\n",
            "Epoch: 136 | Batch_idx: 290 |  Loss: (0.0066) | Acc: (99.97%) (37235/37248)\n",
            "Epoch: 136 | Batch_idx: 300 |  Loss: (0.0066) | Acc: (99.97%) (38515/38528)\n",
            "Epoch: 136 | Batch_idx: 310 |  Loss: (0.0066) | Acc: (99.96%) (39794/39808)\n",
            "Epoch: 136 | Batch_idx: 320 |  Loss: (0.0066) | Acc: (99.97%) (41074/41088)\n",
            "Epoch: 136 | Batch_idx: 330 |  Loss: (0.0066) | Acc: (99.97%) (42354/42368)\n",
            "Epoch: 136 | Batch_idx: 340 |  Loss: (0.0066) | Acc: (99.97%) (43634/43648)\n",
            "Epoch: 136 | Batch_idx: 350 |  Loss: (0.0066) | Acc: (99.97%) (44913/44928)\n",
            "Epoch: 136 | Batch_idx: 360 |  Loss: (0.0066) | Acc: (99.97%) (46192/46208)\n",
            "Epoch: 136 | Batch_idx: 370 |  Loss: (0.0066) | Acc: (99.96%) (47471/47488)\n",
            "Epoch: 136 | Batch_idx: 380 |  Loss: (0.0066) | Acc: (99.97%) (48751/48768)\n",
            "Epoch: 136 | Batch_idx: 390 |  Loss: (0.0067) | Acc: (99.96%) (49982/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2599) | Acc: (93.04%) (9304/10000)\n",
            "Epoch: 137 | Batch_idx: 0 |  Loss: (0.0034) | Acc: (100.00%) (128/128)\n",
            "Epoch: 137 | Batch_idx: 10 |  Loss: (0.0057) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 137 | Batch_idx: 20 |  Loss: (0.0061) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 137 | Batch_idx: 30 |  Loss: (0.0060) | Acc: (100.00%) (3968/3968)\n",
            "Epoch: 137 | Batch_idx: 40 |  Loss: (0.0064) | Acc: (100.00%) (5248/5248)\n",
            "Epoch: 137 | Batch_idx: 50 |  Loss: (0.0067) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 137 | Batch_idx: 60 |  Loss: (0.0065) | Acc: (99.99%) (7807/7808)\n",
            "Epoch: 137 | Batch_idx: 70 |  Loss: (0.0065) | Acc: (99.99%) (9087/9088)\n",
            "Epoch: 137 | Batch_idx: 80 |  Loss: (0.0067) | Acc: (99.99%) (10367/10368)\n",
            "Epoch: 137 | Batch_idx: 90 |  Loss: (0.0066) | Acc: (99.99%) (11647/11648)\n",
            "Epoch: 137 | Batch_idx: 100 |  Loss: (0.0065) | Acc: (99.99%) (12927/12928)\n",
            "Epoch: 137 | Batch_idx: 110 |  Loss: (0.0066) | Acc: (99.99%) (14206/14208)\n",
            "Epoch: 137 | Batch_idx: 120 |  Loss: (0.0065) | Acc: (99.99%) (15486/15488)\n",
            "Epoch: 137 | Batch_idx: 130 |  Loss: (0.0065) | Acc: (99.98%) (16765/16768)\n",
            "Epoch: 137 | Batch_idx: 140 |  Loss: (0.0066) | Acc: (99.98%) (18045/18048)\n",
            "Epoch: 137 | Batch_idx: 150 |  Loss: (0.0065) | Acc: (99.98%) (19325/19328)\n",
            "Epoch: 137 | Batch_idx: 160 |  Loss: (0.0065) | Acc: (99.99%) (20605/20608)\n",
            "Epoch: 137 | Batch_idx: 170 |  Loss: (0.0066) | Acc: (99.98%) (21884/21888)\n",
            "Epoch: 137 | Batch_idx: 180 |  Loss: (0.0067) | Acc: (99.98%) (23164/23168)\n",
            "Epoch: 137 | Batch_idx: 190 |  Loss: (0.0067) | Acc: (99.98%) (24444/24448)\n",
            "Epoch: 137 | Batch_idx: 200 |  Loss: (0.0067) | Acc: (99.98%) (25724/25728)\n",
            "Epoch: 137 | Batch_idx: 210 |  Loss: (0.0066) | Acc: (99.99%) (27004/27008)\n",
            "Epoch: 137 | Batch_idx: 220 |  Loss: (0.0066) | Acc: (99.99%) (28284/28288)\n",
            "Epoch: 137 | Batch_idx: 230 |  Loss: (0.0067) | Acc: (99.98%) (29563/29568)\n",
            "Epoch: 137 | Batch_idx: 240 |  Loss: (0.0066) | Acc: (99.98%) (30843/30848)\n",
            "Epoch: 137 | Batch_idx: 250 |  Loss: (0.0067) | Acc: (99.98%) (32122/32128)\n",
            "Epoch: 137 | Batch_idx: 260 |  Loss: (0.0066) | Acc: (99.98%) (33402/33408)\n",
            "Epoch: 137 | Batch_idx: 270 |  Loss: (0.0066) | Acc: (99.98%) (34682/34688)\n",
            "Epoch: 137 | Batch_idx: 280 |  Loss: (0.0066) | Acc: (99.98%) (35962/35968)\n",
            "Epoch: 137 | Batch_idx: 290 |  Loss: (0.0067) | Acc: (99.98%) (37242/37248)\n",
            "Epoch: 137 | Batch_idx: 300 |  Loss: (0.0066) | Acc: (99.98%) (38522/38528)\n",
            "Epoch: 137 | Batch_idx: 310 |  Loss: (0.0066) | Acc: (99.98%) (39802/39808)\n",
            "Epoch: 137 | Batch_idx: 320 |  Loss: (0.0067) | Acc: (99.98%) (41081/41088)\n",
            "Epoch: 137 | Batch_idx: 330 |  Loss: (0.0067) | Acc: (99.98%) (42360/42368)\n",
            "Epoch: 137 | Batch_idx: 340 |  Loss: (0.0067) | Acc: (99.98%) (43640/43648)\n",
            "Epoch: 137 | Batch_idx: 350 |  Loss: (0.0066) | Acc: (99.98%) (44920/44928)\n",
            "Epoch: 137 | Batch_idx: 360 |  Loss: (0.0066) | Acc: (99.98%) (46200/46208)\n",
            "Epoch: 137 | Batch_idx: 370 |  Loss: (0.0066) | Acc: (99.98%) (47479/47488)\n",
            "Epoch: 137 | Batch_idx: 380 |  Loss: (0.0066) | Acc: (99.98%) (48759/48768)\n",
            "Epoch: 137 | Batch_idx: 390 |  Loss: (0.0067) | Acc: (99.98%) (49989/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2572) | Acc: (93.01%) (9301/10000)\n",
            "Epoch: 138 | Batch_idx: 0 |  Loss: (0.0044) | Acc: (100.00%) (128/128)\n",
            "Epoch: 138 | Batch_idx: 10 |  Loss: (0.0060) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 138 | Batch_idx: 20 |  Loss: (0.0063) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 138 | Batch_idx: 30 |  Loss: (0.0070) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 138 | Batch_idx: 40 |  Loss: (0.0080) | Acc: (99.90%) (5243/5248)\n",
            "Epoch: 138 | Batch_idx: 50 |  Loss: (0.0076) | Acc: (99.92%) (6523/6528)\n",
            "Epoch: 138 | Batch_idx: 60 |  Loss: (0.0074) | Acc: (99.94%) (7803/7808)\n",
            "Epoch: 138 | Batch_idx: 70 |  Loss: (0.0075) | Acc: (99.93%) (9082/9088)\n",
            "Epoch: 138 | Batch_idx: 80 |  Loss: (0.0072) | Acc: (99.94%) (10362/10368)\n",
            "Epoch: 138 | Batch_idx: 90 |  Loss: (0.0071) | Acc: (99.95%) (11642/11648)\n",
            "Epoch: 138 | Batch_idx: 100 |  Loss: (0.0069) | Acc: (99.95%) (12922/12928)\n",
            "Epoch: 138 | Batch_idx: 110 |  Loss: (0.0069) | Acc: (99.96%) (14202/14208)\n",
            "Epoch: 138 | Batch_idx: 120 |  Loss: (0.0070) | Acc: (99.96%) (15482/15488)\n",
            "Epoch: 138 | Batch_idx: 130 |  Loss: (0.0069) | Acc: (99.96%) (16762/16768)\n",
            "Epoch: 138 | Batch_idx: 140 |  Loss: (0.0068) | Acc: (99.97%) (18042/18048)\n",
            "Epoch: 138 | Batch_idx: 150 |  Loss: (0.0068) | Acc: (99.97%) (19322/19328)\n",
            "Epoch: 138 | Batch_idx: 160 |  Loss: (0.0069) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 138 | Batch_idx: 170 |  Loss: (0.0069) | Acc: (99.95%) (21878/21888)\n",
            "Epoch: 138 | Batch_idx: 180 |  Loss: (0.0068) | Acc: (99.96%) (23158/23168)\n",
            "Epoch: 138 | Batch_idx: 190 |  Loss: (0.0067) | Acc: (99.96%) (24438/24448)\n",
            "Epoch: 138 | Batch_idx: 200 |  Loss: (0.0067) | Acc: (99.96%) (25718/25728)\n",
            "Epoch: 138 | Batch_idx: 210 |  Loss: (0.0069) | Acc: (99.95%) (26995/27008)\n",
            "Epoch: 138 | Batch_idx: 220 |  Loss: (0.0070) | Acc: (99.95%) (28273/28288)\n",
            "Epoch: 138 | Batch_idx: 230 |  Loss: (0.0069) | Acc: (99.95%) (29553/29568)\n",
            "Epoch: 138 | Batch_idx: 240 |  Loss: (0.0069) | Acc: (99.95%) (30832/30848)\n",
            "Epoch: 138 | Batch_idx: 250 |  Loss: (0.0070) | Acc: (99.95%) (32112/32128)\n",
            "Epoch: 138 | Batch_idx: 260 |  Loss: (0.0070) | Acc: (99.95%) (33391/33408)\n",
            "Epoch: 138 | Batch_idx: 270 |  Loss: (0.0070) | Acc: (99.95%) (34670/34688)\n",
            "Epoch: 138 | Batch_idx: 280 |  Loss: (0.0069) | Acc: (99.95%) (35949/35968)\n",
            "Epoch: 138 | Batch_idx: 290 |  Loss: (0.0069) | Acc: (99.95%) (37228/37248)\n",
            "Epoch: 138 | Batch_idx: 300 |  Loss: (0.0069) | Acc: (99.95%) (38508/38528)\n",
            "Epoch: 138 | Batch_idx: 310 |  Loss: (0.0068) | Acc: (99.95%) (39788/39808)\n",
            "Epoch: 138 | Batch_idx: 320 |  Loss: (0.0069) | Acc: (99.95%) (41068/41088)\n",
            "Epoch: 138 | Batch_idx: 330 |  Loss: (0.0068) | Acc: (99.95%) (42348/42368)\n",
            "Epoch: 138 | Batch_idx: 340 |  Loss: (0.0068) | Acc: (99.95%) (43628/43648)\n",
            "Epoch: 138 | Batch_idx: 350 |  Loss: (0.0068) | Acc: (99.96%) (44908/44928)\n",
            "Epoch: 138 | Batch_idx: 360 |  Loss: (0.0068) | Acc: (99.95%) (46187/46208)\n",
            "Epoch: 138 | Batch_idx: 370 |  Loss: (0.0068) | Acc: (99.95%) (47466/47488)\n",
            "Epoch: 138 | Batch_idx: 380 |  Loss: (0.0068) | Acc: (99.95%) (48746/48768)\n",
            "Epoch: 138 | Batch_idx: 390 |  Loss: (0.0068) | Acc: (99.96%) (49978/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2576) | Acc: (93.06%) (9306/10000)\n",
            "Epoch: 139 | Batch_idx: 0 |  Loss: (0.0053) | Acc: (100.00%) (128/128)\n",
            "Epoch: 139 | Batch_idx: 10 |  Loss: (0.0060) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 139 | Batch_idx: 20 |  Loss: (0.0060) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 139 | Batch_idx: 30 |  Loss: (0.0063) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 139 | Batch_idx: 40 |  Loss: (0.0067) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 139 | Batch_idx: 50 |  Loss: (0.0066) | Acc: (99.97%) (6526/6528)\n",
            "Epoch: 139 | Batch_idx: 60 |  Loss: (0.0066) | Acc: (99.97%) (7806/7808)\n",
            "Epoch: 139 | Batch_idx: 70 |  Loss: (0.0067) | Acc: (99.98%) (9086/9088)\n",
            "Epoch: 139 | Batch_idx: 80 |  Loss: (0.0064) | Acc: (99.98%) (10366/10368)\n",
            "Epoch: 139 | Batch_idx: 90 |  Loss: (0.0064) | Acc: (99.98%) (11646/11648)\n",
            "Epoch: 139 | Batch_idx: 100 |  Loss: (0.0063) | Acc: (99.98%) (12926/12928)\n",
            "Epoch: 139 | Batch_idx: 110 |  Loss: (0.0063) | Acc: (99.99%) (14206/14208)\n",
            "Epoch: 139 | Batch_idx: 120 |  Loss: (0.0064) | Acc: (99.98%) (15485/15488)\n",
            "Epoch: 139 | Batch_idx: 130 |  Loss: (0.0066) | Acc: (99.98%) (16764/16768)\n",
            "Epoch: 139 | Batch_idx: 140 |  Loss: (0.0065) | Acc: (99.98%) (18044/18048)\n",
            "Epoch: 139 | Batch_idx: 150 |  Loss: (0.0064) | Acc: (99.98%) (19324/19328)\n",
            "Epoch: 139 | Batch_idx: 160 |  Loss: (0.0065) | Acc: (99.98%) (20604/20608)\n",
            "Epoch: 139 | Batch_idx: 170 |  Loss: (0.0065) | Acc: (99.98%) (21883/21888)\n",
            "Epoch: 139 | Batch_idx: 180 |  Loss: (0.0066) | Acc: (99.97%) (23162/23168)\n",
            "Epoch: 139 | Batch_idx: 190 |  Loss: (0.0065) | Acc: (99.98%) (24442/24448)\n",
            "Epoch: 139 | Batch_idx: 200 |  Loss: (0.0065) | Acc: (99.98%) (25722/25728)\n",
            "Epoch: 139 | Batch_idx: 210 |  Loss: (0.0066) | Acc: (99.98%) (27002/27008)\n",
            "Epoch: 139 | Batch_idx: 220 |  Loss: (0.0066) | Acc: (99.98%) (28281/28288)\n",
            "Epoch: 139 | Batch_idx: 230 |  Loss: (0.0066) | Acc: (99.98%) (29561/29568)\n",
            "Epoch: 139 | Batch_idx: 240 |  Loss: (0.0066) | Acc: (99.98%) (30841/30848)\n",
            "Epoch: 139 | Batch_idx: 250 |  Loss: (0.0066) | Acc: (99.98%) (32121/32128)\n",
            "Epoch: 139 | Batch_idx: 260 |  Loss: (0.0066) | Acc: (99.97%) (33399/33408)\n",
            "Epoch: 139 | Batch_idx: 270 |  Loss: (0.0065) | Acc: (99.97%) (34679/34688)\n",
            "Epoch: 139 | Batch_idx: 280 |  Loss: (0.0065) | Acc: (99.97%) (35959/35968)\n",
            "Epoch: 139 | Batch_idx: 290 |  Loss: (0.0065) | Acc: (99.97%) (37238/37248)\n",
            "Epoch: 139 | Batch_idx: 300 |  Loss: (0.0065) | Acc: (99.97%) (38518/38528)\n",
            "Epoch: 139 | Batch_idx: 310 |  Loss: (0.0065) | Acc: (99.97%) (39798/39808)\n",
            "Epoch: 139 | Batch_idx: 320 |  Loss: (0.0065) | Acc: (99.98%) (41078/41088)\n",
            "Epoch: 139 | Batch_idx: 330 |  Loss: (0.0065) | Acc: (99.97%) (42357/42368)\n",
            "Epoch: 139 | Batch_idx: 340 |  Loss: (0.0065) | Acc: (99.97%) (43637/43648)\n",
            "Epoch: 139 | Batch_idx: 350 |  Loss: (0.0066) | Acc: (99.97%) (44915/44928)\n",
            "Epoch: 139 | Batch_idx: 360 |  Loss: (0.0067) | Acc: (99.97%) (46193/46208)\n",
            "Epoch: 139 | Batch_idx: 370 |  Loss: (0.0066) | Acc: (99.97%) (47473/47488)\n",
            "Epoch: 139 | Batch_idx: 380 |  Loss: (0.0066) | Acc: (99.97%) (48753/48768)\n",
            "Epoch: 139 | Batch_idx: 390 |  Loss: (0.0066) | Acc: (99.97%) (49984/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2579) | Acc: (92.99%) (9299/10000)\n",
            "Epoch: 140 | Batch_idx: 0 |  Loss: (0.0066) | Acc: (100.00%) (128/128)\n",
            "Epoch: 140 | Batch_idx: 10 |  Loss: (0.0065) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 140 | Batch_idx: 20 |  Loss: (0.0070) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 140 | Batch_idx: 30 |  Loss: (0.0070) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 140 | Batch_idx: 40 |  Loss: (0.0068) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 140 | Batch_idx: 50 |  Loss: (0.0072) | Acc: (99.95%) (6525/6528)\n",
            "Epoch: 140 | Batch_idx: 60 |  Loss: (0.0070) | Acc: (99.96%) (7805/7808)\n",
            "Epoch: 140 | Batch_idx: 70 |  Loss: (0.0072) | Acc: (99.97%) (9085/9088)\n",
            "Epoch: 140 | Batch_idx: 80 |  Loss: (0.0072) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 140 | Batch_idx: 90 |  Loss: (0.0071) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 140 | Batch_idx: 100 |  Loss: (0.0072) | Acc: (99.96%) (12923/12928)\n",
            "Epoch: 140 | Batch_idx: 110 |  Loss: (0.0071) | Acc: (99.96%) (14203/14208)\n",
            "Epoch: 140 | Batch_idx: 120 |  Loss: (0.0071) | Acc: (99.97%) (15483/15488)\n",
            "Epoch: 140 | Batch_idx: 130 |  Loss: (0.0070) | Acc: (99.96%) (16762/16768)\n",
            "Epoch: 140 | Batch_idx: 140 |  Loss: (0.0070) | Acc: (99.96%) (18041/18048)\n",
            "Epoch: 140 | Batch_idx: 150 |  Loss: (0.0070) | Acc: (99.96%) (19320/19328)\n",
            "Epoch: 140 | Batch_idx: 160 |  Loss: (0.0068) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 140 | Batch_idx: 170 |  Loss: (0.0068) | Acc: (99.96%) (21880/21888)\n",
            "Epoch: 140 | Batch_idx: 180 |  Loss: (0.0068) | Acc: (99.95%) (23157/23168)\n",
            "Epoch: 140 | Batch_idx: 190 |  Loss: (0.0068) | Acc: (99.95%) (24436/24448)\n",
            "Epoch: 140 | Batch_idx: 200 |  Loss: (0.0068) | Acc: (99.95%) (25716/25728)\n",
            "Epoch: 140 | Batch_idx: 210 |  Loss: (0.0069) | Acc: (99.95%) (26995/27008)\n",
            "Epoch: 140 | Batch_idx: 220 |  Loss: (0.0068) | Acc: (99.95%) (28275/28288)\n",
            "Epoch: 140 | Batch_idx: 230 |  Loss: (0.0068) | Acc: (99.96%) (29555/29568)\n",
            "Epoch: 140 | Batch_idx: 240 |  Loss: (0.0068) | Acc: (99.96%) (30835/30848)\n",
            "Epoch: 140 | Batch_idx: 250 |  Loss: (0.0068) | Acc: (99.96%) (32114/32128)\n",
            "Epoch: 140 | Batch_idx: 260 |  Loss: (0.0068) | Acc: (99.96%) (33394/33408)\n",
            "Epoch: 140 | Batch_idx: 270 |  Loss: (0.0068) | Acc: (99.96%) (34673/34688)\n",
            "Epoch: 140 | Batch_idx: 280 |  Loss: (0.0068) | Acc: (99.96%) (35953/35968)\n",
            "Epoch: 140 | Batch_idx: 290 |  Loss: (0.0067) | Acc: (99.96%) (37233/37248)\n",
            "Epoch: 140 | Batch_idx: 300 |  Loss: (0.0067) | Acc: (99.96%) (38513/38528)\n",
            "Epoch: 140 | Batch_idx: 310 |  Loss: (0.0067) | Acc: (99.96%) (39793/39808)\n",
            "Epoch: 140 | Batch_idx: 320 |  Loss: (0.0067) | Acc: (99.96%) (41073/41088)\n",
            "Epoch: 140 | Batch_idx: 330 |  Loss: (0.0067) | Acc: (99.96%) (42352/42368)\n",
            "Epoch: 140 | Batch_idx: 340 |  Loss: (0.0067) | Acc: (99.96%) (43631/43648)\n",
            "Epoch: 140 | Batch_idx: 350 |  Loss: (0.0068) | Acc: (99.96%) (44909/44928)\n",
            "Epoch: 140 | Batch_idx: 360 |  Loss: (0.0067) | Acc: (99.96%) (46189/46208)\n",
            "Epoch: 140 | Batch_idx: 370 |  Loss: (0.0067) | Acc: (99.96%) (47468/47488)\n",
            "Epoch: 140 | Batch_idx: 380 |  Loss: (0.0067) | Acc: (99.96%) (48748/48768)\n",
            "Epoch: 140 | Batch_idx: 390 |  Loss: (0.0067) | Acc: (99.96%) (49978/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2581) | Acc: (93.01%) (9301/10000)\n",
            "Epoch: 141 | Batch_idx: 0 |  Loss: (0.0053) | Acc: (100.00%) (128/128)\n",
            "Epoch: 141 | Batch_idx: 10 |  Loss: (0.0051) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 141 | Batch_idx: 20 |  Loss: (0.0063) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 141 | Batch_idx: 30 |  Loss: (0.0061) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 141 | Batch_idx: 40 |  Loss: (0.0067) | Acc: (99.94%) (5245/5248)\n",
            "Epoch: 141 | Batch_idx: 50 |  Loss: (0.0065) | Acc: (99.95%) (6525/6528)\n",
            "Epoch: 141 | Batch_idx: 60 |  Loss: (0.0066) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 141 | Batch_idx: 70 |  Loss: (0.0063) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 141 | Batch_idx: 80 |  Loss: (0.0065) | Acc: (99.93%) (10361/10368)\n",
            "Epoch: 141 | Batch_idx: 90 |  Loss: (0.0065) | Acc: (99.93%) (11640/11648)\n",
            "Epoch: 141 | Batch_idx: 100 |  Loss: (0.0064) | Acc: (99.94%) (12920/12928)\n",
            "Epoch: 141 | Batch_idx: 110 |  Loss: (0.0064) | Acc: (99.94%) (14200/14208)\n",
            "Epoch: 141 | Batch_idx: 120 |  Loss: (0.0064) | Acc: (99.94%) (15479/15488)\n",
            "Epoch: 141 | Batch_idx: 130 |  Loss: (0.0064) | Acc: (99.95%) (16759/16768)\n",
            "Epoch: 141 | Batch_idx: 140 |  Loss: (0.0063) | Acc: (99.95%) (18039/18048)\n",
            "Epoch: 141 | Batch_idx: 150 |  Loss: (0.0062) | Acc: (99.95%) (19319/19328)\n",
            "Epoch: 141 | Batch_idx: 160 |  Loss: (0.0063) | Acc: (99.96%) (20599/20608)\n",
            "Epoch: 141 | Batch_idx: 170 |  Loss: (0.0063) | Acc: (99.96%) (21879/21888)\n",
            "Epoch: 141 | Batch_idx: 180 |  Loss: (0.0064) | Acc: (99.96%) (23159/23168)\n",
            "Epoch: 141 | Batch_idx: 190 |  Loss: (0.0064) | Acc: (99.96%) (24439/24448)\n",
            "Epoch: 141 | Batch_idx: 200 |  Loss: (0.0064) | Acc: (99.97%) (25719/25728)\n",
            "Epoch: 141 | Batch_idx: 210 |  Loss: (0.0065) | Acc: (99.97%) (26999/27008)\n",
            "Epoch: 141 | Batch_idx: 220 |  Loss: (0.0065) | Acc: (99.97%) (28279/28288)\n",
            "Epoch: 141 | Batch_idx: 230 |  Loss: (0.0065) | Acc: (99.97%) (29559/29568)\n",
            "Epoch: 141 | Batch_idx: 240 |  Loss: (0.0065) | Acc: (99.97%) (30838/30848)\n",
            "Epoch: 141 | Batch_idx: 250 |  Loss: (0.0064) | Acc: (99.97%) (32118/32128)\n",
            "Epoch: 141 | Batch_idx: 260 |  Loss: (0.0064) | Acc: (99.97%) (33398/33408)\n",
            "Epoch: 141 | Batch_idx: 270 |  Loss: (0.0065) | Acc: (99.97%) (34677/34688)\n",
            "Epoch: 141 | Batch_idx: 280 |  Loss: (0.0064) | Acc: (99.97%) (35956/35968)\n",
            "Epoch: 141 | Batch_idx: 290 |  Loss: (0.0065) | Acc: (99.97%) (37235/37248)\n",
            "Epoch: 141 | Batch_idx: 300 |  Loss: (0.0065) | Acc: (99.97%) (38515/38528)\n",
            "Epoch: 141 | Batch_idx: 310 |  Loss: (0.0065) | Acc: (99.97%) (39795/39808)\n",
            "Epoch: 141 | Batch_idx: 320 |  Loss: (0.0065) | Acc: (99.97%) (41074/41088)\n",
            "Epoch: 141 | Batch_idx: 330 |  Loss: (0.0065) | Acc: (99.96%) (42353/42368)\n",
            "Epoch: 141 | Batch_idx: 340 |  Loss: (0.0065) | Acc: (99.97%) (43633/43648)\n",
            "Epoch: 141 | Batch_idx: 350 |  Loss: (0.0065) | Acc: (99.97%) (44913/44928)\n",
            "Epoch: 141 | Batch_idx: 360 |  Loss: (0.0065) | Acc: (99.97%) (46192/46208)\n",
            "Epoch: 141 | Batch_idx: 370 |  Loss: (0.0066) | Acc: (99.96%) (47471/47488)\n",
            "Epoch: 141 | Batch_idx: 380 |  Loss: (0.0066) | Acc: (99.97%) (48751/48768)\n",
            "Epoch: 141 | Batch_idx: 390 |  Loss: (0.0066) | Acc: (99.97%) (49983/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2586) | Acc: (92.96%) (9296/10000)\n",
            "Epoch: 142 | Batch_idx: 0 |  Loss: (0.0050) | Acc: (100.00%) (128/128)\n",
            "Epoch: 142 | Batch_idx: 10 |  Loss: (0.0065) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 142 | Batch_idx: 20 |  Loss: (0.0061) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 142 | Batch_idx: 30 |  Loss: (0.0062) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 142 | Batch_idx: 40 |  Loss: (0.0062) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 142 | Batch_idx: 50 |  Loss: (0.0065) | Acc: (99.95%) (6525/6528)\n",
            "Epoch: 142 | Batch_idx: 60 |  Loss: (0.0063) | Acc: (99.96%) (7805/7808)\n",
            "Epoch: 142 | Batch_idx: 70 |  Loss: (0.0064) | Acc: (99.94%) (9083/9088)\n",
            "Epoch: 142 | Batch_idx: 80 |  Loss: (0.0065) | Acc: (99.93%) (10361/10368)\n",
            "Epoch: 142 | Batch_idx: 90 |  Loss: (0.0065) | Acc: (99.94%) (11641/11648)\n",
            "Epoch: 142 | Batch_idx: 100 |  Loss: (0.0064) | Acc: (99.95%) (12921/12928)\n",
            "Epoch: 142 | Batch_idx: 110 |  Loss: (0.0063) | Acc: (99.95%) (14201/14208)\n",
            "Epoch: 142 | Batch_idx: 120 |  Loss: (0.0064) | Acc: (99.95%) (15481/15488)\n",
            "Epoch: 142 | Batch_idx: 130 |  Loss: (0.0064) | Acc: (99.96%) (16761/16768)\n",
            "Epoch: 142 | Batch_idx: 140 |  Loss: (0.0064) | Acc: (99.96%) (18041/18048)\n",
            "Epoch: 142 | Batch_idx: 150 |  Loss: (0.0066) | Acc: (99.95%) (19319/19328)\n",
            "Epoch: 142 | Batch_idx: 160 |  Loss: (0.0065) | Acc: (99.96%) (20599/20608)\n",
            "Epoch: 142 | Batch_idx: 170 |  Loss: (0.0064) | Acc: (99.96%) (21879/21888)\n",
            "Epoch: 142 | Batch_idx: 180 |  Loss: (0.0064) | Acc: (99.96%) (23158/23168)\n",
            "Epoch: 142 | Batch_idx: 190 |  Loss: (0.0064) | Acc: (99.96%) (24438/24448)\n",
            "Epoch: 142 | Batch_idx: 200 |  Loss: (0.0064) | Acc: (99.96%) (25718/25728)\n",
            "Epoch: 142 | Batch_idx: 210 |  Loss: (0.0065) | Acc: (99.96%) (26996/27008)\n",
            "Epoch: 142 | Batch_idx: 220 |  Loss: (0.0064) | Acc: (99.96%) (28276/28288)\n",
            "Epoch: 142 | Batch_idx: 230 |  Loss: (0.0064) | Acc: (99.96%) (29555/29568)\n",
            "Epoch: 142 | Batch_idx: 240 |  Loss: (0.0064) | Acc: (99.96%) (30835/30848)\n",
            "Epoch: 142 | Batch_idx: 250 |  Loss: (0.0064) | Acc: (99.96%) (32114/32128)\n",
            "Epoch: 142 | Batch_idx: 260 |  Loss: (0.0064) | Acc: (99.95%) (33392/33408)\n",
            "Epoch: 142 | Batch_idx: 270 |  Loss: (0.0065) | Acc: (99.95%) (34672/34688)\n",
            "Epoch: 142 | Batch_idx: 280 |  Loss: (0.0065) | Acc: (99.95%) (35951/35968)\n",
            "Epoch: 142 | Batch_idx: 290 |  Loss: (0.0064) | Acc: (99.95%) (37231/37248)\n",
            "Epoch: 142 | Batch_idx: 300 |  Loss: (0.0064) | Acc: (99.95%) (38510/38528)\n",
            "Epoch: 142 | Batch_idx: 310 |  Loss: (0.0064) | Acc: (99.95%) (39790/39808)\n",
            "Epoch: 142 | Batch_idx: 320 |  Loss: (0.0064) | Acc: (99.96%) (41070/41088)\n",
            "Epoch: 142 | Batch_idx: 330 |  Loss: (0.0064) | Acc: (99.96%) (42350/42368)\n",
            "Epoch: 142 | Batch_idx: 340 |  Loss: (0.0064) | Acc: (99.96%) (43630/43648)\n",
            "Epoch: 142 | Batch_idx: 350 |  Loss: (0.0064) | Acc: (99.96%) (44910/44928)\n",
            "Epoch: 142 | Batch_idx: 360 |  Loss: (0.0063) | Acc: (99.96%) (46190/46208)\n",
            "Epoch: 142 | Batch_idx: 370 |  Loss: (0.0064) | Acc: (99.96%) (47469/47488)\n",
            "Epoch: 142 | Batch_idx: 380 |  Loss: (0.0064) | Acc: (99.96%) (48748/48768)\n",
            "Epoch: 142 | Batch_idx: 390 |  Loss: (0.0064) | Acc: (99.96%) (49980/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2605) | Acc: (92.94%) (9294/10000)\n",
            "Epoch: 143 | Batch_idx: 0 |  Loss: (0.0042) | Acc: (100.00%) (128/128)\n",
            "Epoch: 143 | Batch_idx: 10 |  Loss: (0.0053) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 143 | Batch_idx: 20 |  Loss: (0.0071) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 143 | Batch_idx: 30 |  Loss: (0.0073) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 143 | Batch_idx: 40 |  Loss: (0.0070) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 143 | Batch_idx: 50 |  Loss: (0.0068) | Acc: (99.97%) (6526/6528)\n",
            "Epoch: 143 | Batch_idx: 60 |  Loss: (0.0066) | Acc: (99.97%) (7806/7808)\n",
            "Epoch: 143 | Batch_idx: 70 |  Loss: (0.0064) | Acc: (99.98%) (9086/9088)\n",
            "Epoch: 143 | Batch_idx: 80 |  Loss: (0.0063) | Acc: (99.98%) (10366/10368)\n",
            "Epoch: 143 | Batch_idx: 90 |  Loss: (0.0064) | Acc: (99.98%) (11646/11648)\n",
            "Epoch: 143 | Batch_idx: 100 |  Loss: (0.0063) | Acc: (99.98%) (12926/12928)\n",
            "Epoch: 143 | Batch_idx: 110 |  Loss: (0.0062) | Acc: (99.99%) (14206/14208)\n",
            "Epoch: 143 | Batch_idx: 120 |  Loss: (0.0063) | Acc: (99.99%) (15486/15488)\n",
            "Epoch: 143 | Batch_idx: 130 |  Loss: (0.0065) | Acc: (99.96%) (16762/16768)\n",
            "Epoch: 143 | Batch_idx: 140 |  Loss: (0.0065) | Acc: (99.96%) (18041/18048)\n",
            "Epoch: 143 | Batch_idx: 150 |  Loss: (0.0067) | Acc: (99.95%) (19319/19328)\n",
            "Epoch: 143 | Batch_idx: 160 |  Loss: (0.0068) | Acc: (99.96%) (20599/20608)\n",
            "Epoch: 143 | Batch_idx: 170 |  Loss: (0.0068) | Acc: (99.96%) (21879/21888)\n",
            "Epoch: 143 | Batch_idx: 180 |  Loss: (0.0068) | Acc: (99.96%) (23159/23168)\n",
            "Epoch: 143 | Batch_idx: 190 |  Loss: (0.0067) | Acc: (99.96%) (24439/24448)\n",
            "Epoch: 143 | Batch_idx: 200 |  Loss: (0.0067) | Acc: (99.96%) (25718/25728)\n",
            "Epoch: 143 | Batch_idx: 210 |  Loss: (0.0067) | Acc: (99.96%) (26997/27008)\n",
            "Epoch: 143 | Batch_idx: 220 |  Loss: (0.0068) | Acc: (99.95%) (28275/28288)\n",
            "Epoch: 143 | Batch_idx: 230 |  Loss: (0.0068) | Acc: (99.95%) (29554/29568)\n",
            "Epoch: 143 | Batch_idx: 240 |  Loss: (0.0069) | Acc: (99.94%) (30831/30848)\n",
            "Epoch: 143 | Batch_idx: 250 |  Loss: (0.0068) | Acc: (99.95%) (32111/32128)\n",
            "Epoch: 143 | Batch_idx: 260 |  Loss: (0.0068) | Acc: (99.95%) (33391/33408)\n",
            "Epoch: 143 | Batch_idx: 270 |  Loss: (0.0068) | Acc: (99.95%) (34670/34688)\n",
            "Epoch: 143 | Batch_idx: 280 |  Loss: (0.0068) | Acc: (99.95%) (35950/35968)\n",
            "Epoch: 143 | Batch_idx: 290 |  Loss: (0.0068) | Acc: (99.95%) (37230/37248)\n",
            "Epoch: 143 | Batch_idx: 300 |  Loss: (0.0067) | Acc: (99.95%) (38510/38528)\n",
            "Epoch: 143 | Batch_idx: 310 |  Loss: (0.0067) | Acc: (99.95%) (39790/39808)\n",
            "Epoch: 143 | Batch_idx: 320 |  Loss: (0.0066) | Acc: (99.96%) (41070/41088)\n",
            "Epoch: 143 | Batch_idx: 330 |  Loss: (0.0066) | Acc: (99.96%) (42350/42368)\n",
            "Epoch: 143 | Batch_idx: 340 |  Loss: (0.0066) | Acc: (99.96%) (43630/43648)\n",
            "Epoch: 143 | Batch_idx: 350 |  Loss: (0.0066) | Acc: (99.96%) (44910/44928)\n",
            "Epoch: 143 | Batch_idx: 360 |  Loss: (0.0066) | Acc: (99.96%) (46189/46208)\n",
            "Epoch: 143 | Batch_idx: 370 |  Loss: (0.0066) | Acc: (99.96%) (47469/47488)\n",
            "Epoch: 143 | Batch_idx: 380 |  Loss: (0.0066) | Acc: (99.96%) (48749/48768)\n",
            "Epoch: 143 | Batch_idx: 390 |  Loss: (0.0066) | Acc: (99.96%) (49981/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2574) | Acc: (92.99%) (9299/10000)\n",
            "Epoch: 144 | Batch_idx: 0 |  Loss: (0.0040) | Acc: (100.00%) (128/128)\n",
            "Epoch: 144 | Batch_idx: 10 |  Loss: (0.0051) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 144 | Batch_idx: 20 |  Loss: (0.0066) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 144 | Batch_idx: 30 |  Loss: (0.0069) | Acc: (99.92%) (3965/3968)\n",
            "Epoch: 144 | Batch_idx: 40 |  Loss: (0.0068) | Acc: (99.94%) (5245/5248)\n",
            "Epoch: 144 | Batch_idx: 50 |  Loss: (0.0067) | Acc: (99.95%) (6525/6528)\n",
            "Epoch: 144 | Batch_idx: 60 |  Loss: (0.0069) | Acc: (99.96%) (7805/7808)\n",
            "Epoch: 144 | Batch_idx: 70 |  Loss: (0.0071) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 144 | Batch_idx: 80 |  Loss: (0.0070) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 144 | Batch_idx: 90 |  Loss: (0.0069) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 144 | Batch_idx: 100 |  Loss: (0.0069) | Acc: (99.96%) (12923/12928)\n",
            "Epoch: 144 | Batch_idx: 110 |  Loss: (0.0070) | Acc: (99.96%) (14203/14208)\n",
            "Epoch: 144 | Batch_idx: 120 |  Loss: (0.0072) | Acc: (99.95%) (15481/15488)\n",
            "Epoch: 144 | Batch_idx: 130 |  Loss: (0.0071) | Acc: (99.96%) (16761/16768)\n",
            "Epoch: 144 | Batch_idx: 140 |  Loss: (0.0071) | Acc: (99.96%) (18040/18048)\n",
            "Epoch: 144 | Batch_idx: 150 |  Loss: (0.0071) | Acc: (99.96%) (19320/19328)\n",
            "Epoch: 144 | Batch_idx: 160 |  Loss: (0.0071) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 144 | Batch_idx: 170 |  Loss: (0.0070) | Acc: (99.96%) (21880/21888)\n",
            "Epoch: 144 | Batch_idx: 180 |  Loss: (0.0070) | Acc: (99.96%) (23159/23168)\n",
            "Epoch: 144 | Batch_idx: 190 |  Loss: (0.0069) | Acc: (99.96%) (24439/24448)\n",
            "Epoch: 144 | Batch_idx: 200 |  Loss: (0.0069) | Acc: (99.96%) (25718/25728)\n",
            "Epoch: 144 | Batch_idx: 210 |  Loss: (0.0068) | Acc: (99.96%) (26997/27008)\n",
            "Epoch: 144 | Batch_idx: 220 |  Loss: (0.0068) | Acc: (99.96%) (28276/28288)\n",
            "Epoch: 144 | Batch_idx: 230 |  Loss: (0.0067) | Acc: (99.96%) (29556/29568)\n",
            "Epoch: 144 | Batch_idx: 240 |  Loss: (0.0066) | Acc: (99.96%) (30836/30848)\n",
            "Epoch: 144 | Batch_idx: 250 |  Loss: (0.0066) | Acc: (99.96%) (32116/32128)\n",
            "Epoch: 144 | Batch_idx: 260 |  Loss: (0.0066) | Acc: (99.96%) (33396/33408)\n",
            "Epoch: 144 | Batch_idx: 270 |  Loss: (0.0066) | Acc: (99.96%) (34675/34688)\n",
            "Epoch: 144 | Batch_idx: 280 |  Loss: (0.0066) | Acc: (99.96%) (35954/35968)\n",
            "Epoch: 144 | Batch_idx: 290 |  Loss: (0.0066) | Acc: (99.96%) (37233/37248)\n",
            "Epoch: 144 | Batch_idx: 300 |  Loss: (0.0067) | Acc: (99.96%) (38512/38528)\n",
            "Epoch: 144 | Batch_idx: 310 |  Loss: (0.0067) | Acc: (99.96%) (39792/39808)\n",
            "Epoch: 144 | Batch_idx: 320 |  Loss: (0.0066) | Acc: (99.96%) (41072/41088)\n",
            "Epoch: 144 | Batch_idx: 330 |  Loss: (0.0066) | Acc: (99.96%) (42352/42368)\n",
            "Epoch: 144 | Batch_idx: 340 |  Loss: (0.0065) | Acc: (99.96%) (43632/43648)\n",
            "Epoch: 144 | Batch_idx: 350 |  Loss: (0.0065) | Acc: (99.96%) (44912/44928)\n",
            "Epoch: 144 | Batch_idx: 360 |  Loss: (0.0065) | Acc: (99.96%) (46191/46208)\n",
            "Epoch: 144 | Batch_idx: 370 |  Loss: (0.0065) | Acc: (99.96%) (47471/47488)\n",
            "Epoch: 144 | Batch_idx: 380 |  Loss: (0.0066) | Acc: (99.97%) (48751/48768)\n",
            "Epoch: 144 | Batch_idx: 390 |  Loss: (0.0066) | Acc: (99.97%) (49983/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2588) | Acc: (93.08%) (9308/10000)\n",
            "Epoch: 145 | Batch_idx: 0 |  Loss: (0.0069) | Acc: (100.00%) (128/128)\n",
            "Epoch: 145 | Batch_idx: 10 |  Loss: (0.0057) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 145 | Batch_idx: 20 |  Loss: (0.0058) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 145 | Batch_idx: 30 |  Loss: (0.0066) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 145 | Batch_idx: 40 |  Loss: (0.0063) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 145 | Batch_idx: 50 |  Loss: (0.0067) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 145 | Batch_idx: 60 |  Loss: (0.0067) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 145 | Batch_idx: 70 |  Loss: (0.0066) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 145 | Batch_idx: 80 |  Loss: (0.0063) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 145 | Batch_idx: 90 |  Loss: (0.0063) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 145 | Batch_idx: 100 |  Loss: (0.0065) | Acc: (99.96%) (12923/12928)\n",
            "Epoch: 145 | Batch_idx: 110 |  Loss: (0.0066) | Acc: (99.96%) (14202/14208)\n",
            "Epoch: 145 | Batch_idx: 120 |  Loss: (0.0065) | Acc: (99.96%) (15482/15488)\n",
            "Epoch: 145 | Batch_idx: 130 |  Loss: (0.0065) | Acc: (99.96%) (16762/16768)\n",
            "Epoch: 145 | Batch_idx: 140 |  Loss: (0.0065) | Acc: (99.97%) (18042/18048)\n",
            "Epoch: 145 | Batch_idx: 150 |  Loss: (0.0065) | Acc: (99.96%) (19321/19328)\n",
            "Epoch: 145 | Batch_idx: 160 |  Loss: (0.0067) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 145 | Batch_idx: 170 |  Loss: (0.0066) | Acc: (99.96%) (21880/21888)\n",
            "Epoch: 145 | Batch_idx: 180 |  Loss: (0.0065) | Acc: (99.97%) (23160/23168)\n",
            "Epoch: 145 | Batch_idx: 190 |  Loss: (0.0066) | Acc: (99.97%) (24440/24448)\n",
            "Epoch: 145 | Batch_idx: 200 |  Loss: (0.0066) | Acc: (99.97%) (25720/25728)\n",
            "Epoch: 145 | Batch_idx: 210 |  Loss: (0.0067) | Acc: (99.96%) (26998/27008)\n",
            "Epoch: 145 | Batch_idx: 220 |  Loss: (0.0067) | Acc: (99.96%) (28278/28288)\n",
            "Epoch: 145 | Batch_idx: 230 |  Loss: (0.0067) | Acc: (99.96%) (29557/29568)\n",
            "Epoch: 145 | Batch_idx: 240 |  Loss: (0.0068) | Acc: (99.96%) (30836/30848)\n",
            "Epoch: 145 | Batch_idx: 250 |  Loss: (0.0067) | Acc: (99.96%) (32116/32128)\n",
            "Epoch: 145 | Batch_idx: 260 |  Loss: (0.0067) | Acc: (99.96%) (33396/33408)\n",
            "Epoch: 145 | Batch_idx: 270 |  Loss: (0.0067) | Acc: (99.96%) (34675/34688)\n",
            "Epoch: 145 | Batch_idx: 280 |  Loss: (0.0066) | Acc: (99.96%) (35955/35968)\n",
            "Epoch: 145 | Batch_idx: 290 |  Loss: (0.0066) | Acc: (99.97%) (37235/37248)\n",
            "Epoch: 145 | Batch_idx: 300 |  Loss: (0.0066) | Acc: (99.96%) (38514/38528)\n",
            "Epoch: 145 | Batch_idx: 310 |  Loss: (0.0067) | Acc: (99.96%) (39793/39808)\n",
            "Epoch: 145 | Batch_idx: 320 |  Loss: (0.0067) | Acc: (99.96%) (41073/41088)\n",
            "Epoch: 145 | Batch_idx: 330 |  Loss: (0.0067) | Acc: (99.96%) (42353/42368)\n",
            "Epoch: 145 | Batch_idx: 340 |  Loss: (0.0068) | Acc: (99.97%) (43633/43648)\n",
            "Epoch: 145 | Batch_idx: 350 |  Loss: (0.0068) | Acc: (99.96%) (44912/44928)\n",
            "Epoch: 145 | Batch_idx: 360 |  Loss: (0.0068) | Acc: (99.97%) (46192/46208)\n",
            "Epoch: 145 | Batch_idx: 370 |  Loss: (0.0068) | Acc: (99.97%) (47472/47488)\n",
            "Epoch: 145 | Batch_idx: 380 |  Loss: (0.0068) | Acc: (99.97%) (48751/48768)\n",
            "Epoch: 145 | Batch_idx: 390 |  Loss: (0.0068) | Acc: (99.97%) (49983/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2587) | Acc: (92.96%) (9296/10000)\n",
            "Epoch: 146 | Batch_idx: 0 |  Loss: (0.0047) | Acc: (100.00%) (128/128)\n",
            "Epoch: 146 | Batch_idx: 10 |  Loss: (0.0082) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 146 | Batch_idx: 20 |  Loss: (0.0069) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 146 | Batch_idx: 30 |  Loss: (0.0068) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 146 | Batch_idx: 40 |  Loss: (0.0069) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 146 | Batch_idx: 50 |  Loss: (0.0068) | Acc: (99.97%) (6526/6528)\n",
            "Epoch: 146 | Batch_idx: 60 |  Loss: (0.0067) | Acc: (99.97%) (7806/7808)\n",
            "Epoch: 146 | Batch_idx: 70 |  Loss: (0.0067) | Acc: (99.98%) (9086/9088)\n",
            "Epoch: 146 | Batch_idx: 80 |  Loss: (0.0067) | Acc: (99.97%) (10365/10368)\n",
            "Epoch: 146 | Batch_idx: 90 |  Loss: (0.0068) | Acc: (99.97%) (11645/11648)\n",
            "Epoch: 146 | Batch_idx: 100 |  Loss: (0.0067) | Acc: (99.98%) (12925/12928)\n",
            "Epoch: 146 | Batch_idx: 110 |  Loss: (0.0066) | Acc: (99.98%) (14205/14208)\n",
            "Epoch: 146 | Batch_idx: 120 |  Loss: (0.0065) | Acc: (99.97%) (15484/15488)\n",
            "Epoch: 146 | Batch_idx: 130 |  Loss: (0.0065) | Acc: (99.98%) (16764/16768)\n",
            "Epoch: 146 | Batch_idx: 140 |  Loss: (0.0068) | Acc: (99.97%) (18043/18048)\n",
            "Epoch: 146 | Batch_idx: 150 |  Loss: (0.0068) | Acc: (99.96%) (19321/19328)\n",
            "Epoch: 146 | Batch_idx: 160 |  Loss: (0.0067) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 146 | Batch_idx: 170 |  Loss: (0.0068) | Acc: (99.96%) (21880/21888)\n",
            "Epoch: 146 | Batch_idx: 180 |  Loss: (0.0069) | Acc: (99.96%) (23159/23168)\n",
            "Epoch: 146 | Batch_idx: 190 |  Loss: (0.0068) | Acc: (99.96%) (24439/24448)\n",
            "Epoch: 146 | Batch_idx: 200 |  Loss: (0.0067) | Acc: (99.97%) (25719/25728)\n",
            "Epoch: 146 | Batch_idx: 210 |  Loss: (0.0067) | Acc: (99.97%) (26999/27008)\n",
            "Epoch: 146 | Batch_idx: 220 |  Loss: (0.0068) | Acc: (99.96%) (28278/28288)\n",
            "Epoch: 146 | Batch_idx: 230 |  Loss: (0.0067) | Acc: (99.97%) (29558/29568)\n",
            "Epoch: 146 | Batch_idx: 240 |  Loss: (0.0068) | Acc: (99.97%) (30838/30848)\n",
            "Epoch: 146 | Batch_idx: 250 |  Loss: (0.0068) | Acc: (99.97%) (32117/32128)\n",
            "Epoch: 146 | Batch_idx: 260 |  Loss: (0.0068) | Acc: (99.97%) (33397/33408)\n",
            "Epoch: 146 | Batch_idx: 270 |  Loss: (0.0067) | Acc: (99.97%) (34677/34688)\n",
            "Epoch: 146 | Batch_idx: 280 |  Loss: (0.0067) | Acc: (99.97%) (35957/35968)\n",
            "Epoch: 146 | Batch_idx: 290 |  Loss: (0.0066) | Acc: (99.97%) (37237/37248)\n",
            "Epoch: 146 | Batch_idx: 300 |  Loss: (0.0066) | Acc: (99.97%) (38517/38528)\n",
            "Epoch: 146 | Batch_idx: 310 |  Loss: (0.0066) | Acc: (99.97%) (39797/39808)\n",
            "Epoch: 146 | Batch_idx: 320 |  Loss: (0.0066) | Acc: (99.97%) (41077/41088)\n",
            "Epoch: 146 | Batch_idx: 330 |  Loss: (0.0065) | Acc: (99.97%) (42357/42368)\n",
            "Epoch: 146 | Batch_idx: 340 |  Loss: (0.0065) | Acc: (99.97%) (43637/43648)\n",
            "Epoch: 146 | Batch_idx: 350 |  Loss: (0.0065) | Acc: (99.97%) (44916/44928)\n",
            "Epoch: 146 | Batch_idx: 360 |  Loss: (0.0065) | Acc: (99.97%) (46196/46208)\n",
            "Epoch: 146 | Batch_idx: 370 |  Loss: (0.0065) | Acc: (99.97%) (47475/47488)\n",
            "Epoch: 146 | Batch_idx: 380 |  Loss: (0.0065) | Acc: (99.97%) (48755/48768)\n",
            "Epoch: 146 | Batch_idx: 390 |  Loss: (0.0065) | Acc: (99.97%) (49986/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2575) | Acc: (93.00%) (9300/10000)\n",
            "Epoch: 147 | Batch_idx: 0 |  Loss: (0.0033) | Acc: (100.00%) (128/128)\n",
            "Epoch: 147 | Batch_idx: 10 |  Loss: (0.0054) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 147 | Batch_idx: 20 |  Loss: (0.0057) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 147 | Batch_idx: 30 |  Loss: (0.0056) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 147 | Batch_idx: 40 |  Loss: (0.0058) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 147 | Batch_idx: 50 |  Loss: (0.0059) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 147 | Batch_idx: 60 |  Loss: (0.0061) | Acc: (99.99%) (7807/7808)\n",
            "Epoch: 147 | Batch_idx: 70 |  Loss: (0.0060) | Acc: (99.99%) (9087/9088)\n",
            "Epoch: 147 | Batch_idx: 80 |  Loss: (0.0062) | Acc: (99.99%) (10367/10368)\n",
            "Epoch: 147 | Batch_idx: 90 |  Loss: (0.0061) | Acc: (99.98%) (11646/11648)\n",
            "Epoch: 147 | Batch_idx: 100 |  Loss: (0.0060) | Acc: (99.98%) (12926/12928)\n",
            "Epoch: 147 | Batch_idx: 110 |  Loss: (0.0060) | Acc: (99.99%) (14206/14208)\n",
            "Epoch: 147 | Batch_idx: 120 |  Loss: (0.0060) | Acc: (99.99%) (15486/15488)\n",
            "Epoch: 147 | Batch_idx: 130 |  Loss: (0.0059) | Acc: (99.99%) (16766/16768)\n",
            "Epoch: 147 | Batch_idx: 140 |  Loss: (0.0062) | Acc: (99.98%) (18044/18048)\n",
            "Epoch: 147 | Batch_idx: 150 |  Loss: (0.0061) | Acc: (99.98%) (19324/19328)\n",
            "Epoch: 147 | Batch_idx: 160 |  Loss: (0.0061) | Acc: (99.98%) (20604/20608)\n",
            "Epoch: 147 | Batch_idx: 170 |  Loss: (0.0062) | Acc: (99.98%) (21884/21888)\n",
            "Epoch: 147 | Batch_idx: 180 |  Loss: (0.0062) | Acc: (99.98%) (23164/23168)\n",
            "Epoch: 147 | Batch_idx: 190 |  Loss: (0.0061) | Acc: (99.98%) (24444/24448)\n",
            "Epoch: 147 | Batch_idx: 200 |  Loss: (0.0061) | Acc: (99.98%) (25724/25728)\n",
            "Epoch: 147 | Batch_idx: 210 |  Loss: (0.0061) | Acc: (99.99%) (27004/27008)\n",
            "Epoch: 147 | Batch_idx: 220 |  Loss: (0.0062) | Acc: (99.98%) (28282/28288)\n",
            "Epoch: 147 | Batch_idx: 230 |  Loss: (0.0062) | Acc: (99.98%) (29562/29568)\n",
            "Epoch: 147 | Batch_idx: 240 |  Loss: (0.0062) | Acc: (99.98%) (30842/30848)\n",
            "Epoch: 147 | Batch_idx: 250 |  Loss: (0.0062) | Acc: (99.98%) (32122/32128)\n",
            "Epoch: 147 | Batch_idx: 260 |  Loss: (0.0062) | Acc: (99.98%) (33401/33408)\n",
            "Epoch: 147 | Batch_idx: 270 |  Loss: (0.0062) | Acc: (99.98%) (34681/34688)\n",
            "Epoch: 147 | Batch_idx: 280 |  Loss: (0.0062) | Acc: (99.98%) (35961/35968)\n",
            "Epoch: 147 | Batch_idx: 290 |  Loss: (0.0063) | Acc: (99.98%) (37240/37248)\n",
            "Epoch: 147 | Batch_idx: 300 |  Loss: (0.0063) | Acc: (99.98%) (38520/38528)\n",
            "Epoch: 147 | Batch_idx: 310 |  Loss: (0.0063) | Acc: (99.98%) (39800/39808)\n",
            "Epoch: 147 | Batch_idx: 320 |  Loss: (0.0063) | Acc: (99.98%) (41079/41088)\n",
            "Epoch: 147 | Batch_idx: 330 |  Loss: (0.0064) | Acc: (99.98%) (42359/42368)\n",
            "Epoch: 147 | Batch_idx: 340 |  Loss: (0.0064) | Acc: (99.97%) (43637/43648)\n",
            "Epoch: 147 | Batch_idx: 350 |  Loss: (0.0064) | Acc: (99.97%) (44916/44928)\n",
            "Epoch: 147 | Batch_idx: 360 |  Loss: (0.0064) | Acc: (99.97%) (46196/46208)\n",
            "Epoch: 147 | Batch_idx: 370 |  Loss: (0.0064) | Acc: (99.97%) (47475/47488)\n",
            "Epoch: 147 | Batch_idx: 380 |  Loss: (0.0064) | Acc: (99.97%) (48755/48768)\n",
            "Epoch: 147 | Batch_idx: 390 |  Loss: (0.0065) | Acc: (99.97%) (49987/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2579) | Acc: (93.09%) (9309/10000)\n",
            "Epoch: 148 | Batch_idx: 0 |  Loss: (0.0053) | Acc: (100.00%) (128/128)\n",
            "Epoch: 148 | Batch_idx: 10 |  Loss: (0.0076) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 148 | Batch_idx: 20 |  Loss: (0.0065) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 148 | Batch_idx: 30 |  Loss: (0.0061) | Acc: (100.00%) (3968/3968)\n",
            "Epoch: 148 | Batch_idx: 40 |  Loss: (0.0064) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 148 | Batch_idx: 50 |  Loss: (0.0064) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 148 | Batch_idx: 60 |  Loss: (0.0065) | Acc: (99.99%) (7807/7808)\n",
            "Epoch: 148 | Batch_idx: 70 |  Loss: (0.0065) | Acc: (99.98%) (9086/9088)\n",
            "Epoch: 148 | Batch_idx: 80 |  Loss: (0.0064) | Acc: (99.98%) (10366/10368)\n",
            "Epoch: 148 | Batch_idx: 90 |  Loss: (0.0065) | Acc: (99.97%) (11645/11648)\n",
            "Epoch: 148 | Batch_idx: 100 |  Loss: (0.0065) | Acc: (99.98%) (12925/12928)\n",
            "Epoch: 148 | Batch_idx: 110 |  Loss: (0.0064) | Acc: (99.98%) (14205/14208)\n",
            "Epoch: 148 | Batch_idx: 120 |  Loss: (0.0065) | Acc: (99.97%) (15484/15488)\n",
            "Epoch: 148 | Batch_idx: 130 |  Loss: (0.0064) | Acc: (99.98%) (16764/16768)\n",
            "Epoch: 148 | Batch_idx: 140 |  Loss: (0.0064) | Acc: (99.98%) (18044/18048)\n",
            "Epoch: 148 | Batch_idx: 150 |  Loss: (0.0064) | Acc: (99.97%) (19323/19328)\n",
            "Epoch: 148 | Batch_idx: 160 |  Loss: (0.0064) | Acc: (99.98%) (20603/20608)\n",
            "Epoch: 148 | Batch_idx: 170 |  Loss: (0.0065) | Acc: (99.97%) (21882/21888)\n",
            "Epoch: 148 | Batch_idx: 180 |  Loss: (0.0066) | Acc: (99.97%) (23162/23168)\n",
            "Epoch: 148 | Batch_idx: 190 |  Loss: (0.0065) | Acc: (99.98%) (24442/24448)\n",
            "Epoch: 148 | Batch_idx: 200 |  Loss: (0.0067) | Acc: (99.97%) (25721/25728)\n",
            "Epoch: 148 | Batch_idx: 210 |  Loss: (0.0067) | Acc: (99.97%) (27000/27008)\n",
            "Epoch: 148 | Batch_idx: 220 |  Loss: (0.0067) | Acc: (99.97%) (28280/28288)\n",
            "Epoch: 148 | Batch_idx: 230 |  Loss: (0.0067) | Acc: (99.97%) (29560/29568)\n",
            "Epoch: 148 | Batch_idx: 240 |  Loss: (0.0067) | Acc: (99.97%) (30840/30848)\n",
            "Epoch: 148 | Batch_idx: 250 |  Loss: (0.0067) | Acc: (99.97%) (32119/32128)\n",
            "Epoch: 148 | Batch_idx: 260 |  Loss: (0.0066) | Acc: (99.97%) (33398/33408)\n",
            "Epoch: 148 | Batch_idx: 270 |  Loss: (0.0065) | Acc: (99.97%) (34678/34688)\n",
            "Epoch: 148 | Batch_idx: 280 |  Loss: (0.0065) | Acc: (99.97%) (35958/35968)\n",
            "Epoch: 148 | Batch_idx: 290 |  Loss: (0.0065) | Acc: (99.97%) (37238/37248)\n",
            "Epoch: 148 | Batch_idx: 300 |  Loss: (0.0065) | Acc: (99.97%) (38517/38528)\n",
            "Epoch: 148 | Batch_idx: 310 |  Loss: (0.0065) | Acc: (99.97%) (39797/39808)\n",
            "Epoch: 148 | Batch_idx: 320 |  Loss: (0.0065) | Acc: (99.97%) (41077/41088)\n",
            "Epoch: 148 | Batch_idx: 330 |  Loss: (0.0065) | Acc: (99.97%) (42357/42368)\n",
            "Epoch: 148 | Batch_idx: 340 |  Loss: (0.0065) | Acc: (99.97%) (43637/43648)\n",
            "Epoch: 148 | Batch_idx: 350 |  Loss: (0.0064) | Acc: (99.98%) (44917/44928)\n",
            "Epoch: 148 | Batch_idx: 360 |  Loss: (0.0065) | Acc: (99.97%) (46196/46208)\n",
            "Epoch: 148 | Batch_idx: 370 |  Loss: (0.0065) | Acc: (99.97%) (47474/47488)\n",
            "Epoch: 148 | Batch_idx: 380 |  Loss: (0.0065) | Acc: (99.97%) (48754/48768)\n",
            "Epoch: 148 | Batch_idx: 390 |  Loss: (0.0065) | Acc: (99.97%) (49986/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2573) | Acc: (92.97%) (9297/10000)\n",
            "Epoch: 149 | Batch_idx: 0 |  Loss: (0.0047) | Acc: (100.00%) (128/128)\n",
            "Epoch: 149 | Batch_idx: 10 |  Loss: (0.0052) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 149 | Batch_idx: 20 |  Loss: (0.0064) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 149 | Batch_idx: 30 |  Loss: (0.0067) | Acc: (99.92%) (3965/3968)\n",
            "Epoch: 149 | Batch_idx: 40 |  Loss: (0.0068) | Acc: (99.94%) (5245/5248)\n",
            "Epoch: 149 | Batch_idx: 50 |  Loss: (0.0066) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 149 | Batch_idx: 60 |  Loss: (0.0065) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 149 | Batch_idx: 70 |  Loss: (0.0063) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 149 | Batch_idx: 80 |  Loss: (0.0064) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 149 | Batch_idx: 90 |  Loss: (0.0064) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 149 | Batch_idx: 100 |  Loss: (0.0064) | Acc: (99.96%) (12923/12928)\n",
            "Epoch: 149 | Batch_idx: 110 |  Loss: (0.0063) | Acc: (99.96%) (14203/14208)\n",
            "Epoch: 149 | Batch_idx: 120 |  Loss: (0.0065) | Acc: (99.97%) (15483/15488)\n",
            "Epoch: 149 | Batch_idx: 130 |  Loss: (0.0066) | Acc: (99.97%) (16763/16768)\n",
            "Epoch: 149 | Batch_idx: 140 |  Loss: (0.0065) | Acc: (99.97%) (18043/18048)\n",
            "Epoch: 149 | Batch_idx: 150 |  Loss: (0.0066) | Acc: (99.97%) (19322/19328)\n",
            "Epoch: 149 | Batch_idx: 160 |  Loss: (0.0065) | Acc: (99.97%) (20602/20608)\n",
            "Epoch: 149 | Batch_idx: 170 |  Loss: (0.0064) | Acc: (99.97%) (21882/21888)\n",
            "Epoch: 149 | Batch_idx: 180 |  Loss: (0.0064) | Acc: (99.97%) (23162/23168)\n",
            "Epoch: 149 | Batch_idx: 190 |  Loss: (0.0064) | Acc: (99.98%) (24442/24448)\n",
            "Epoch: 149 | Batch_idx: 200 |  Loss: (0.0064) | Acc: (99.98%) (25722/25728)\n",
            "Epoch: 149 | Batch_idx: 210 |  Loss: (0.0063) | Acc: (99.98%) (27002/27008)\n",
            "Epoch: 149 | Batch_idx: 220 |  Loss: (0.0063) | Acc: (99.98%) (28282/28288)\n",
            "Epoch: 149 | Batch_idx: 230 |  Loss: (0.0063) | Acc: (99.98%) (29562/29568)\n",
            "Epoch: 149 | Batch_idx: 240 |  Loss: (0.0063) | Acc: (99.98%) (30842/30848)\n",
            "Epoch: 149 | Batch_idx: 250 |  Loss: (0.0063) | Acc: (99.98%) (32120/32128)\n",
            "Epoch: 149 | Batch_idx: 260 |  Loss: (0.0062) | Acc: (99.98%) (33400/33408)\n",
            "Epoch: 149 | Batch_idx: 270 |  Loss: (0.0062) | Acc: (99.98%) (34680/34688)\n",
            "Epoch: 149 | Batch_idx: 280 |  Loss: (0.0063) | Acc: (99.97%) (35959/35968)\n",
            "Epoch: 149 | Batch_idx: 290 |  Loss: (0.0063) | Acc: (99.98%) (37239/37248)\n",
            "Epoch: 149 | Batch_idx: 300 |  Loss: (0.0063) | Acc: (99.98%) (38519/38528)\n",
            "Epoch: 149 | Batch_idx: 310 |  Loss: (0.0063) | Acc: (99.98%) (39799/39808)\n",
            "Epoch: 149 | Batch_idx: 320 |  Loss: (0.0063) | Acc: (99.98%) (41079/41088)\n",
            "Epoch: 149 | Batch_idx: 330 |  Loss: (0.0063) | Acc: (99.98%) (42358/42368)\n",
            "Epoch: 149 | Batch_idx: 340 |  Loss: (0.0063) | Acc: (99.98%) (43638/43648)\n",
            "Epoch: 149 | Batch_idx: 350 |  Loss: (0.0063) | Acc: (99.98%) (44917/44928)\n",
            "Epoch: 149 | Batch_idx: 360 |  Loss: (0.0063) | Acc: (99.97%) (46196/46208)\n",
            "Epoch: 149 | Batch_idx: 370 |  Loss: (0.0063) | Acc: (99.97%) (47476/47488)\n",
            "Epoch: 149 | Batch_idx: 380 |  Loss: (0.0063) | Acc: (99.97%) (48755/48768)\n",
            "Epoch: 149 | Batch_idx: 390 |  Loss: (0.0063) | Acc: (99.97%) (49986/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2605) | Acc: (92.95%) (9295/10000)\n",
            "Epoch: 150 | Batch_idx: 0 |  Loss: (0.0030) | Acc: (100.00%) (128/128)\n",
            "Epoch: 150 | Batch_idx: 10 |  Loss: (0.0049) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 150 | Batch_idx: 20 |  Loss: (0.0057) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 150 | Batch_idx: 30 |  Loss: (0.0061) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 150 | Batch_idx: 40 |  Loss: (0.0059) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 150 | Batch_idx: 50 |  Loss: (0.0062) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 150 | Batch_idx: 60 |  Loss: (0.0066) | Acc: (99.96%) (7805/7808)\n",
            "Epoch: 150 | Batch_idx: 70 |  Loss: (0.0066) | Acc: (99.97%) (9085/9088)\n",
            "Epoch: 150 | Batch_idx: 80 |  Loss: (0.0066) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 150 | Batch_idx: 90 |  Loss: (0.0066) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 150 | Batch_idx: 100 |  Loss: (0.0068) | Acc: (99.96%) (12923/12928)\n",
            "Epoch: 150 | Batch_idx: 110 |  Loss: (0.0067) | Acc: (99.96%) (14202/14208)\n",
            "Epoch: 150 | Batch_idx: 120 |  Loss: (0.0068) | Acc: (99.95%) (15480/15488)\n",
            "Epoch: 150 | Batch_idx: 130 |  Loss: (0.0068) | Acc: (99.95%) (16760/16768)\n",
            "Epoch: 150 | Batch_idx: 140 |  Loss: (0.0068) | Acc: (99.96%) (18040/18048)\n",
            "Epoch: 150 | Batch_idx: 150 |  Loss: (0.0069) | Acc: (99.96%) (19320/19328)\n",
            "Epoch: 150 | Batch_idx: 160 |  Loss: (0.0068) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 150 | Batch_idx: 170 |  Loss: (0.0067) | Acc: (99.96%) (21880/21888)\n",
            "Epoch: 150 | Batch_idx: 180 |  Loss: (0.0067) | Acc: (99.97%) (23160/23168)\n",
            "Epoch: 150 | Batch_idx: 190 |  Loss: (0.0067) | Acc: (99.97%) (24440/24448)\n",
            "Epoch: 150 | Batch_idx: 200 |  Loss: (0.0069) | Acc: (99.97%) (25719/25728)\n",
            "Epoch: 150 | Batch_idx: 210 |  Loss: (0.0068) | Acc: (99.96%) (26998/27008)\n",
            "Epoch: 150 | Batch_idx: 220 |  Loss: (0.0068) | Acc: (99.96%) (28276/28288)\n",
            "Epoch: 150 | Batch_idx: 230 |  Loss: (0.0068) | Acc: (99.96%) (29555/29568)\n",
            "Epoch: 150 | Batch_idx: 240 |  Loss: (0.0069) | Acc: (99.95%) (30834/30848)\n",
            "Epoch: 150 | Batch_idx: 250 |  Loss: (0.0068) | Acc: (99.96%) (32114/32128)\n",
            "Epoch: 150 | Batch_idx: 260 |  Loss: (0.0068) | Acc: (99.96%) (33394/33408)\n",
            "Epoch: 150 | Batch_idx: 270 |  Loss: (0.0068) | Acc: (99.96%) (34674/34688)\n",
            "Epoch: 150 | Batch_idx: 280 |  Loss: (0.0067) | Acc: (99.96%) (35954/35968)\n",
            "Epoch: 150 | Batch_idx: 290 |  Loss: (0.0067) | Acc: (99.96%) (37234/37248)\n",
            "Epoch: 150 | Batch_idx: 300 |  Loss: (0.0067) | Acc: (99.96%) (38513/38528)\n",
            "Epoch: 150 | Batch_idx: 310 |  Loss: (0.0066) | Acc: (99.96%) (39793/39808)\n",
            "Epoch: 150 | Batch_idx: 320 |  Loss: (0.0066) | Acc: (99.96%) (41072/41088)\n",
            "Epoch: 150 | Batch_idx: 330 |  Loss: (0.0067) | Acc: (99.96%) (42352/42368)\n",
            "Epoch: 150 | Batch_idx: 340 |  Loss: (0.0067) | Acc: (99.96%) (43632/43648)\n",
            "Epoch: 150 | Batch_idx: 350 |  Loss: (0.0067) | Acc: (99.96%) (44912/44928)\n",
            "Epoch: 150 | Batch_idx: 360 |  Loss: (0.0067) | Acc: (99.96%) (46190/46208)\n",
            "Epoch: 150 | Batch_idx: 370 |  Loss: (0.0067) | Acc: (99.96%) (47470/47488)\n",
            "Epoch: 150 | Batch_idx: 380 |  Loss: (0.0067) | Acc: (99.96%) (48749/48768)\n",
            "Epoch: 150 | Batch_idx: 390 |  Loss: (0.0067) | Acc: (99.96%) (49981/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2572) | Acc: (93.05%) (9305/10000)\n",
            "Epoch: 151 | Batch_idx: 0 |  Loss: (0.0060) | Acc: (100.00%) (128/128)\n",
            "Epoch: 151 | Batch_idx: 10 |  Loss: (0.0061) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 151 | Batch_idx: 20 |  Loss: (0.0060) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 151 | Batch_idx: 30 |  Loss: (0.0064) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 151 | Batch_idx: 40 |  Loss: (0.0064) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 151 | Batch_idx: 50 |  Loss: (0.0064) | Acc: (99.95%) (6525/6528)\n",
            "Epoch: 151 | Batch_idx: 60 |  Loss: (0.0066) | Acc: (99.96%) (7805/7808)\n",
            "Epoch: 151 | Batch_idx: 70 |  Loss: (0.0066) | Acc: (99.97%) (9085/9088)\n",
            "Epoch: 151 | Batch_idx: 80 |  Loss: (0.0066) | Acc: (99.97%) (10365/10368)\n",
            "Epoch: 151 | Batch_idx: 90 |  Loss: (0.0065) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 151 | Batch_idx: 100 |  Loss: (0.0066) | Acc: (99.97%) (12924/12928)\n",
            "Epoch: 151 | Batch_idx: 110 |  Loss: (0.0066) | Acc: (99.97%) (14204/14208)\n",
            "Epoch: 151 | Batch_idx: 120 |  Loss: (0.0066) | Acc: (99.97%) (15484/15488)\n",
            "Epoch: 151 | Batch_idx: 130 |  Loss: (0.0065) | Acc: (99.98%) (16764/16768)\n",
            "Epoch: 151 | Batch_idx: 140 |  Loss: (0.0064) | Acc: (99.98%) (18044/18048)\n",
            "Epoch: 151 | Batch_idx: 150 |  Loss: (0.0066) | Acc: (99.97%) (19323/19328)\n",
            "Epoch: 151 | Batch_idx: 160 |  Loss: (0.0068) | Acc: (99.96%) (20600/20608)\n",
            "Epoch: 151 | Batch_idx: 170 |  Loss: (0.0068) | Acc: (99.96%) (21879/21888)\n",
            "Epoch: 151 | Batch_idx: 180 |  Loss: (0.0069) | Acc: (99.96%) (23158/23168)\n",
            "Epoch: 151 | Batch_idx: 190 |  Loss: (0.0068) | Acc: (99.96%) (24438/24448)\n",
            "Epoch: 151 | Batch_idx: 200 |  Loss: (0.0069) | Acc: (99.96%) (25718/25728)\n",
            "Epoch: 151 | Batch_idx: 210 |  Loss: (0.0069) | Acc: (99.96%) (26996/27008)\n",
            "Epoch: 151 | Batch_idx: 220 |  Loss: (0.0069) | Acc: (99.95%) (28275/28288)\n",
            "Epoch: 151 | Batch_idx: 230 |  Loss: (0.0068) | Acc: (99.96%) (29555/29568)\n",
            "Epoch: 151 | Batch_idx: 240 |  Loss: (0.0068) | Acc: (99.96%) (30835/30848)\n",
            "Epoch: 151 | Batch_idx: 250 |  Loss: (0.0067) | Acc: (99.96%) (32115/32128)\n",
            "Epoch: 151 | Batch_idx: 260 |  Loss: (0.0067) | Acc: (99.96%) (33394/33408)\n",
            "Epoch: 151 | Batch_idx: 270 |  Loss: (0.0067) | Acc: (99.96%) (34674/34688)\n",
            "Epoch: 151 | Batch_idx: 280 |  Loss: (0.0067) | Acc: (99.96%) (35953/35968)\n",
            "Epoch: 151 | Batch_idx: 290 |  Loss: (0.0067) | Acc: (99.96%) (37233/37248)\n",
            "Epoch: 151 | Batch_idx: 300 |  Loss: (0.0067) | Acc: (99.96%) (38513/38528)\n",
            "Epoch: 151 | Batch_idx: 310 |  Loss: (0.0067) | Acc: (99.96%) (39792/39808)\n",
            "Epoch: 151 | Batch_idx: 320 |  Loss: (0.0067) | Acc: (99.96%) (41072/41088)\n",
            "Epoch: 151 | Batch_idx: 330 |  Loss: (0.0067) | Acc: (99.96%) (42352/42368)\n",
            "Epoch: 151 | Batch_idx: 340 |  Loss: (0.0067) | Acc: (99.96%) (43630/43648)\n",
            "Epoch: 151 | Batch_idx: 350 |  Loss: (0.0067) | Acc: (99.96%) (44910/44928)\n",
            "Epoch: 151 | Batch_idx: 360 |  Loss: (0.0067) | Acc: (99.96%) (46190/46208)\n",
            "Epoch: 151 | Batch_idx: 370 |  Loss: (0.0067) | Acc: (99.96%) (47470/47488)\n",
            "Epoch: 151 | Batch_idx: 380 |  Loss: (0.0067) | Acc: (99.96%) (48750/48768)\n",
            "Epoch: 151 | Batch_idx: 390 |  Loss: (0.0067) | Acc: (99.96%) (49982/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2579) | Acc: (92.92%) (9292/10000)\n",
            "Epoch: 152 | Batch_idx: 0 |  Loss: (0.0109) | Acc: (100.00%) (128/128)\n",
            "Epoch: 152 | Batch_idx: 10 |  Loss: (0.0071) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 152 | Batch_idx: 20 |  Loss: (0.0070) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 152 | Batch_idx: 30 |  Loss: (0.0070) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 152 | Batch_idx: 40 |  Loss: (0.0068) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 152 | Batch_idx: 50 |  Loss: (0.0066) | Acc: (99.97%) (6526/6528)\n",
            "Epoch: 152 | Batch_idx: 60 |  Loss: (0.0064) | Acc: (99.97%) (7806/7808)\n",
            "Epoch: 152 | Batch_idx: 70 |  Loss: (0.0065) | Acc: (99.97%) (9085/9088)\n",
            "Epoch: 152 | Batch_idx: 80 |  Loss: (0.0064) | Acc: (99.97%) (10365/10368)\n",
            "Epoch: 152 | Batch_idx: 90 |  Loss: (0.0066) | Acc: (99.94%) (11641/11648)\n",
            "Epoch: 152 | Batch_idx: 100 |  Loss: (0.0065) | Acc: (99.95%) (12921/12928)\n",
            "Epoch: 152 | Batch_idx: 110 |  Loss: (0.0066) | Acc: (99.95%) (14201/14208)\n",
            "Epoch: 152 | Batch_idx: 120 |  Loss: (0.0068) | Acc: (99.93%) (15477/15488)\n",
            "Epoch: 152 | Batch_idx: 130 |  Loss: (0.0067) | Acc: (99.93%) (16757/16768)\n",
            "Epoch: 152 | Batch_idx: 140 |  Loss: (0.0067) | Acc: (99.94%) (18037/18048)\n",
            "Epoch: 152 | Batch_idx: 150 |  Loss: (0.0067) | Acc: (99.94%) (19316/19328)\n",
            "Epoch: 152 | Batch_idx: 160 |  Loss: (0.0068) | Acc: (99.93%) (20594/20608)\n",
            "Epoch: 152 | Batch_idx: 170 |  Loss: (0.0068) | Acc: (99.94%) (21874/21888)\n",
            "Epoch: 152 | Batch_idx: 180 |  Loss: (0.0068) | Acc: (99.94%) (23154/23168)\n",
            "Epoch: 152 | Batch_idx: 190 |  Loss: (0.0067) | Acc: (99.94%) (24433/24448)\n",
            "Epoch: 152 | Batch_idx: 200 |  Loss: (0.0067) | Acc: (99.94%) (25713/25728)\n",
            "Epoch: 152 | Batch_idx: 210 |  Loss: (0.0067) | Acc: (99.94%) (26993/27008)\n",
            "Epoch: 152 | Batch_idx: 220 |  Loss: (0.0067) | Acc: (99.94%) (28272/28288)\n",
            "Epoch: 152 | Batch_idx: 230 |  Loss: (0.0068) | Acc: (99.95%) (29552/29568)\n",
            "Epoch: 152 | Batch_idx: 240 |  Loss: (0.0067) | Acc: (99.94%) (30831/30848)\n",
            "Epoch: 152 | Batch_idx: 250 |  Loss: (0.0067) | Acc: (99.94%) (32110/32128)\n",
            "Epoch: 152 | Batch_idx: 260 |  Loss: (0.0067) | Acc: (99.94%) (33388/33408)\n",
            "Epoch: 152 | Batch_idx: 270 |  Loss: (0.0067) | Acc: (99.94%) (34668/34688)\n",
            "Epoch: 152 | Batch_idx: 280 |  Loss: (0.0067) | Acc: (99.94%) (35948/35968)\n",
            "Epoch: 152 | Batch_idx: 290 |  Loss: (0.0067) | Acc: (99.94%) (37227/37248)\n",
            "Epoch: 152 | Batch_idx: 300 |  Loss: (0.0066) | Acc: (99.95%) (38507/38528)\n",
            "Epoch: 152 | Batch_idx: 310 |  Loss: (0.0066) | Acc: (99.95%) (39787/39808)\n",
            "Epoch: 152 | Batch_idx: 320 |  Loss: (0.0066) | Acc: (99.95%) (41067/41088)\n",
            "Epoch: 152 | Batch_idx: 330 |  Loss: (0.0066) | Acc: (99.95%) (42347/42368)\n",
            "Epoch: 152 | Batch_idx: 340 |  Loss: (0.0066) | Acc: (99.95%) (43627/43648)\n",
            "Epoch: 152 | Batch_idx: 350 |  Loss: (0.0065) | Acc: (99.95%) (44907/44928)\n",
            "Epoch: 152 | Batch_idx: 360 |  Loss: (0.0066) | Acc: (99.95%) (46186/46208)\n",
            "Epoch: 152 | Batch_idx: 370 |  Loss: (0.0066) | Acc: (99.95%) (47465/47488)\n",
            "Epoch: 152 | Batch_idx: 380 |  Loss: (0.0066) | Acc: (99.95%) (48744/48768)\n",
            "Epoch: 152 | Batch_idx: 390 |  Loss: (0.0066) | Acc: (99.95%) (49975/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2586) | Acc: (93.06%) (9306/10000)\n",
            "Epoch: 153 | Batch_idx: 0 |  Loss: (0.0055) | Acc: (100.00%) (128/128)\n",
            "Epoch: 153 | Batch_idx: 10 |  Loss: (0.0048) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 153 | Batch_idx: 20 |  Loss: (0.0057) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 153 | Batch_idx: 30 |  Loss: (0.0058) | Acc: (100.00%) (3968/3968)\n",
            "Epoch: 153 | Batch_idx: 40 |  Loss: (0.0059) | Acc: (100.00%) (5248/5248)\n",
            "Epoch: 153 | Batch_idx: 50 |  Loss: (0.0061) | Acc: (100.00%) (6528/6528)\n",
            "Epoch: 153 | Batch_idx: 60 |  Loss: (0.0059) | Acc: (100.00%) (7808/7808)\n",
            "Epoch: 153 | Batch_idx: 70 |  Loss: (0.0057) | Acc: (100.00%) (9088/9088)\n",
            "Epoch: 153 | Batch_idx: 80 |  Loss: (0.0057) | Acc: (100.00%) (10368/10368)\n",
            "Epoch: 153 | Batch_idx: 90 |  Loss: (0.0058) | Acc: (100.00%) (11648/11648)\n",
            "Epoch: 153 | Batch_idx: 100 |  Loss: (0.0058) | Acc: (99.99%) (12927/12928)\n",
            "Epoch: 153 | Batch_idx: 110 |  Loss: (0.0059) | Acc: (99.99%) (14206/14208)\n",
            "Epoch: 153 | Batch_idx: 120 |  Loss: (0.0061) | Acc: (99.98%) (15485/15488)\n",
            "Epoch: 153 | Batch_idx: 130 |  Loss: (0.0061) | Acc: (99.98%) (16765/16768)\n",
            "Epoch: 153 | Batch_idx: 140 |  Loss: (0.0061) | Acc: (99.98%) (18045/18048)\n",
            "Epoch: 153 | Batch_idx: 150 |  Loss: (0.0061) | Acc: (99.98%) (19324/19328)\n",
            "Epoch: 153 | Batch_idx: 160 |  Loss: (0.0061) | Acc: (99.97%) (20602/20608)\n",
            "Epoch: 153 | Batch_idx: 170 |  Loss: (0.0060) | Acc: (99.97%) (21882/21888)\n",
            "Epoch: 153 | Batch_idx: 180 |  Loss: (0.0060) | Acc: (99.97%) (23162/23168)\n",
            "Epoch: 153 | Batch_idx: 190 |  Loss: (0.0060) | Acc: (99.97%) (24441/24448)\n",
            "Epoch: 153 | Batch_idx: 200 |  Loss: (0.0060) | Acc: (99.97%) (25721/25728)\n",
            "Epoch: 153 | Batch_idx: 210 |  Loss: (0.0061) | Acc: (99.97%) (27001/27008)\n",
            "Epoch: 153 | Batch_idx: 220 |  Loss: (0.0061) | Acc: (99.98%) (28281/28288)\n",
            "Epoch: 153 | Batch_idx: 230 |  Loss: (0.0061) | Acc: (99.98%) (29561/29568)\n",
            "Epoch: 153 | Batch_idx: 240 |  Loss: (0.0061) | Acc: (99.98%) (30841/30848)\n",
            "Epoch: 153 | Batch_idx: 250 |  Loss: (0.0061) | Acc: (99.98%) (32121/32128)\n",
            "Epoch: 153 | Batch_idx: 260 |  Loss: (0.0061) | Acc: (99.98%) (33400/33408)\n",
            "Epoch: 153 | Batch_idx: 270 |  Loss: (0.0061) | Acc: (99.97%) (34679/34688)\n",
            "Epoch: 153 | Batch_idx: 280 |  Loss: (0.0062) | Acc: (99.97%) (35958/35968)\n",
            "Epoch: 153 | Batch_idx: 290 |  Loss: (0.0062) | Acc: (99.97%) (37237/37248)\n",
            "Epoch: 153 | Batch_idx: 300 |  Loss: (0.0062) | Acc: (99.97%) (38517/38528)\n",
            "Epoch: 153 | Batch_idx: 310 |  Loss: (0.0062) | Acc: (99.97%) (39797/39808)\n",
            "Epoch: 153 | Batch_idx: 320 |  Loss: (0.0062) | Acc: (99.97%) (41077/41088)\n",
            "Epoch: 153 | Batch_idx: 330 |  Loss: (0.0062) | Acc: (99.97%) (42356/42368)\n",
            "Epoch: 153 | Batch_idx: 340 |  Loss: (0.0062) | Acc: (99.97%) (43635/43648)\n",
            "Epoch: 153 | Batch_idx: 350 |  Loss: (0.0062) | Acc: (99.97%) (44915/44928)\n",
            "Epoch: 153 | Batch_idx: 360 |  Loss: (0.0063) | Acc: (99.97%) (46195/46208)\n",
            "Epoch: 153 | Batch_idx: 370 |  Loss: (0.0062) | Acc: (99.97%) (47475/47488)\n",
            "Epoch: 153 | Batch_idx: 380 |  Loss: (0.0063) | Acc: (99.97%) (48755/48768)\n",
            "Epoch: 153 | Batch_idx: 390 |  Loss: (0.0063) | Acc: (99.97%) (49987/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2585) | Acc: (92.93%) (9293/10000)\n",
            "Epoch: 154 | Batch_idx: 0 |  Loss: (0.0053) | Acc: (100.00%) (128/128)\n",
            "Epoch: 154 | Batch_idx: 10 |  Loss: (0.0071) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 154 | Batch_idx: 20 |  Loss: (0.0070) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 154 | Batch_idx: 30 |  Loss: (0.0070) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 154 | Batch_idx: 40 |  Loss: (0.0071) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 154 | Batch_idx: 50 |  Loss: (0.0068) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 154 | Batch_idx: 60 |  Loss: (0.0065) | Acc: (99.99%) (7807/7808)\n",
            "Epoch: 154 | Batch_idx: 70 |  Loss: (0.0067) | Acc: (99.98%) (9086/9088)\n",
            "Epoch: 154 | Batch_idx: 80 |  Loss: (0.0069) | Acc: (99.97%) (10365/10368)\n",
            "Epoch: 154 | Batch_idx: 90 |  Loss: (0.0069) | Acc: (99.97%) (11645/11648)\n",
            "Epoch: 154 | Batch_idx: 100 |  Loss: (0.0068) | Acc: (99.98%) (12925/12928)\n",
            "Epoch: 154 | Batch_idx: 110 |  Loss: (0.0068) | Acc: (99.98%) (14205/14208)\n",
            "Epoch: 154 | Batch_idx: 120 |  Loss: (0.0068) | Acc: (99.98%) (15485/15488)\n",
            "Epoch: 154 | Batch_idx: 130 |  Loss: (0.0070) | Acc: (99.98%) (16765/16768)\n",
            "Epoch: 154 | Batch_idx: 140 |  Loss: (0.0069) | Acc: (99.98%) (18045/18048)\n",
            "Epoch: 154 | Batch_idx: 150 |  Loss: (0.0069) | Acc: (99.98%) (19325/19328)\n",
            "Epoch: 154 | Batch_idx: 160 |  Loss: (0.0069) | Acc: (99.99%) (20605/20608)\n",
            "Epoch: 154 | Batch_idx: 170 |  Loss: (0.0069) | Acc: (99.99%) (21885/21888)\n",
            "Epoch: 154 | Batch_idx: 180 |  Loss: (0.0069) | Acc: (99.98%) (23164/23168)\n",
            "Epoch: 154 | Batch_idx: 190 |  Loss: (0.0068) | Acc: (99.98%) (24444/24448)\n",
            "Epoch: 154 | Batch_idx: 200 |  Loss: (0.0068) | Acc: (99.98%) (25723/25728)\n",
            "Epoch: 154 | Batch_idx: 210 |  Loss: (0.0068) | Acc: (99.98%) (27003/27008)\n",
            "Epoch: 154 | Batch_idx: 220 |  Loss: (0.0067) | Acc: (99.98%) (28283/28288)\n",
            "Epoch: 154 | Batch_idx: 230 |  Loss: (0.0067) | Acc: (99.98%) (29561/29568)\n",
            "Epoch: 154 | Batch_idx: 240 |  Loss: (0.0068) | Acc: (99.97%) (30839/30848)\n",
            "Epoch: 154 | Batch_idx: 250 |  Loss: (0.0067) | Acc: (99.97%) (32118/32128)\n",
            "Epoch: 154 | Batch_idx: 260 |  Loss: (0.0066) | Acc: (99.97%) (33398/33408)\n",
            "Epoch: 154 | Batch_idx: 270 |  Loss: (0.0066) | Acc: (99.97%) (34677/34688)\n",
            "Epoch: 154 | Batch_idx: 280 |  Loss: (0.0067) | Acc: (99.97%) (35957/35968)\n",
            "Epoch: 154 | Batch_idx: 290 |  Loss: (0.0066) | Acc: (99.97%) (37237/37248)\n",
            "Epoch: 154 | Batch_idx: 300 |  Loss: (0.0066) | Acc: (99.97%) (38517/38528)\n",
            "Epoch: 154 | Batch_idx: 310 |  Loss: (0.0066) | Acc: (99.97%) (39797/39808)\n",
            "Epoch: 154 | Batch_idx: 320 |  Loss: (0.0066) | Acc: (99.97%) (41075/41088)\n",
            "Epoch: 154 | Batch_idx: 330 |  Loss: (0.0066) | Acc: (99.97%) (42354/42368)\n",
            "Epoch: 154 | Batch_idx: 340 |  Loss: (0.0066) | Acc: (99.97%) (43633/43648)\n",
            "Epoch: 154 | Batch_idx: 350 |  Loss: (0.0066) | Acc: (99.97%) (44913/44928)\n",
            "Epoch: 154 | Batch_idx: 360 |  Loss: (0.0065) | Acc: (99.97%) (46193/46208)\n",
            "Epoch: 154 | Batch_idx: 370 |  Loss: (0.0065) | Acc: (99.97%) (47472/47488)\n",
            "Epoch: 154 | Batch_idx: 380 |  Loss: (0.0065) | Acc: (99.97%) (48752/48768)\n",
            "Epoch: 154 | Batch_idx: 390 |  Loss: (0.0065) | Acc: (99.97%) (49983/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2582) | Acc: (92.95%) (9295/10000)\n",
            "Epoch: 155 | Batch_idx: 0 |  Loss: (0.0054) | Acc: (100.00%) (128/128)\n",
            "Epoch: 155 | Batch_idx: 10 |  Loss: (0.0054) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 155 | Batch_idx: 20 |  Loss: (0.0068) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 155 | Batch_idx: 30 |  Loss: (0.0066) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 155 | Batch_idx: 40 |  Loss: (0.0065) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 155 | Batch_idx: 50 |  Loss: (0.0063) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 155 | Batch_idx: 60 |  Loss: (0.0064) | Acc: (99.97%) (7806/7808)\n",
            "Epoch: 155 | Batch_idx: 70 |  Loss: (0.0063) | Acc: (99.98%) (9086/9088)\n",
            "Epoch: 155 | Batch_idx: 80 |  Loss: (0.0064) | Acc: (99.97%) (10365/10368)\n",
            "Epoch: 155 | Batch_idx: 90 |  Loss: (0.0063) | Acc: (99.97%) (11645/11648)\n",
            "Epoch: 155 | Batch_idx: 100 |  Loss: (0.0063) | Acc: (99.97%) (12924/12928)\n",
            "Epoch: 155 | Batch_idx: 110 |  Loss: (0.0062) | Acc: (99.97%) (14204/14208)\n",
            "Epoch: 155 | Batch_idx: 120 |  Loss: (0.0062) | Acc: (99.97%) (15483/15488)\n",
            "Epoch: 155 | Batch_idx: 130 |  Loss: (0.0062) | Acc: (99.96%) (16762/16768)\n",
            "Epoch: 155 | Batch_idx: 140 |  Loss: (0.0061) | Acc: (99.97%) (18042/18048)\n",
            "Epoch: 155 | Batch_idx: 150 |  Loss: (0.0062) | Acc: (99.97%) (19322/19328)\n",
            "Epoch: 155 | Batch_idx: 160 |  Loss: (0.0063) | Acc: (99.97%) (20601/20608)\n",
            "Epoch: 155 | Batch_idx: 170 |  Loss: (0.0063) | Acc: (99.97%) (21881/21888)\n",
            "Epoch: 155 | Batch_idx: 180 |  Loss: (0.0064) | Acc: (99.96%) (23159/23168)\n",
            "Epoch: 155 | Batch_idx: 190 |  Loss: (0.0065) | Acc: (99.96%) (24438/24448)\n",
            "Epoch: 155 | Batch_idx: 200 |  Loss: (0.0065) | Acc: (99.96%) (25718/25728)\n",
            "Epoch: 155 | Batch_idx: 210 |  Loss: (0.0065) | Acc: (99.96%) (26998/27008)\n",
            "Epoch: 155 | Batch_idx: 220 |  Loss: (0.0065) | Acc: (99.96%) (28278/28288)\n",
            "Epoch: 155 | Batch_idx: 230 |  Loss: (0.0066) | Acc: (99.96%) (29557/29568)\n",
            "Epoch: 155 | Batch_idx: 240 |  Loss: (0.0066) | Acc: (99.96%) (30836/30848)\n",
            "Epoch: 155 | Batch_idx: 250 |  Loss: (0.0066) | Acc: (99.96%) (32116/32128)\n",
            "Epoch: 155 | Batch_idx: 260 |  Loss: (0.0065) | Acc: (99.96%) (33396/33408)\n",
            "Epoch: 155 | Batch_idx: 270 |  Loss: (0.0065) | Acc: (99.97%) (34676/34688)\n",
            "Epoch: 155 | Batch_idx: 280 |  Loss: (0.0065) | Acc: (99.97%) (35956/35968)\n",
            "Epoch: 155 | Batch_idx: 290 |  Loss: (0.0065) | Acc: (99.97%) (37235/37248)\n",
            "Epoch: 155 | Batch_idx: 300 |  Loss: (0.0064) | Acc: (99.97%) (38515/38528)\n",
            "Epoch: 155 | Batch_idx: 310 |  Loss: (0.0064) | Acc: (99.96%) (39794/39808)\n",
            "Epoch: 155 | Batch_idx: 320 |  Loss: (0.0064) | Acc: (99.97%) (41074/41088)\n",
            "Epoch: 155 | Batch_idx: 330 |  Loss: (0.0064) | Acc: (99.97%) (42354/42368)\n",
            "Epoch: 155 | Batch_idx: 340 |  Loss: (0.0065) | Acc: (99.97%) (43633/43648)\n",
            "Epoch: 155 | Batch_idx: 350 |  Loss: (0.0065) | Acc: (99.97%) (44913/44928)\n",
            "Epoch: 155 | Batch_idx: 360 |  Loss: (0.0065) | Acc: (99.97%) (46193/46208)\n",
            "Epoch: 155 | Batch_idx: 370 |  Loss: (0.0065) | Acc: (99.97%) (47472/47488)\n",
            "Epoch: 155 | Batch_idx: 380 |  Loss: (0.0065) | Acc: (99.97%) (48752/48768)\n",
            "Epoch: 155 | Batch_idx: 390 |  Loss: (0.0065) | Acc: (99.97%) (49984/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2580) | Acc: (92.92%) (9292/10000)\n",
            "Epoch: 156 | Batch_idx: 0 |  Loss: (0.0081) | Acc: (100.00%) (128/128)\n",
            "Epoch: 156 | Batch_idx: 10 |  Loss: (0.0061) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 156 | Batch_idx: 20 |  Loss: (0.0069) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 156 | Batch_idx: 30 |  Loss: (0.0064) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 156 | Batch_idx: 40 |  Loss: (0.0063) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 156 | Batch_idx: 50 |  Loss: (0.0060) | Acc: (99.97%) (6526/6528)\n",
            "Epoch: 156 | Batch_idx: 60 |  Loss: (0.0059) | Acc: (99.97%) (7806/7808)\n",
            "Epoch: 156 | Batch_idx: 70 |  Loss: (0.0063) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 156 | Batch_idx: 80 |  Loss: (0.0064) | Acc: (99.95%) (10363/10368)\n",
            "Epoch: 156 | Batch_idx: 90 |  Loss: (0.0063) | Acc: (99.96%) (11643/11648)\n",
            "Epoch: 156 | Batch_idx: 100 |  Loss: (0.0066) | Acc: (99.95%) (12921/12928)\n",
            "Epoch: 156 | Batch_idx: 110 |  Loss: (0.0066) | Acc: (99.95%) (14201/14208)\n",
            "Epoch: 156 | Batch_idx: 120 |  Loss: (0.0065) | Acc: (99.95%) (15481/15488)\n",
            "Epoch: 156 | Batch_idx: 130 |  Loss: (0.0065) | Acc: (99.95%) (16760/16768)\n",
            "Epoch: 156 | Batch_idx: 140 |  Loss: (0.0066) | Acc: (99.96%) (18040/18048)\n",
            "Epoch: 156 | Batch_idx: 150 |  Loss: (0.0066) | Acc: (99.96%) (19320/19328)\n",
            "Epoch: 156 | Batch_idx: 160 |  Loss: (0.0066) | Acc: (99.96%) (20599/20608)\n",
            "Epoch: 156 | Batch_idx: 170 |  Loss: (0.0066) | Acc: (99.96%) (21879/21888)\n",
            "Epoch: 156 | Batch_idx: 180 |  Loss: (0.0065) | Acc: (99.96%) (23159/23168)\n",
            "Epoch: 156 | Batch_idx: 190 |  Loss: (0.0065) | Acc: (99.96%) (24439/24448)\n",
            "Epoch: 156 | Batch_idx: 200 |  Loss: (0.0065) | Acc: (99.97%) (25719/25728)\n",
            "Epoch: 156 | Batch_idx: 210 |  Loss: (0.0065) | Acc: (99.96%) (26998/27008)\n",
            "Epoch: 156 | Batch_idx: 220 |  Loss: (0.0065) | Acc: (99.96%) (28276/28288)\n",
            "Epoch: 156 | Batch_idx: 230 |  Loss: (0.0065) | Acc: (99.96%) (29556/29568)\n",
            "Epoch: 156 | Batch_idx: 240 |  Loss: (0.0065) | Acc: (99.96%) (30835/30848)\n",
            "Epoch: 156 | Batch_idx: 250 |  Loss: (0.0065) | Acc: (99.96%) (32114/32128)\n",
            "Epoch: 156 | Batch_idx: 260 |  Loss: (0.0066) | Acc: (99.96%) (33393/33408)\n",
            "Epoch: 156 | Batch_idx: 270 |  Loss: (0.0065) | Acc: (99.96%) (34673/34688)\n",
            "Epoch: 156 | Batch_idx: 280 |  Loss: (0.0065) | Acc: (99.96%) (35953/35968)\n",
            "Epoch: 156 | Batch_idx: 290 |  Loss: (0.0065) | Acc: (99.96%) (37233/37248)\n",
            "Epoch: 156 | Batch_idx: 300 |  Loss: (0.0065) | Acc: (99.96%) (38513/38528)\n",
            "Epoch: 156 | Batch_idx: 310 |  Loss: (0.0064) | Acc: (99.96%) (39793/39808)\n",
            "Epoch: 156 | Batch_idx: 320 |  Loss: (0.0064) | Acc: (99.96%) (41073/41088)\n",
            "Epoch: 156 | Batch_idx: 330 |  Loss: (0.0064) | Acc: (99.96%) (42353/42368)\n",
            "Epoch: 156 | Batch_idx: 340 |  Loss: (0.0064) | Acc: (99.97%) (43633/43648)\n",
            "Epoch: 156 | Batch_idx: 350 |  Loss: (0.0064) | Acc: (99.96%) (44912/44928)\n",
            "Epoch: 156 | Batch_idx: 360 |  Loss: (0.0064) | Acc: (99.97%) (46192/46208)\n",
            "Epoch: 156 | Batch_idx: 370 |  Loss: (0.0064) | Acc: (99.97%) (47472/47488)\n",
            "Epoch: 156 | Batch_idx: 380 |  Loss: (0.0064) | Acc: (99.97%) (48751/48768)\n",
            "Epoch: 156 | Batch_idx: 390 |  Loss: (0.0065) | Acc: (99.96%) (49980/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2593) | Acc: (92.92%) (9292/10000)\n",
            "Epoch: 157 | Batch_idx: 0 |  Loss: (0.0049) | Acc: (100.00%) (128/128)\n",
            "Epoch: 157 | Batch_idx: 10 |  Loss: (0.0061) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 157 | Batch_idx: 20 |  Loss: (0.0063) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 157 | Batch_idx: 30 |  Loss: (0.0061) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 157 | Batch_idx: 40 |  Loss: (0.0063) | Acc: (99.92%) (5244/5248)\n",
            "Epoch: 157 | Batch_idx: 50 |  Loss: (0.0061) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 157 | Batch_idx: 60 |  Loss: (0.0060) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 157 | Batch_idx: 70 |  Loss: (0.0062) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 157 | Batch_idx: 80 |  Loss: (0.0060) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 157 | Batch_idx: 90 |  Loss: (0.0064) | Acc: (99.96%) (11643/11648)\n",
            "Epoch: 157 | Batch_idx: 100 |  Loss: (0.0066) | Acc: (99.95%) (12921/12928)\n",
            "Epoch: 157 | Batch_idx: 110 |  Loss: (0.0064) | Acc: (99.95%) (14201/14208)\n",
            "Epoch: 157 | Batch_idx: 120 |  Loss: (0.0063) | Acc: (99.95%) (15481/15488)\n",
            "Epoch: 157 | Batch_idx: 130 |  Loss: (0.0063) | Acc: (99.95%) (16760/16768)\n",
            "Epoch: 157 | Batch_idx: 140 |  Loss: (0.0063) | Acc: (99.96%) (18040/18048)\n",
            "Epoch: 157 | Batch_idx: 150 |  Loss: (0.0064) | Acc: (99.96%) (19320/19328)\n",
            "Epoch: 157 | Batch_idx: 160 |  Loss: (0.0066) | Acc: (99.96%) (20599/20608)\n",
            "Epoch: 157 | Batch_idx: 170 |  Loss: (0.0065) | Acc: (99.96%) (21879/21888)\n",
            "Epoch: 157 | Batch_idx: 180 |  Loss: (0.0065) | Acc: (99.96%) (23159/23168)\n",
            "Epoch: 157 | Batch_idx: 190 |  Loss: (0.0065) | Acc: (99.96%) (24439/24448)\n",
            "Epoch: 157 | Batch_idx: 200 |  Loss: (0.0065) | Acc: (99.96%) (25718/25728)\n",
            "Epoch: 157 | Batch_idx: 210 |  Loss: (0.0064) | Acc: (99.96%) (26998/27008)\n",
            "Epoch: 157 | Batch_idx: 220 |  Loss: (0.0065) | Acc: (99.96%) (28278/28288)\n",
            "Epoch: 157 | Batch_idx: 230 |  Loss: (0.0064) | Acc: (99.97%) (29558/29568)\n",
            "Epoch: 157 | Batch_idx: 240 |  Loss: (0.0065) | Acc: (99.96%) (30837/30848)\n",
            "Epoch: 157 | Batch_idx: 250 |  Loss: (0.0065) | Acc: (99.97%) (32117/32128)\n",
            "Epoch: 157 | Batch_idx: 260 |  Loss: (0.0065) | Acc: (99.97%) (33397/33408)\n",
            "Epoch: 157 | Batch_idx: 270 |  Loss: (0.0065) | Acc: (99.97%) (34677/34688)\n",
            "Epoch: 157 | Batch_idx: 280 |  Loss: (0.0066) | Acc: (99.96%) (35952/35968)\n",
            "Epoch: 157 | Batch_idx: 290 |  Loss: (0.0066) | Acc: (99.95%) (37231/37248)\n",
            "Epoch: 157 | Batch_idx: 300 |  Loss: (0.0067) | Acc: (99.95%) (38510/38528)\n",
            "Epoch: 157 | Batch_idx: 310 |  Loss: (0.0066) | Acc: (99.95%) (39789/39808)\n",
            "Epoch: 157 | Batch_idx: 320 |  Loss: (0.0066) | Acc: (99.95%) (41069/41088)\n",
            "Epoch: 157 | Batch_idx: 330 |  Loss: (0.0066) | Acc: (99.95%) (42348/42368)\n",
            "Epoch: 157 | Batch_idx: 340 |  Loss: (0.0065) | Acc: (99.95%) (43628/43648)\n",
            "Epoch: 157 | Batch_idx: 350 |  Loss: (0.0065) | Acc: (99.96%) (44908/44928)\n",
            "Epoch: 157 | Batch_idx: 360 |  Loss: (0.0065) | Acc: (99.96%) (46188/46208)\n",
            "Epoch: 157 | Batch_idx: 370 |  Loss: (0.0065) | Acc: (99.96%) (47468/47488)\n",
            "Epoch: 157 | Batch_idx: 380 |  Loss: (0.0064) | Acc: (99.96%) (48748/48768)\n",
            "Epoch: 157 | Batch_idx: 390 |  Loss: (0.0064) | Acc: (99.96%) (49979/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2591) | Acc: (93.02%) (9302/10000)\n",
            "Epoch: 158 | Batch_idx: 0 |  Loss: (0.0059) | Acc: (100.00%) (128/128)\n",
            "Epoch: 158 | Batch_idx: 10 |  Loss: (0.0056) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 158 | Batch_idx: 20 |  Loss: (0.0073) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 158 | Batch_idx: 30 |  Loss: (0.0068) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 158 | Batch_idx: 40 |  Loss: (0.0071) | Acc: (99.94%) (5245/5248)\n",
            "Epoch: 158 | Batch_idx: 50 |  Loss: (0.0069) | Acc: (99.95%) (6525/6528)\n",
            "Epoch: 158 | Batch_idx: 60 |  Loss: (0.0069) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 158 | Batch_idx: 70 |  Loss: (0.0068) | Acc: (99.96%) (9084/9088)\n",
            "Epoch: 158 | Batch_idx: 80 |  Loss: (0.0065) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 158 | Batch_idx: 90 |  Loss: (0.0065) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 158 | Batch_idx: 100 |  Loss: (0.0064) | Acc: (99.97%) (12924/12928)\n",
            "Epoch: 158 | Batch_idx: 110 |  Loss: (0.0062) | Acc: (99.97%) (14204/14208)\n",
            "Epoch: 158 | Batch_idx: 120 |  Loss: (0.0063) | Acc: (99.97%) (15484/15488)\n",
            "Epoch: 158 | Batch_idx: 130 |  Loss: (0.0063) | Acc: (99.97%) (16763/16768)\n",
            "Epoch: 158 | Batch_idx: 140 |  Loss: (0.0065) | Acc: (99.97%) (18042/18048)\n",
            "Epoch: 158 | Batch_idx: 150 |  Loss: (0.0065) | Acc: (99.97%) (19322/19328)\n",
            "Epoch: 158 | Batch_idx: 160 |  Loss: (0.0065) | Acc: (99.97%) (20601/20608)\n",
            "Epoch: 158 | Batch_idx: 170 |  Loss: (0.0065) | Acc: (99.97%) (21881/21888)\n",
            "Epoch: 158 | Batch_idx: 180 |  Loss: (0.0066) | Acc: (99.97%) (23161/23168)\n",
            "Epoch: 158 | Batch_idx: 190 |  Loss: (0.0066) | Acc: (99.96%) (24439/24448)\n",
            "Epoch: 158 | Batch_idx: 200 |  Loss: (0.0065) | Acc: (99.97%) (25719/25728)\n",
            "Epoch: 158 | Batch_idx: 210 |  Loss: (0.0066) | Acc: (99.96%) (26998/27008)\n",
            "Epoch: 158 | Batch_idx: 220 |  Loss: (0.0066) | Acc: (99.96%) (28278/28288)\n",
            "Epoch: 158 | Batch_idx: 230 |  Loss: (0.0066) | Acc: (99.97%) (29558/29568)\n",
            "Epoch: 158 | Batch_idx: 240 |  Loss: (0.0066) | Acc: (99.96%) (30837/30848)\n",
            "Epoch: 158 | Batch_idx: 250 |  Loss: (0.0066) | Acc: (99.97%) (32117/32128)\n",
            "Epoch: 158 | Batch_idx: 260 |  Loss: (0.0065) | Acc: (99.97%) (33397/33408)\n",
            "Epoch: 158 | Batch_idx: 270 |  Loss: (0.0065) | Acc: (99.97%) (34676/34688)\n",
            "Epoch: 158 | Batch_idx: 280 |  Loss: (0.0065) | Acc: (99.97%) (35956/35968)\n",
            "Epoch: 158 | Batch_idx: 290 |  Loss: (0.0065) | Acc: (99.97%) (37236/37248)\n",
            "Epoch: 158 | Batch_idx: 300 |  Loss: (0.0065) | Acc: (99.97%) (38516/38528)\n",
            "Epoch: 158 | Batch_idx: 310 |  Loss: (0.0065) | Acc: (99.97%) (39795/39808)\n",
            "Epoch: 158 | Batch_idx: 320 |  Loss: (0.0065) | Acc: (99.97%) (41075/41088)\n",
            "Epoch: 158 | Batch_idx: 330 |  Loss: (0.0065) | Acc: (99.97%) (42355/42368)\n",
            "Epoch: 158 | Batch_idx: 340 |  Loss: (0.0065) | Acc: (99.97%) (43634/43648)\n",
            "Epoch: 158 | Batch_idx: 350 |  Loss: (0.0065) | Acc: (99.97%) (44913/44928)\n",
            "Epoch: 158 | Batch_idx: 360 |  Loss: (0.0066) | Acc: (99.96%) (46191/46208)\n",
            "Epoch: 158 | Batch_idx: 370 |  Loss: (0.0066) | Acc: (99.96%) (47470/47488)\n",
            "Epoch: 158 | Batch_idx: 380 |  Loss: (0.0065) | Acc: (99.96%) (48750/48768)\n",
            "Epoch: 158 | Batch_idx: 390 |  Loss: (0.0065) | Acc: (99.96%) (49982/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2574) | Acc: (92.93%) (9293/10000)\n",
            "Epoch: 159 | Batch_idx: 0 |  Loss: (0.0030) | Acc: (100.00%) (128/128)\n",
            "Epoch: 159 | Batch_idx: 10 |  Loss: (0.0063) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 159 | Batch_idx: 20 |  Loss: (0.0058) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 159 | Batch_idx: 30 |  Loss: (0.0060) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 159 | Batch_idx: 40 |  Loss: (0.0061) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 159 | Batch_idx: 50 |  Loss: (0.0060) | Acc: (99.97%) (6526/6528)\n",
            "Epoch: 159 | Batch_idx: 60 |  Loss: (0.0061) | Acc: (99.96%) (7805/7808)\n",
            "Epoch: 159 | Batch_idx: 70 |  Loss: (0.0059) | Acc: (99.97%) (9085/9088)\n",
            "Epoch: 159 | Batch_idx: 80 |  Loss: (0.0060) | Acc: (99.96%) (10364/10368)\n",
            "Epoch: 159 | Batch_idx: 90 |  Loss: (0.0060) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 159 | Batch_idx: 100 |  Loss: (0.0061) | Acc: (99.97%) (12924/12928)\n",
            "Epoch: 159 | Batch_idx: 110 |  Loss: (0.0061) | Acc: (99.97%) (14204/14208)\n",
            "Epoch: 159 | Batch_idx: 120 |  Loss: (0.0062) | Acc: (99.97%) (15483/15488)\n",
            "Epoch: 159 | Batch_idx: 130 |  Loss: (0.0064) | Acc: (99.96%) (16762/16768)\n",
            "Epoch: 159 | Batch_idx: 140 |  Loss: (0.0063) | Acc: (99.97%) (18042/18048)\n",
            "Epoch: 159 | Batch_idx: 150 |  Loss: (0.0064) | Acc: (99.97%) (19322/19328)\n",
            "Epoch: 159 | Batch_idx: 160 |  Loss: (0.0063) | Acc: (99.97%) (20602/20608)\n",
            "Epoch: 159 | Batch_idx: 170 |  Loss: (0.0064) | Acc: (99.97%) (21881/21888)\n",
            "Epoch: 159 | Batch_idx: 180 |  Loss: (0.0065) | Acc: (99.96%) (23159/23168)\n",
            "Epoch: 159 | Batch_idx: 190 |  Loss: (0.0065) | Acc: (99.96%) (24438/24448)\n",
            "Epoch: 159 | Batch_idx: 200 |  Loss: (0.0065) | Acc: (99.96%) (25718/25728)\n",
            "Epoch: 159 | Batch_idx: 210 |  Loss: (0.0065) | Acc: (99.96%) (26997/27008)\n",
            "Epoch: 159 | Batch_idx: 220 |  Loss: (0.0065) | Acc: (99.96%) (28277/28288)\n",
            "Epoch: 159 | Batch_idx: 230 |  Loss: (0.0065) | Acc: (99.96%) (29556/29568)\n",
            "Epoch: 159 | Batch_idx: 240 |  Loss: (0.0064) | Acc: (99.96%) (30835/30848)\n",
            "Epoch: 159 | Batch_idx: 250 |  Loss: (0.0064) | Acc: (99.96%) (32115/32128)\n",
            "Epoch: 159 | Batch_idx: 260 |  Loss: (0.0064) | Acc: (99.96%) (33395/33408)\n",
            "Epoch: 159 | Batch_idx: 270 |  Loss: (0.0064) | Acc: (99.96%) (34675/34688)\n",
            "Epoch: 159 | Batch_idx: 280 |  Loss: (0.0064) | Acc: (99.96%) (35954/35968)\n",
            "Epoch: 159 | Batch_idx: 290 |  Loss: (0.0064) | Acc: (99.96%) (37234/37248)\n",
            "Epoch: 159 | Batch_idx: 300 |  Loss: (0.0064) | Acc: (99.96%) (38512/38528)\n",
            "Epoch: 159 | Batch_idx: 310 |  Loss: (0.0064) | Acc: (99.96%) (39792/39808)\n",
            "Epoch: 159 | Batch_idx: 320 |  Loss: (0.0064) | Acc: (99.96%) (41072/41088)\n",
            "Epoch: 159 | Batch_idx: 330 |  Loss: (0.0065) | Acc: (99.96%) (42352/42368)\n",
            "Epoch: 159 | Batch_idx: 340 |  Loss: (0.0064) | Acc: (99.96%) (43632/43648)\n",
            "Epoch: 159 | Batch_idx: 350 |  Loss: (0.0064) | Acc: (99.96%) (44912/44928)\n",
            "Epoch: 159 | Batch_idx: 360 |  Loss: (0.0064) | Acc: (99.97%) (46192/46208)\n",
            "Epoch: 159 | Batch_idx: 370 |  Loss: (0.0064) | Acc: (99.96%) (47471/47488)\n",
            "Epoch: 159 | Batch_idx: 380 |  Loss: (0.0064) | Acc: (99.97%) (48751/48768)\n",
            "Epoch: 159 | Batch_idx: 390 |  Loss: (0.0065) | Acc: (99.97%) (49983/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2583) | Acc: (92.98%) (9298/10000)\n",
            "Epoch: 160 | Batch_idx: 0 |  Loss: (0.0032) | Acc: (100.00%) (128/128)\n",
            "Epoch: 160 | Batch_idx: 10 |  Loss: (0.0057) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 160 | Batch_idx: 20 |  Loss: (0.0063) | Acc: (99.96%) (2687/2688)\n",
            "Epoch: 160 | Batch_idx: 30 |  Loss: (0.0063) | Acc: (99.97%) (3967/3968)\n",
            "Epoch: 160 | Batch_idx: 40 |  Loss: (0.0063) | Acc: (99.98%) (5247/5248)\n",
            "Epoch: 160 | Batch_idx: 50 |  Loss: (0.0062) | Acc: (99.98%) (6527/6528)\n",
            "Epoch: 160 | Batch_idx: 60 |  Loss: (0.0061) | Acc: (99.99%) (7807/7808)\n",
            "Epoch: 160 | Batch_idx: 70 |  Loss: (0.0062) | Acc: (99.99%) (9087/9088)\n",
            "Epoch: 160 | Batch_idx: 80 |  Loss: (0.0061) | Acc: (99.99%) (10367/10368)\n",
            "Epoch: 160 | Batch_idx: 90 |  Loss: (0.0062) | Acc: (99.97%) (11645/11648)\n",
            "Epoch: 160 | Batch_idx: 100 |  Loss: (0.0061) | Acc: (99.98%) (12925/12928)\n",
            "Epoch: 160 | Batch_idx: 110 |  Loss: (0.0061) | Acc: (99.98%) (14205/14208)\n",
            "Epoch: 160 | Batch_idx: 120 |  Loss: (0.0061) | Acc: (99.98%) (15485/15488)\n",
            "Epoch: 160 | Batch_idx: 130 |  Loss: (0.0062) | Acc: (99.98%) (16764/16768)\n",
            "Epoch: 160 | Batch_idx: 140 |  Loss: (0.0063) | Acc: (99.97%) (18043/18048)\n",
            "Epoch: 160 | Batch_idx: 150 |  Loss: (0.0062) | Acc: (99.97%) (19323/19328)\n",
            "Epoch: 160 | Batch_idx: 160 |  Loss: (0.0062) | Acc: (99.98%) (20603/20608)\n",
            "Epoch: 160 | Batch_idx: 170 |  Loss: (0.0063) | Acc: (99.98%) (21883/21888)\n",
            "Epoch: 160 | Batch_idx: 180 |  Loss: (0.0063) | Acc: (99.98%) (23163/23168)\n",
            "Epoch: 160 | Batch_idx: 190 |  Loss: (0.0063) | Acc: (99.98%) (24443/24448)\n",
            "Epoch: 160 | Batch_idx: 200 |  Loss: (0.0063) | Acc: (99.98%) (25723/25728)\n",
            "Epoch: 160 | Batch_idx: 210 |  Loss: (0.0062) | Acc: (99.98%) (27003/27008)\n",
            "Epoch: 160 | Batch_idx: 220 |  Loss: (0.0062) | Acc: (99.98%) (28283/28288)\n",
            "Epoch: 160 | Batch_idx: 230 |  Loss: (0.0062) | Acc: (99.98%) (29562/29568)\n",
            "Epoch: 160 | Batch_idx: 240 |  Loss: (0.0063) | Acc: (99.97%) (30840/30848)\n",
            "Epoch: 160 | Batch_idx: 250 |  Loss: (0.0064) | Acc: (99.97%) (32119/32128)\n",
            "Epoch: 160 | Batch_idx: 260 |  Loss: (0.0064) | Acc: (99.97%) (33399/33408)\n",
            "Epoch: 160 | Batch_idx: 270 |  Loss: (0.0063) | Acc: (99.97%) (34679/34688)\n",
            "Epoch: 160 | Batch_idx: 280 |  Loss: (0.0063) | Acc: (99.97%) (35959/35968)\n",
            "Epoch: 160 | Batch_idx: 290 |  Loss: (0.0064) | Acc: (99.97%) (37238/37248)\n",
            "Epoch: 160 | Batch_idx: 300 |  Loss: (0.0064) | Acc: (99.97%) (38517/38528)\n",
            "Epoch: 160 | Batch_idx: 310 |  Loss: (0.0064) | Acc: (99.97%) (39797/39808)\n",
            "Epoch: 160 | Batch_idx: 320 |  Loss: (0.0064) | Acc: (99.97%) (41077/41088)\n",
            "Epoch: 160 | Batch_idx: 330 |  Loss: (0.0064) | Acc: (99.97%) (42356/42368)\n",
            "Epoch: 160 | Batch_idx: 340 |  Loss: (0.0064) | Acc: (99.97%) (43636/43648)\n",
            "Epoch: 160 | Batch_idx: 350 |  Loss: (0.0064) | Acc: (99.97%) (44916/44928)\n",
            "Epoch: 160 | Batch_idx: 360 |  Loss: (0.0064) | Acc: (99.97%) (46195/46208)\n",
            "Epoch: 160 | Batch_idx: 370 |  Loss: (0.0065) | Acc: (99.97%) (47472/47488)\n",
            "Epoch: 160 | Batch_idx: 380 |  Loss: (0.0065) | Acc: (99.97%) (48752/48768)\n",
            "Epoch: 160 | Batch_idx: 390 |  Loss: (0.0064) | Acc: (99.97%) (49984/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2574) | Acc: (92.89%) (9289/10000)\n",
            "Epoch: 161 | Batch_idx: 0 |  Loss: (0.0062) | Acc: (100.00%) (128/128)\n",
            "Epoch: 161 | Batch_idx: 10 |  Loss: (0.0066) | Acc: (99.86%) (1406/1408)\n",
            "Epoch: 161 | Batch_idx: 20 |  Loss: (0.0058) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 161 | Batch_idx: 30 |  Loss: (0.0061) | Acc: (99.92%) (3965/3968)\n",
            "Epoch: 161 | Batch_idx: 40 |  Loss: (0.0064) | Acc: (99.92%) (5244/5248)\n",
            "Epoch: 161 | Batch_idx: 50 |  Loss: (0.0065) | Acc: (99.92%) (6523/6528)\n",
            "Epoch: 161 | Batch_idx: 60 |  Loss: (0.0065) | Acc: (99.92%) (7802/7808)\n",
            "Epoch: 161 | Batch_idx: 70 |  Loss: (0.0065) | Acc: (99.92%) (9081/9088)\n",
            "Epoch: 161 | Batch_idx: 80 |  Loss: (0.0063) | Acc: (99.93%) (10361/10368)\n",
            "Epoch: 161 | Batch_idx: 90 |  Loss: (0.0065) | Acc: (99.93%) (11640/11648)\n",
            "Epoch: 161 | Batch_idx: 100 |  Loss: (0.0065) | Acc: (99.93%) (12919/12928)\n",
            "Epoch: 161 | Batch_idx: 110 |  Loss: (0.0064) | Acc: (99.94%) (14199/14208)\n",
            "Epoch: 161 | Batch_idx: 120 |  Loss: (0.0065) | Acc: (99.94%) (15479/15488)\n",
            "Epoch: 161 | Batch_idx: 130 |  Loss: (0.0066) | Acc: (99.94%) (16758/16768)\n",
            "Epoch: 161 | Batch_idx: 140 |  Loss: (0.0067) | Acc: (99.94%) (18037/18048)\n",
            "Epoch: 161 | Batch_idx: 150 |  Loss: (0.0068) | Acc: (99.94%) (19317/19328)\n",
            "Epoch: 161 | Batch_idx: 160 |  Loss: (0.0067) | Acc: (99.95%) (20597/20608)\n",
            "Epoch: 161 | Batch_idx: 170 |  Loss: (0.0067) | Acc: (99.95%) (21876/21888)\n",
            "Epoch: 161 | Batch_idx: 180 |  Loss: (0.0067) | Acc: (99.95%) (23156/23168)\n",
            "Epoch: 161 | Batch_idx: 190 |  Loss: (0.0066) | Acc: (99.95%) (24435/24448)\n",
            "Epoch: 161 | Batch_idx: 200 |  Loss: (0.0066) | Acc: (99.95%) (25715/25728)\n",
            "Epoch: 161 | Batch_idx: 210 |  Loss: (0.0066) | Acc: (99.95%) (26995/27008)\n",
            "Epoch: 161 | Batch_idx: 220 |  Loss: (0.0067) | Acc: (99.95%) (28274/28288)\n",
            "Epoch: 161 | Batch_idx: 230 |  Loss: (0.0066) | Acc: (99.95%) (29554/29568)\n",
            "Epoch: 161 | Batch_idx: 240 |  Loss: (0.0065) | Acc: (99.95%) (30834/30848)\n",
            "Epoch: 161 | Batch_idx: 250 |  Loss: (0.0066) | Acc: (99.96%) (32114/32128)\n",
            "Epoch: 161 | Batch_idx: 260 |  Loss: (0.0065) | Acc: (99.96%) (33393/33408)\n",
            "Epoch: 161 | Batch_idx: 270 |  Loss: (0.0065) | Acc: (99.96%) (34673/34688)\n",
            "Epoch: 161 | Batch_idx: 280 |  Loss: (0.0066) | Acc: (99.96%) (35952/35968)\n",
            "Epoch: 161 | Batch_idx: 290 |  Loss: (0.0066) | Acc: (99.96%) (37232/37248)\n",
            "Epoch: 161 | Batch_idx: 300 |  Loss: (0.0066) | Acc: (99.96%) (38512/38528)\n",
            "Epoch: 161 | Batch_idx: 310 |  Loss: (0.0066) | Acc: (99.96%) (39792/39808)\n",
            "Epoch: 161 | Batch_idx: 320 |  Loss: (0.0066) | Acc: (99.96%) (41072/41088)\n",
            "Epoch: 161 | Batch_idx: 330 |  Loss: (0.0066) | Acc: (99.96%) (42351/42368)\n",
            "Epoch: 161 | Batch_idx: 340 |  Loss: (0.0066) | Acc: (99.96%) (43631/43648)\n",
            "Epoch: 161 | Batch_idx: 350 |  Loss: (0.0066) | Acc: (99.96%) (44910/44928)\n",
            "Epoch: 161 | Batch_idx: 360 |  Loss: (0.0066) | Acc: (99.96%) (46190/46208)\n",
            "Epoch: 161 | Batch_idx: 370 |  Loss: (0.0066) | Acc: (99.96%) (47469/47488)\n",
            "Epoch: 161 | Batch_idx: 380 |  Loss: (0.0066) | Acc: (99.96%) (48749/48768)\n",
            "Epoch: 161 | Batch_idx: 390 |  Loss: (0.0066) | Acc: (99.96%) (49980/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2591) | Acc: (92.97%) (9297/10000)\n",
            "Epoch: 162 | Batch_idx: 0 |  Loss: (0.0031) | Acc: (100.00%) (128/128)\n",
            "Epoch: 162 | Batch_idx: 10 |  Loss: (0.0077) | Acc: (99.93%) (1407/1408)\n",
            "Epoch: 162 | Batch_idx: 20 |  Loss: (0.0077) | Acc: (99.89%) (2685/2688)\n",
            "Epoch: 162 | Batch_idx: 30 |  Loss: (0.0072) | Acc: (99.92%) (3965/3968)\n",
            "Epoch: 162 | Batch_idx: 40 |  Loss: (0.0076) | Acc: (99.92%) (5244/5248)\n",
            "Epoch: 162 | Batch_idx: 50 |  Loss: (0.0073) | Acc: (99.94%) (6524/6528)\n",
            "Epoch: 162 | Batch_idx: 60 |  Loss: (0.0070) | Acc: (99.95%) (7804/7808)\n",
            "Epoch: 162 | Batch_idx: 70 |  Loss: (0.0074) | Acc: (99.93%) (9082/9088)\n",
            "Epoch: 162 | Batch_idx: 80 |  Loss: (0.0072) | Acc: (99.93%) (10361/10368)\n",
            "Epoch: 162 | Batch_idx: 90 |  Loss: (0.0070) | Acc: (99.94%) (11641/11648)\n",
            "Epoch: 162 | Batch_idx: 100 |  Loss: (0.0068) | Acc: (99.95%) (12921/12928)\n",
            "Epoch: 162 | Batch_idx: 110 |  Loss: (0.0068) | Acc: (99.95%) (14201/14208)\n",
            "Epoch: 162 | Batch_idx: 120 |  Loss: (0.0067) | Acc: (99.95%) (15481/15488)\n",
            "Epoch: 162 | Batch_idx: 130 |  Loss: (0.0067) | Acc: (99.95%) (16760/16768)\n",
            "Epoch: 162 | Batch_idx: 140 |  Loss: (0.0066) | Acc: (99.95%) (18039/18048)\n",
            "Epoch: 162 | Batch_idx: 150 |  Loss: (0.0067) | Acc: (99.95%) (19318/19328)\n",
            "Epoch: 162 | Batch_idx: 160 |  Loss: (0.0069) | Acc: (99.94%) (20596/20608)\n",
            "Epoch: 162 | Batch_idx: 170 |  Loss: (0.0068) | Acc: (99.95%) (21876/21888)\n",
            "Epoch: 162 | Batch_idx: 180 |  Loss: (0.0068) | Acc: (99.95%) (23156/23168)\n",
            "Epoch: 162 | Batch_idx: 190 |  Loss: (0.0068) | Acc: (99.95%) (24435/24448)\n",
            "Epoch: 162 | Batch_idx: 200 |  Loss: (0.0068) | Acc: (99.95%) (25714/25728)\n",
            "Epoch: 162 | Batch_idx: 210 |  Loss: (0.0068) | Acc: (99.94%) (26993/27008)\n",
            "Epoch: 162 | Batch_idx: 220 |  Loss: (0.0068) | Acc: (99.94%) (28272/28288)\n",
            "Epoch: 162 | Batch_idx: 230 |  Loss: (0.0068) | Acc: (99.94%) (29551/29568)\n",
            "Epoch: 162 | Batch_idx: 240 |  Loss: (0.0068) | Acc: (99.94%) (30831/30848)\n",
            "Epoch: 162 | Batch_idx: 250 |  Loss: (0.0067) | Acc: (99.95%) (32111/32128)\n",
            "Epoch: 162 | Batch_idx: 260 |  Loss: (0.0067) | Acc: (99.95%) (33390/33408)\n",
            "Epoch: 162 | Batch_idx: 270 |  Loss: (0.0067) | Acc: (99.95%) (34670/34688)\n",
            "Epoch: 162 | Batch_idx: 280 |  Loss: (0.0067) | Acc: (99.95%) (35950/35968)\n",
            "Epoch: 162 | Batch_idx: 290 |  Loss: (0.0067) | Acc: (99.95%) (37230/37248)\n",
            "Epoch: 162 | Batch_idx: 300 |  Loss: (0.0067) | Acc: (99.95%) (38510/38528)\n",
            "Epoch: 162 | Batch_idx: 310 |  Loss: (0.0067) | Acc: (99.95%) (39789/39808)\n",
            "Epoch: 162 | Batch_idx: 320 |  Loss: (0.0067) | Acc: (99.95%) (41069/41088)\n",
            "Epoch: 162 | Batch_idx: 330 |  Loss: (0.0066) | Acc: (99.96%) (42349/42368)\n",
            "Epoch: 162 | Batch_idx: 340 |  Loss: (0.0066) | Acc: (99.95%) (43628/43648)\n",
            "Epoch: 162 | Batch_idx: 350 |  Loss: (0.0066) | Acc: (99.96%) (44908/44928)\n",
            "Epoch: 162 | Batch_idx: 360 |  Loss: (0.0066) | Acc: (99.96%) (46188/46208)\n",
            "Epoch: 162 | Batch_idx: 370 |  Loss: (0.0066) | Acc: (99.96%) (47468/47488)\n",
            "Epoch: 162 | Batch_idx: 380 |  Loss: (0.0066) | Acc: (99.96%) (48747/48768)\n",
            "Epoch: 162 | Batch_idx: 390 |  Loss: (0.0066) | Acc: (99.96%) (49978/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2576) | Acc: (92.99%) (9299/10000)\n",
            "Epoch: 163 | Batch_idx: 0 |  Loss: (0.0087) | Acc: (100.00%) (128/128)\n",
            "Epoch: 163 | Batch_idx: 10 |  Loss: (0.0054) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 163 | Batch_idx: 20 |  Loss: (0.0056) | Acc: (100.00%) (2688/2688)\n",
            "Epoch: 163 | Batch_idx: 30 |  Loss: (0.0054) | Acc: (100.00%) (3968/3968)\n",
            "Epoch: 163 | Batch_idx: 40 |  Loss: (0.0056) | Acc: (100.00%) (5248/5248)\n",
            "Epoch: 163 | Batch_idx: 50 |  Loss: (0.0056) | Acc: (100.00%) (6528/6528)\n",
            "Epoch: 163 | Batch_idx: 60 |  Loss: (0.0059) | Acc: (100.00%) (7808/7808)\n",
            "Epoch: 163 | Batch_idx: 70 |  Loss: (0.0058) | Acc: (100.00%) (9088/9088)\n",
            "Epoch: 163 | Batch_idx: 80 |  Loss: (0.0057) | Acc: (100.00%) (10368/10368)\n",
            "Epoch: 163 | Batch_idx: 90 |  Loss: (0.0056) | Acc: (100.00%) (11648/11648)\n",
            "Epoch: 163 | Batch_idx: 100 |  Loss: (0.0057) | Acc: (100.00%) (12928/12928)\n",
            "Epoch: 163 | Batch_idx: 110 |  Loss: (0.0059) | Acc: (100.00%) (14208/14208)\n",
            "Epoch: 163 | Batch_idx: 120 |  Loss: (0.0059) | Acc: (100.00%) (15488/15488)\n",
            "Epoch: 163 | Batch_idx: 130 |  Loss: (0.0060) | Acc: (100.00%) (16768/16768)\n",
            "Epoch: 163 | Batch_idx: 140 |  Loss: (0.0060) | Acc: (100.00%) (18048/18048)\n",
            "Epoch: 163 | Batch_idx: 150 |  Loss: (0.0060) | Acc: (99.99%) (19327/19328)\n",
            "Epoch: 163 | Batch_idx: 160 |  Loss: (0.0061) | Acc: (99.99%) (20606/20608)\n",
            "Epoch: 163 | Batch_idx: 170 |  Loss: (0.0062) | Acc: (99.99%) (21886/21888)\n",
            "Epoch: 163 | Batch_idx: 180 |  Loss: (0.0063) | Acc: (99.98%) (23164/23168)\n",
            "Epoch: 163 | Batch_idx: 190 |  Loss: (0.0063) | Acc: (99.98%) (24443/24448)\n",
            "Epoch: 163 | Batch_idx: 200 |  Loss: (0.0063) | Acc: (99.97%) (25721/25728)\n",
            "Epoch: 163 | Batch_idx: 210 |  Loss: (0.0063) | Acc: (99.97%) (27001/27008)\n",
            "Epoch: 163 | Batch_idx: 220 |  Loss: (0.0063) | Acc: (99.97%) (28279/28288)\n",
            "Epoch: 163 | Batch_idx: 230 |  Loss: (0.0063) | Acc: (99.97%) (29558/29568)\n",
            "Epoch: 163 | Batch_idx: 240 |  Loss: (0.0062) | Acc: (99.97%) (30838/30848)\n",
            "Epoch: 163 | Batch_idx: 250 |  Loss: (0.0062) | Acc: (99.97%) (32118/32128)\n",
            "Epoch: 163 | Batch_idx: 260 |  Loss: (0.0062) | Acc: (99.97%) (33398/33408)\n",
            "Epoch: 163 | Batch_idx: 270 |  Loss: (0.0062) | Acc: (99.97%) (34678/34688)\n",
            "Epoch: 163 | Batch_idx: 280 |  Loss: (0.0062) | Acc: (99.97%) (35958/35968)\n",
            "Epoch: 163 | Batch_idx: 290 |  Loss: (0.0061) | Acc: (99.97%) (37237/37248)\n",
            "Epoch: 163 | Batch_idx: 300 |  Loss: (0.0061) | Acc: (99.97%) (38517/38528)\n",
            "Epoch: 163 | Batch_idx: 310 |  Loss: (0.0062) | Acc: (99.97%) (39796/39808)\n",
            "Epoch: 163 | Batch_idx: 320 |  Loss: (0.0062) | Acc: (99.97%) (41076/41088)\n",
            "Epoch: 163 | Batch_idx: 330 |  Loss: (0.0063) | Acc: (99.97%) (42355/42368)\n",
            "Epoch: 163 | Batch_idx: 340 |  Loss: (0.0063) | Acc: (99.97%) (43635/43648)\n",
            "Epoch: 163 | Batch_idx: 350 |  Loss: (0.0063) | Acc: (99.97%) (44915/44928)\n",
            "Epoch: 163 | Batch_idx: 360 |  Loss: (0.0063) | Acc: (99.97%) (46194/46208)\n",
            "Epoch: 163 | Batch_idx: 370 |  Loss: (0.0063) | Acc: (99.97%) (47473/47488)\n",
            "Epoch: 163 | Batch_idx: 380 |  Loss: (0.0063) | Acc: (99.97%) (48753/48768)\n",
            "Epoch: 163 | Batch_idx: 390 |  Loss: (0.0063) | Acc: (99.96%) (49982/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2593) | Acc: (92.90%) (9290/10000)\n",
            "Epoch: 164 | Batch_idx: 0 |  Loss: (0.0033) | Acc: (100.00%) (128/128)\n",
            "Epoch: 164 | Batch_idx: 10 |  Loss: (0.0056) | Acc: (100.00%) (1408/1408)\n",
            "Epoch: 164 | Batch_idx: 20 |  Loss: (0.0065) | Acc: (99.93%) (2686/2688)\n",
            "Epoch: 164 | Batch_idx: 30 |  Loss: (0.0063) | Acc: (99.95%) (3966/3968)\n",
            "Epoch: 164 | Batch_idx: 40 |  Loss: (0.0061) | Acc: (99.96%) (5246/5248)\n",
            "Epoch: 164 | Batch_idx: 50 |  Loss: (0.0062) | Acc: (99.95%) (6525/6528)\n",
            "Epoch: 164 | Batch_idx: 60 |  Loss: (0.0064) | Acc: (99.96%) (7805/7808)\n",
            "Epoch: 164 | Batch_idx: 70 |  Loss: (0.0063) | Acc: (99.97%) (9085/9088)\n",
            "Epoch: 164 | Batch_idx: 80 |  Loss: (0.0063) | Acc: (99.97%) (10365/10368)\n",
            "Epoch: 164 | Batch_idx: 90 |  Loss: (0.0065) | Acc: (99.97%) (11644/11648)\n",
            "Epoch: 164 | Batch_idx: 100 |  Loss: (0.0065) | Acc: (99.96%) (12923/12928)\n",
            "Epoch: 164 | Batch_idx: 110 |  Loss: (0.0066) | Acc: (99.96%) (14203/14208)\n",
            "Epoch: 164 | Batch_idx: 120 |  Loss: (0.0066) | Acc: (99.97%) (15483/15488)\n",
            "Epoch: 164 | Batch_idx: 130 |  Loss: (0.0066) | Acc: (99.97%) (16763/16768)\n",
            "Epoch: 164 | Batch_idx: 140 |  Loss: (0.0067) | Acc: (99.97%) (18043/18048)\n",
            "Epoch: 164 | Batch_idx: 150 |  Loss: (0.0066) | Acc: (99.97%) (19322/19328)\n",
            "Epoch: 164 | Batch_idx: 160 |  Loss: (0.0066) | Acc: (99.97%) (20602/20608)\n",
            "Epoch: 164 | Batch_idx: 170 |  Loss: (0.0066) | Acc: (99.97%) (21882/21888)\n",
            "Epoch: 164 | Batch_idx: 180 |  Loss: (0.0066) | Acc: (99.97%) (23161/23168)\n",
            "Epoch: 164 | Batch_idx: 190 |  Loss: (0.0066) | Acc: (99.97%) (24441/24448)\n",
            "Epoch: 164 | Batch_idx: 200 |  Loss: (0.0066) | Acc: (99.97%) (25721/25728)\n",
            "Epoch: 164 | Batch_idx: 210 |  Loss: (0.0066) | Acc: (99.97%) (27000/27008)\n",
            "Epoch: 164 | Batch_idx: 220 |  Loss: (0.0066) | Acc: (99.97%) (28280/28288)\n",
            "Epoch: 164 | Batch_idx: 230 |  Loss: (0.0067) | Acc: (99.96%) (29557/29568)\n",
            "Epoch: 164 | Batch_idx: 240 |  Loss: (0.0066) | Acc: (99.96%) (30836/30848)\n",
            "Epoch: 164 | Batch_idx: 250 |  Loss: (0.0066) | Acc: (99.96%) (32116/32128)\n",
            "Epoch: 164 | Batch_idx: 260 |  Loss: (0.0065) | Acc: (99.96%) (33396/33408)\n",
            "Epoch: 164 | Batch_idx: 270 |  Loss: (0.0065) | Acc: (99.97%) (34676/34688)\n",
            "Epoch: 164 | Batch_idx: 280 |  Loss: (0.0065) | Acc: (99.96%) (35954/35968)\n",
            "Epoch: 164 | Batch_idx: 290 |  Loss: (0.0065) | Acc: (99.96%) (37234/37248)\n",
            "Epoch: 164 | Batch_idx: 300 |  Loss: (0.0065) | Acc: (99.96%) (38514/38528)\n",
            "Epoch: 164 | Batch_idx: 310 |  Loss: (0.0065) | Acc: (99.96%) (39793/39808)\n",
            "Epoch: 164 | Batch_idx: 320 |  Loss: (0.0065) | Acc: (99.96%) (41072/41088)\n",
            "Epoch: 164 | Batch_idx: 330 |  Loss: (0.0066) | Acc: (99.96%) (42352/42368)\n",
            "Epoch: 164 | Batch_idx: 340 |  Loss: (0.0066) | Acc: (99.96%) (43629/43648)\n",
            "Epoch: 164 | Batch_idx: 350 |  Loss: (0.0066) | Acc: (99.96%) (44909/44928)\n",
            "Epoch: 164 | Batch_idx: 360 |  Loss: (0.0066) | Acc: (99.96%) (46189/46208)\n",
            "Epoch: 164 | Batch_idx: 370 |  Loss: (0.0067) | Acc: (99.96%) (47467/47488)\n",
            "Epoch: 164 | Batch_idx: 380 |  Loss: (0.0067) | Acc: (99.95%) (48745/48768)\n",
            "Epoch: 164 | Batch_idx: 390 |  Loss: (0.0067) | Acc: (99.95%) (49976/50000)\n",
            "=> saving checkpoint\n",
            "# TEST : Loss: (0.2585) | Acc: (92.99%) (9299/10000)\n",
            "2 hours 38 mins 34 secs for training\n"
          ]
        }
      ]
    }
  ]
}
